<<<PAGE=1>>>
Stewart, R., Dayal, H. and Langer, L. (2017) ‘Terminology and tensions within 
evidence-informed decision-making in South Africa over a 15-year period’. 
Research for All , 1 (2): 252–64. DOI https://doi.org/10.18546/RFA.01.2.03
*Corresponding author – email: ruths@uj.ac.za ©Copyright 2017 Stewart, Dayal and Langer. This is an Open Access 
article distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, 
distribution, and reproduction in any medium, provided the original author and source are credited.
Terminology and tensions within evidence-
informed decision-making in South Africa over a 
15-year period 
Ruth Stewart* – University of Johannesburg, South Africa
Harsha Dayal – National Department for Planning, Monitoring and Evaluation, 
South Africa
Laurenz Langer – University of Johannesburg, South Africa
Abstract
In this article, we examine a key premise underlying evidence-informed decision-
making (EIDM) – that research is for all, including service users and potential users, 
service providers and a wide range of decision-makers, from those running local 
services to national government officials and international agencies. Qualitative 
data collected on terminology used when writing and talking about EIDM over a 
period of 15 years during the implementation of a number of capacity development 
programmes in South Africa were combined with critical reflections in practice. 
Findings reveal that tensions exist in the titles and terminology used to describe 
the relationships between academia and government or between research 
and policy, and that these tensions have shifted over time, but not necessarily 
diminished. An analysis and critique of this terminology is provided to identify and 
unpack these tensions, which challenge the central premise of ‘research for all’. 
The perpetuation of divisive labels that profile people, of job titles and specific 
terminology that describe agency, as well as the use of technical language, 
continues to exclude people from the approach. These have the effect of setting 
up users against producers of evidence. In conclusion, we challenge the advocates 
of the EIDM approach to review language and terminology to be more inclusive, 
to enable relationship-building and ease the process of engagement to ensure 
evidence-informed decision-making is true to its premise that research is for all.
Keywords: evidence-informed; evidence-based; terminology; division; inclusion
Key messages
●	 This article establishes that how people write and talk about evidence-informed 
decision-making can polarize and exclude stakeholders, undermining the key 
premise of evidence-informed decision-making that research is for all. 
●	 It finds that the tensions in the terminology we use to describe the relationships 
between academia and government, and research and policy have shifted over 
time, but have not necessarily diminished.
●	 This article challenges us to review our language and terminology around evidence-
informed decision-making to be more inclusive, to enable relationship-building and 
ease the process of engagement to ensure that all actors can contribute equally to 
the process of using evidence to inform policy and practice decisions.
<<<PAGE=2>>>
Terminology and tensions within evidence-informed decision-making in South Africa 253
Research for All 1 (2) 2017
Introduction
The use of research in evidence-informed decision-making (EIDM) is positioned as 
a public good, which anchors the concept firmly with the research for all premise. 
Research is perceived as serving to improve the lives of service users, and to inform 
the decisions that affect us all (Chalmers et al., 2012). In order to use research to inform 
decisions and to improve lives, research needs to be relevant to all and informed by 
those with real-life experience. If research is not for all, that is if certain groups are 
excluded from the EIDM process, this challenges the merit and benefit of research use. 
Groups can be excluded from research use in multiple ways: not being able to access 
published research; not being able to make sense of research because it is overly 
technical; not being able to integrate research results with groups’ contexts as the 
research scope or emphasis is not relevant to these contexts. In each of the examples, 
users are hindered to consider research when making decisions and thus excluded 
from the potential insights and benefits of research for their decision-making. If not 
everybody is able to access research, consider it, and use it to improve lives (where 
relevant), then research is not for all. Research use and EIDM remain a privilege of 
those with access to research and the technical skills to make sense of it – a situation 
that violates its research for all premise. 
Evidence-informed decision-making and research for all
Evidence-informed (or evidence-based) decision-making (EIDM/EBDM) encourages 
the use of rigorous evidence for better decisions. For the purpose of this article, we will 
use the term EIDM to refer to both the practice and academic discipline of research 
use. We therefore operationally define EIDM as a process whereby multiple sources 
of information, including the best available research evidence, are consulted before 
making a decision to plan, implement and (where relevant) revise policies, programmes 
and other services. EIDM has a history in evidence-based medicine that ‘integrates the 
best external evidence with individual clinical expertise and patients’ choice’ (Sackett 
et al ., 1996: 72). In addition to health care, EIDM has an important track record of 
improving policies and practices ranging across different sectors (for example, social 
welfare, education, environment change, justice and international development). 
Public programmes around juvenile crime preventions, saving household electricity 
or early childhood education pay testimony to the benefits of EIDM (Petrosino et al., 
2013; Halpern, 2015; Fiennes and Wulf, 2014). 
The case of learning styles
The myth of learning styles presents an illustrative case to underline the centrality of 
the research for all premise in EIDM. Despite rigorous synthesized research showing 
that there is no scientific evidence indicating a positive impact (or even existence) 
of teaching according to different learning styles (Pashler et al ., 2008; Rohrer and 
Pashler, 2012; Coffield et al., 2004), and these findings being communicated through 
multiple channels and by multiple sources, teachers’ support for the use of learning 
styles is almost universal (Dekker et al., 2012; Newton, 2015; Goldhill, 2016). What is 
more, teaching according to perceived learning styles is also highly popular among 
parents (Dekker et al ., 2012; Newton, 2015; Goldhill, 2016). There is therefore a 
clear disconnect between the research findings and the preferences and beliefs of 
educational practitioners and stakeholders. These two groups were not included in 
the process of synthesizing and assessing the research evidence on learning styles
<<<PAGE=3>>>
254 Ruth Stewart, Harsha Dayal and Laurenz Langer
Research for All 1 (2) 2017
and they do not access the insights and benefits of the produced research (that is, 
understanding the limitations of teaching according to learning styles and using more 
effective teaching methods). In this case, research did not influence decision-making 
beyond the academic context, and its potential positive impact on educational practice 
was not realized. 
Describing evidence use: Inclusivity in terminology? 
In the literature, and in practice, research for all is in many ways an ideology, rather 
than a reality; something that should be aspired to. The journal Research for All itself 
represents a shift in terminology towards a positive, inclusive environment. We are 
defining ‘terminology’ as the words used to describe and label individuals, concepts 
and activities in EIDM. Where terminology is used to categorize professional levels, we 
have used the term ‘title’. Despite the wide range of efforts to ensure research is for 
all, in both its production and its incorporation into decision-making, the terminology 
relating to this field is loaded with division, even when the emphasis is on working 
beyond or breaking down that division. People may feel excluded because they do 
not understand the use of technical language, they do not relate to the stakeholders 
referred to, or simply because there is an over-emphasis on the lack of evidence-
informed decision-making, which feels judgemental or exclusive. For example, 
evidence use is often referred to as: bridging the evidence–policy gap (Grimshaw et 
al., 2012; Crewe and Young, 2002); getting research into practice (Bero et al., 1998); 
knowledge translation or transfer (Davis et al., 2003); or knowledge brokering (Meyer, 
2010). The box below lists the many terms applied in the literature to describe the 
process of using research and the study of research use. It is worth noting that: the 
verbs used are almost exclusively about  ‘pushing’ or ‘transferring’, indicating a linear 
movement from point A to point B; the starting point is invariably the research and 
not the decision-maker or decision-making process; and lastly, that very few terms 
describe a system rather than an action. Furthermore, the various stakeholders in 
the evidence-use process are categorized as being on either side of a divide. On the 
one side, users are often described as policymakers, decision-makers, practitioners 
or adopters; on the other side, producers are located as researchers and academics, 
thereby reinforcing the polarity between generators and users of research. An 
exception to this two-community model of research use (Caplan, 1979) is the provision 
for intermediaries such as ‘knowledge brokers’, but even this terminology for those ‘in 
between’ the generators and users of research implies some kind of conflict between 
two communities that the broker has to bridge and resolve.
Commonly used EIDM terminology
Note that ‘research’, ‘evidence’ and ‘knowledge’ are all used interchangeably in 
these common terminologies.
Research utilization; research dissemination; research implementation; research 
diffusion; research uptake; knowledge mobilization; evidence use; knowledge 
application; knowledge exchange; knowledge translation; knowledge transfer; 
evidence broker; evidence champion; research adoption; knowledge sharing; 
evidence to policy; evidence into action; research into practice; research 
impact; evidence-based policy and practice; evidence-informed policy and practice; 
evidence-based decision-making; evidence-informed decision-making; evidence 
ecosystem; evidence–policy gap; evidence system.
<<<PAGE=4>>>
Terminology and tensions within evidence-informed decision-making in South Africa 255
Research for All 1 (2) 2017
This article explores how this terminology of research use in evidence-informed 
decision-making has shifted over time in South Africa, and discusses future directions 
for use of terminology and inclusivity in this field.
Methods
Data sources
South Africa has a growing community of people interested in supporting the use 
of evidence in decision-making (see Stewart, 2015 and Dayal, 2016 for more on this 
context). Our analysis draws on our own work in South Africa spanning 15 years, during 
which time we have been adopting and adapting terminology to suit the circumstances 
in which we have been working. We are specifically drawing on data that we collected 
during three phases of work, in 2001, 2007 and 2014–16, supporting others to engage 
with evidence-informed decision-making (Ellison et al., 2001; Stewart, 2001; Stewart, 
2015). In each of these three phases, colleagues from the universities of Johannesburg 
and London (including the authors of this article) have collaborated with a range of 
decision-makers and researchers within South Africa to encourage EIDM, including 
health and education practitioners, non-governmental organizations and colleagues 
in national government. This has included provision of workshops and mentorships 
in EIDM, as well as conducting joint research. Data on the terminology used have 
been collected using a variety of methods, including: PowerPoint slides, handouts and 
worksheets used to support the use of EIDM over the three phases; transcribed audio 
recordings of workshop discussions; and flip charts on which key issues have been 
noted. Verbal consent was given in all cases for the use of these materials for future 
analysis and publications. Written consent was obtained whenever audio recordings 
were made.
Analysis
We have conducted a secondary analysis of the data using thematic content analysis 
of terminology use over time (Cameron, 2001). We followed Guest et al. (2012) and 
developed descriptive and analytical themes for the data from our three phases of 
work separately. Therein, we used a mix of inductive and deductive themes (Fereday 
and Muir-Cochrane, 2006), that is for each data period we identified themes according 
to a predefined framework drawing on the authors’ combined experience of working 
in the field of EIDM. These themes focused on: (1) who are the perceived actors in 
the evidence-use process and how are they positioned in relation to each other; (2) 
how the process of evidence use was described; and (3) who is perceived to drive the 
evidence-use process. In addition to the exploration of these deductive themes, we 
further interrogated the data for indicative themes. In this, we used a configurative 
approach vested in the assumption that language use in psychological terms reflects 
thoughts and feelings, which can reveal implicit assumptions and power relations (for 
example, the divisive positioning of research users and producers discussed earlier) 
(Thomas, 2006). Trends and shifts in terminology over time were noted, and patterns 
identified through reflection and discussion.
Authors’ roles
This paper has been co-authored by university-based researchers and a colleague in 
a central government department. The authors were involved in the delivery of EIDM
<<<PAGE=5>>>
256 Ruth Stewart, Harsha Dayal and Laurenz Langer
Research for All 1 (2) 2017
projects from which the data for this analysis were collected. The use of ‘we’ in this 
article therefore neither represents the research community nor the public service. 
In addition, the special working relationship between the authors, and our dual 
role of providing data and analysing data, deserve attention. First, there is a question 
of objectivity. Our close working relationship might influence the way we interpret 
data. We had to take precautions so as not to be influenced by hindsight bias, which 
might have led us to rearrange data and patterns to lead to a linear and coherent story 
to fit with our current intellectual positions. This was particularly important in relation 
to the development of our own joint working relationship, and we actively interrogated 
ourselves so as to counter the illusion that our current working collaboration as 
decision-makers and researchers was all but inevitable or primarily of our own making. 
Second, the joint reflection on the data set and its analysis led us to discover and 
challenge a range of assumptions that we thought we mutually shared. For example, 
we assumed that we had developed a joint understanding and shared views about 
the titles and terminologies we were using. It was interesting to realize that, in some 
instances, different authors had simply taken up certain terms because they had heard 
trusted colleagues using them. Often, we had assumed that terms were adopted 
because we had a shared understanding about their conceptual advantage in reducing 
tensions, which was not necessarily the reason for adopting the term. Revisiting the 
data together for this article led to many interesting discussions tracing back the initial 
conceptual argument in favour of a term that some of us had simply accepted as a 
more relevant term through social interaction. Third, by reviewing the data collected 
at different points in our joint engagement, we realized how our own behaviour and 
use of terminology had influenced our work. This was a key difficulty in re-analysing the 
data, as it often made us aware of our own shortcomings and, at times, naivety in trying 
to support the use of evidence. 
Findings
Our analysis revealed five overarching tensions in the terminologies used to describe 
EIDM: (1) in the titles that are used to address people; (2) in how EIDM is conceptualized; 
(3) in how EIDM is practised; (4) in how power relationships are constructed; and (5) in 
the concept of research for all. We discuss each in turn below.
Tensions in how people are addressed
Our data indicate a real and persistent tension in descriptions of people, particularly 
those working in ‘other’ environments (what we are referring to as ‘titles’ for 
people). These tensions are characterized by uncertainty and frustration with the 
apparent emphasis on hierarchies and job titles by those working in academia and 
in government, suggesting a process of adjustment is required while communities of 
practice that are not used to working together establish their individual status within 
this multidisciplinary environment. An example of this is the way in which a senior 
government colleague dismissed academics’ obsessions with hierarchy and titles: 
‘They insist on being called “professor”, but they don’t call us “director”’. At the same 
time, colleagues in universities have highlighted the many hierarchies in government 
and expressed annoyance with the way in which some senior civil servants expect to 
be treated because of their positions. For example, one senior civil servant at our 
workshops resented being sent an invitation by email and requested something more 
formal. Despite noting these current frustrations in our data, we have noted shifts in 
terminology among some groups, perhaps best illustrated by how we (the authors)
<<<PAGE=6>>>
Terminology and tensions within evidence-informed decision-making in South Africa 257
Research for All 1 (2) 2017
refer to one another. Currently we use terms such as ‘colleagues’ and ‘partners’ to 
characterize our relationship, but initially terms such as ‘collaborators’, ‘workshop 
participants’ and ‘evidence champions’, and the use of titles such as ‘professor’, 
shaped our positioning of each other. The initial terms we used were informed by 
how we came to know each other, in contrast to the current terms we use, which are 
informed by how we work together. 
Tensions also extend to how the research-use process is itself entitled. Our 
analysis indicated the following four key issues with the terminology that people use 
that potentially leads to tensions in how we describe the use of research: (1) the supply 
of evidence is described as a linear push process, indicating few democratic elements 
of participation, multiple voices and shared control; (2) terms such as ‘research 
communication’, ‘dissemination’, ‘diffusion’ and ‘transfer’ all invoke a linear process 
in which researchers enlighten users about their findings; (3) there is no two-way flow 
of information or challenge to researchers’ status as the bearers of knowledge that is 
implicit in these terms – evidence is produced and then supplied, and this is as far as 
the researchers’ involvement in the EIDM process goes; and (4) encouragingly, this 
model of EIDM and its associated terms are now used less, with some proclaiming, for 
example, that ‘dissemination is dead’ (Clark, 2016). 
Divisive terminology in the EIDM concept and process
The terminology used to conceptualize EIDM is divisive. The importance of demand 
for evidence and the agency of evidence users  to shape EIDM is not reflected in 
most terminology. We identified within our data the use of terms that assume that 
the evidence produced is the finished product that has only to be accessed and used 
by decision-makers, including ‘knowledge transfer’, ‘research uptake’ and ‘knowledge 
application’. The implication is that evidence will automatically be taken up and 
applied (implemented, mobilized, utilized, adopted). This terminology does not cater 
for a change of the evidence by the users, who are semantically presented as a passive 
entity. We observed a few terms that allow for a more active involvement of decision-
makers in constructing evidence and an increased agency to determine its usage. 
These refer to knowledge ‘sharing’ and ‘exchange’, which are less one-sided and 
less one-directional, recognizing that all stakeholders have knowledge to share and 
exchange. However, these terms seem to miss the evidence use and decision-making 
aspect of EIDM and do not move beyond the supply of evidence. 
Furthermore, the findings demonstrate that the EIDM process was frequently 
presented as something initiated by researchers. The few terms that picture EIDM as 
a process (for example, ‘research into action ’, ‘knowledge into practice ’, ‘evidence 
to policy ’) all position the process as starting with the research. This reinforces the 
push element of EIDM, does not see EIDM actors on the same level and gives EIDM 
a research-dominated feel. The assumption within the terminology we observed, that 
EIDM is initiated by researchers who push knowledge into action, suggests that the 
users of research should merely receive knowledge in a passive way. We saw repeated 
use of research-driven process terms that construct a false dichotomy between research 
and action, knowledge and practice, evidence and policy. 
Despite the conceptualization of decision-makers as the recipients and not the 
initiators of EIDM, recent innovations in EIDM terminology present a more organic 
and systemic understanding of evidence use. We recorded the use of terms such as 
‘research–policy landscape’, ‘evidence–policy interface’, ‘evidence discourse’ and 
‘evidence ecosystem’, all of which seem to indicate a more democratic understanding 
of evidence and decision-making. The process of use in these examples from our data
<<<PAGE=7>>>
258 Ruth Stewart, Harsha Dayal and Laurenz Langer
Research for All 1 (2) 2017
is more interactive and less linear; actors involved in EIDM are seen on equal footing 
and possess the same power to shape the process. Such systemic terms are far from 
the norm and can likewise fall into a more divisive conception of EIDM (as is the case of 
talking about ‘evidence–policy gaps’ and ‘divides to bridge’). This raises the question 
of what to prioritize: research or policy? In South Africa, for example, Dayal (2016), 
writing as a public official in government, uses the term ‘policy–research interface’, 
while Choge et al. (2014), who are writing from an academic perspective, refer to the 
‘research–policy landscape’. Clearly, even the use of these compound terms reveals 
institutional biases and one’s inherent conception of EIDM. 
Tensions in the practice of EIDM 
Our analysis indicated that the inherent tensions of EIDM terminology had important 
implications for the practice of EIDM. We found multiple instances where EIDM terms 
coined by academics (examples include ‘systematic review’ and ‘critical appraisal’) 
did not resonate with decision-makers, resulting in a reluctance to engage with 
EIDM and even, at times, hostility towards the approach. For example, one decision-
maker branded the EIDM approach as too critical, as it seemed to imply that current 
government decisions are evidence- uninformed. Another public servant recalled 
being talked down to by researchers who knew little of how government works or how 
policy is made. A strong theme in the data was a researcher-driven construction or 
implicit suggestion of a deficit model of decision-making in practice and policy that, 
unsurprisingly, increased reservations toward EIDM. For example, civil servants felt that 
academics made unhelpful assumptions about people working in government, labelling 
them collectively as policymakers and assuming that they need to have their capacity 
built. We have noted repeated challenges from colleagues working for government 
that university academics, and other outsiders, know little about social policy or the 
processes of government. In line with this theme was our observation that when EIDM 
is presented to decision-makers by others within government, it is better received than 
when presented by outsiders. On one level, this internal promotion of EIDM within 
government reduces or even removes the tensions discussed above, where individuals 
are busy emphasizing their own status through the use of hierarchical titles, and pushing 
an approach from ‘outside’ with poor understanding of internal government language 
and practice. On another level, it seems that if EIDM is presented by government 
colleagues or encouraged by government institutions, the use of evidence is demand-
led and is deemed as a more legitimate process, and potential best practice, in the 
public service sector. 
Not only did we find assumptions that civil servants have a skills deficit that 
needs to be addressed, we also found no acknowledgement in the terminology used 
that researchers have a deficit in their own skills and understanding. For example, civil 
servants repeatedly spoke of frustration at the assumption that they need their ‘capacity 
built’, and expressed annoyance at how poorly informed researchers are about how 
government works. Taking this point further, we also found limited EIDM capacities 
among the researchers who were themselves promoting the approach. For example, 
few EIDM capacity-building programmes with which we engaged were informed by 
rigorous systematic review evidence of which capacity-building models work best 
(Stewart, 2015; Langer et al., 2016). Our analysis also showed that when promoting 
and supporting systematic reviews (often positioned as the gold standard for evidence 
within the EIDM paradigm), most researchers who are unfamiliar with EIDM themselves 
feel threatened and become defensive. They are not always comfortable with the ideas 
of critical appraisal and of excluding research based on predefined criteria. By virtue of
<<<PAGE=8>>>
Terminology and tensions within evidence-informed decision-making in South Africa 259
Research for All 1 (2) 2017
making decision-making explicit and requiring criteria for what information is relevant 
and trustworthy to inform this process, EIDM can be perceived as challenging and 
threatening (Stewart, 2007). The defensive responses observed by decision-makers 
and researchers alike suggest that some tension is inherent within the approach, which 
presents a conceptual challenge given EIDM’s embrace of the research for all premise. 
Our data indicate a progressive shift over the 15-year period towards a more 
inclusive practice of EIDM. This included changes from using terms such as ‘training’, 
to ‘capacity building’ and ‘capacity sharing’. It also included an increasing emphasis on 
the need for broader, arguably non-research, capacities to complement the research 
skills that were promoted in the earlier stages of our work. The EIDM paradigm calls 
for research skills and/or evidence-use skills to be built (searching for, appraising and 
synthesizing evidence), and these were very much the emphasis of our workshops in 
2001 and 2007. In 2015/16, we saw a shift towards the inclusion of programme planning 
and programme evaluation within this EIDM capacity-building package. Given the 
strong role that government has taken in South Africa in building the evidence system, 
there is also an increasing shift to conceiving evidence use as a process that starts with 
the user rather than the producer of evidence (Langer and Stewart, 2016; Dayal, 2016).
Prevalent but shifting power dynamics
The practice of EIDM also reveals a tension in power relations. This has changed over 
time, but still remains today. An early theme in our data was the unease among non-
academics when talking about research and researchers. We observed nervousness, 
laughter and sometimes silence when non-academics were invited to comment on a 
piece of research. However, this has shifted over time, with a qualified and experienced 
cohort of people working within government who are confident in their engagement 
with academia and with consultancies. There is less dependency on researchers to 
come up with answers alone, allowing increasing numbers of research users to assume 
agency in setting the research agenda as well as the terms of research usage. Also, 
more researchers are entering, and have taken up a career path in, the public service 
sector, which may be contributing to the shift in terminology and practice. In 2001, data 
show unease with research terminology by decision-makers. Non-research participants 
in our workshops repeatedly asked for terms to be explained and expressed frustration 
with researchers, feeling that they were being deliberately excluded by researchers’ use 
of technical language. In 2007, the unease about research terminology was noticeably 
reduced among participants, who were more comfortable discussing pieces of research 
and research methods more generally. It is impossible to know whether this shift has 
taken place across the board in South Africa, but it was noticeable from our data that 
people felt more comfortable with the concept of EIDM and the terminology used. 
Finally, in 2015/16 we have experienced discussions in which government 
colleagues are confident in talking about research and the use of technical language 
around EIDM. On the other side, non-government colleagues are beginning to 
recognize how much they have to learn about how government works, and we 
have come across very few examples of academics with a good understanding of 
policymaking and procedures in government.
This shift in power extends to meta-discussions about what is research. Now 
we are observing tensions within government in how people understand ‘research’; 
questions about how research informs planning, monitoring and evaluation; and 
whether ‘evidence’ is a better term to use. It is important to note that these are 
discussions we are having with colleagues in government, and not with researchers – a 
clear indication of how the agency in EIDM has shifted. Government colleagues are
<<<PAGE=9>>>
260 Ruth Stewart, Harsha Dayal and Laurenz Langer
Research for All 1 (2) 2017
starting to play a greater role in agenda-setting for EIDM. For example, they demand 
‘policy-relevant’ research, which entails tailoring research methods and production to 
take decision-makers’ needs into consideration. Government is no longer a passive 
consumer of research, but is developing an active voice about what it understands as 
relevant research and how it can be supported to use this research for decision-making. 
From tensions towards research for all
This analysis has shown how the way in which EIDM is presented is directly challenging 
for decision-makers and provokes an unhelpful tension or polarity between those 
who produce research and those who use it, challenging the premise of research for 
all. We have, however, observed a marked shift in how the approach is presented 
in response to these tensions and in an attempt to diminish them. EIDM is often 
introduced using spectra and cycles to illustrate the process of integrating evidence 
into policy decisions. These tools are generally developed by those outside of 
the policy space and promoted to decision-makers. In South Africa, we have clear 
indications of how this has progressed in our capacity-building activities. In 2001, we 
used a linear model and imposed it from outside with ‘unsubstantiated certainty’ at 
one end of the spectrum and ‘evidence-based certainty’ at the other end. In 2007, we 
changed our linear illustration of EIDM into a cyclical diagram to describe the EIDM 
decision-making process. This had less emphasis on the strengths or weaknesses of an 
individual’s position, and more focus on the cycle of reviewing the evidence, making 
a decision, implementing it and evaluating again. However, this cycle continues to be 
developed by the researchers and presented to decision-makers as a way of improving 
their work. In 2015, we saw a marked shift as we are now being encouraged to promote 
an evidence-based policymaking cycle developed by colleagues inside of government 
(DPME, 2014). This shift has implications for capacity-building curricula for EIDM, as 
well as for the general approach to research use. 
In addition, challenges to the dichotomy between research users and producers 
have increased, now taking the form of strong opposition rather than merely concerns 
or challenges. There has been an increase in calls for co-production and co-use. In 
2001, we had calls for co-authorship with non-university attendees, who felt that they 
were left out of publications that were about them. Non-researchers voiced criticisms 
of researchers for conducting research with the decision-makers as research subjects, 
and then disappearing and not reporting back, giving those same decision-makers 
no chance to learn from the research. By 2015, there had been several calls for the 
breaking down of the system structure that suggests that some people produce and 
others use research. In many cases, calls for co-production have been conceptual 
arguments for a shift in the terminology used and the ideas explored. In other cases, 
we have started to see co-production where those employed in academia are working 
with those employed in government to co-produce evidence maps to support EIDM. 
These welcome shifts towards a more inclusive conception and practice of EIDM, 
reducing tensions and changing roles and power relations to move closer towards 
the premise of research for all can be observed in a number of key terminology 
changes as well. EIDM itself – while still imperfect – presents a shift away from a top-
down research-driven conception of evidence use, as in evidence-based practice 
and policymaking. ‘Evidence- informed’ acknowledges the many factors other than 
evidence that influence decision-making, while the use of ‘decision-maker’ rather 
than ‘policymaker’ acknowledges the diversity of public sector professionals and their 
roles. In South Africa, the term ‘evidence’ is increasingly substituted for ‘research’ to
<<<PAGE=10>>>
Terminology and tensions within evidence-informed decision-making in South Africa 261
Research for All 1 (2) 2017
incorporate monitoring and evaluation data, and government internal evaluations 
and reports. 
Discussion
In our analysis of the titles and terminology used in EIDM in South Africa over a 15-year 
period, we have noted several areas of tension, while also observing some marked 
shifts towards research for all. We are nonetheless left with some important questions, 
which are outlined below. 
One issue is whether the word ‘research’ is actually accurate or whether 
‘evidence’ is a preferable term, particularly if different people use ‘research’ broadly or 
narrowly, without necessarily providing a definition. It is not clear which term is more 
accurate or inclusive. Both words exist in lay terminology but mean slightly different 
things in different contexts: a child may research a topic for homework, a lawyer may 
bring evidence to a court room. 
A second issue is whether, if research is for all, the terminology around research 
use should be adapted to overcome the two-dimensional conception of research 
and decision-making, and perhaps be described instead as an organic process within 
an evidence ecosystem. By avoiding a two-sided conception, this also avoids the 
necessity to name one side first: the policy–research versus research–policy tension. 
This overlaps with some related discussions about ‘evidence literacies’ (Newman, 2014) 
and ‘evidence thinking’ (Dayal, 2016). Both authors wonder whether evidence use is 
a behaviour that should be inherent in the decision-making of all actors in society. 
This view positions the use of evidence as a behaviour emerging from the bottom 
up in society, and that shares and reinforces the skills associated with it at all levels 
of decision-making. Individuals shaped by society to be receptive towards the use 
of evidence, then merely advance and apply this behaviour of using evidence when 
assuming decision-making positions.
A third issue that we are deliberating is whether or not it matters if practice 
is keeping up with terminology. Should we be proposing and using more inclusive 
terminology to show the direction in which we hope to move; or should we be using the 
terminology that stakeholders prefer in their contexts, whatever those terms might be? 
This line of thought equally applies to the term ‘research for all’. Would practitioners, 
decision-makers, and the general public actually agree that research should be for all, 
or is this an assumption made by academics on behalf of others?
This question leads us to our last issue, which some might view as the solution 
to our tensions, issues in finding common terminology and practice of supporting 
the use of evidence: co-production. Many argue that institutionalizing co-production 
will diminish tensions in terminology and practice of EIDM, as researchers and 
decision-makers become more familiar with one other, develop common practice, 
and discuss relationships and language, resulting in evidence that is useful and will 
be used. This might be a desirable but somewhat romantic notion of co-production. 
Our reflections and experience indicate that co-production can be a powerful tool to 
establish trusted relationships and support evidence use. We have also experienced 
the practicalities of making co-production work and the politics associated with this 
method. Co-production enters both parties into a situation of interdependency, which, 
if successful, can emphasize each other’s respective strengths and expertise. It can 
also reduce stress and tensions, for example users’ concerns about whether the final 
product will be useful to them or producers’ worries about whether there is actually 
anybody waiting to use their research. However, we have also observed that making
<<<PAGE=11>>>
262 Ruth Stewart, Harsha Dayal and Laurenz Langer
Research for All 1 (2) 2017
co-production work requires an investment in financial and non-financial resources, 
including time and perseverance by both parties, and that there is not a single method 
of co-production. Co-producing with government officials, highly skilled and equipped 
with postgraduate degrees, differs from co-producing with others who may work in 
different spheres. If one does not tailor co-production carefully, there is again a danger 
of placing users into EIDM boxes in which they do not fit. 
The user-involvement movement in health promoted as much user engagement 
as possible, with ‘user-led’ research promoted as a gold standard by some (Faulkner 
and Thomas, 2002). Is this the ultimate means to achieve research for all, that is 
removing the researcher as an institution in the research process, and if so, what does 
this mean for EIDM? We are cautious about advocating such a complete blurring of 
professions, in which the profession of researcher and its associated skill set become 
obsolete as this function is primarily exercised by users. Stewart and Liabo (2012) 
argue for contributions from all (research users and producers), but taking care not 
to diminish the technical skills required to conduct research. In 2017, we reflect on 
this and extend the argument that co-production equally runs the risk of diminishing 
the technical skills of civil servants. We are wondering whether research for all can 
best be seen as a spectrum of engagement that varies with the particular context of 
research producers and users. This would position co-production not as a prescriptive 
end goal, but instead as a spectrum that recognizes the different elements required 
in EIDM and how people need to work together with mutual respect to achieve the 
multidisciplinary, participatory and problem-based processes required. The context in 
which evidence is to be used will determine what expertise is required, as well as how 
much to blur the boundaries between different types of expertise. 
Conclusion
Terminology can both reflect and create barriers, and the kinds of partnership working 
that breaks down those barriers. If EIDM is to reflect the principle of research for all 
on which it is based, there is a need to continue the process that we have begun, of 
reflecting on the terminology we use, critiquing our own positions and looking to a 
future where our terminology facilitates the production and use of evidence. 
Notes on the contributors
Ruth Stewart is Director of the Africa Centre for Evidence, University of Johannesburg 
and chair of the Africa Evidence Network. She has worked for nearly 20 years on the 
production of synthesized evidence to inform decision-making, including systematic 
reviews, rapid evidence assessments and evidence maps. She is committed to making 
a difference through her work by ensuring that research evidence is useful and used, 
most recently through leading the three-year University of Johannesburg Building 
Capacity to Use Research Evidence programme (UJ-BCURE). 
Harsha Dayal is Director: Research Management at the National Department for 
Planning, Monitoring and Evaluation in South Africa. Harsha has worked extensively 
on the nexus of science and government, including positions at the University of 
Witwatersrand and the South African Human Science Research Council, before joining 
the public sector. She led the 20-year review in government and recently published a 
report on using evidence to reflect on South Africa’s 20 years of democracy.
<<<PAGE=12>>>
Terminology and tensions within evidence-informed decision-making in South Africa 263
Research for All 1 (2) 2017
Laurenz Langer is an Evidence Synthesis Specialist at the Africa Centre for Evidence, 
University of Johannesburg. He has engaged in a range of capacity-building and co-
production projects with government departments on adapting evidence synthesis 
methods in a policy context. Laurenz has produced multiple systematic reviews and 
evidence maps, including reviews published by the Campbell Collaboration, the 
Collaboration for Environmental Evidence and the EPPI-Centre at UCL Institute of 
Education, University College London. 
References 
Bero, L.A., Grilli, R., Grimshaw, J.M., Harvey, E., Oxman, A.D. and Thomson, M.A. (1998) ‘Getting 
research findings into practice: Closing the gap between research and practice: An overview of 
systematic reviews of interventions to promote the implementation of research findings’. British 
Medical Journal, 317 (7156), 465–8.
Cameron, D. (2001) Working with Spoken Discourse. London: SAGE Publications.
Caplan, N. (1979) ‘The two-communities theory and knowledge utilization’. American Behavioral 
Scientist, 22 (3), 459–70.
Chalmers, I., Essali, A., Rezk, E. and Crowe, S. (2012) ‘Is academia meeting the needs of non-
academic users of the results of research?’. Lancet, 380, 43.
Choge, I., Omondi, O., Erasmus, Y., Zaranyika, H., Langer, L. and Stewart, R. (2014) Landscape 
Review: An overview of role players facilitating evidence-informed decision-making in 
South Africa. Johannesburg: UJ-BCURE, Centre for Anthropological Research, University of 
Johannesburg.
Clark, T. (2016) ‘Dissemination is dead: Long live learning’. Video. Online. www.youtube.com/
watch?v=7Juy99v-0zo (accessed 29 July 2016).
Coffield, F ., Moseley, D., Hall, E. and Ecclestone, K. (2004) Learning Styles and Pedagogy in Post-16 
Learning: A systematic and critical review. London: Learning and Skills Research Centre.
Crewe, E. and Young, J. (2002) Bridging Research and Policy: Context, evidence and links (Working 
Paper 173). London: Overseas Development Institute.
Davis, D., Evans, M., Jadad, A., Perrier, L., Rath, D., Ryan, D., Sibbald, G., Straus, S., Rappolt, S., 
Wowk, M. and Zwarenstein, M. (2003) ‘Learning in Practice: The case for knowledge translation: 
Shortening the journey from evidence to effect’. British Medical Journal, 327 (7405), 33–5.
Dayal, H. (2016) Using Evidence to Reflect on South Africa’s 20 Years of Democracy: Insights from 
within the policy space (Working Paper 7). Jakarta: Knowledge Sector Initiative. Online. www.ksi-
indonesia.org/files/1460366649$1$VW8F$.pdf (accessed 25 March 2017).
Dekker, S., Lee, N.C., Howard-Jones, P . and Jolles, J. (2012) ‘Neuromyths in education: Prevalence 
and predictors of misconceptions among teachers’. Frontiers in Psychology, 3, Article 429, 1–8. 
Online. http://dx.doi.org/10.3389/fpsyg.2012.00429 (accessed 29 July 2016).
DPME (Department of Planning, Monitoring and Evaluation) (2014) Overview Paper: What is 
evidence-based policy-making and implementation? Pretoria: DPME. Online. www.dpme.gov.
za/keyfocusareas/evaluationsSite/Evaluations/What%20is%20EBPM%2014%2010%2013_mp.pdf 
(accessed 29 July 2016).
Ellison, G., Wiggins, M., Stewart, R. and Thomas, J. (2001) The HIVSA Training Manual. London: 
Social Science Research Unit, Institute of Education, University of London.
Faulkner, A. and Thomas, P . (2002) ‘User-led research and evidence-based medicine’. British Journal 
of Psychiatry, 180 (1), 1–3.
Fereday, J. and Muir-Cochrane, E. (2006) ‘Demonstrating rigor using thematic analysis: A hybrid 
approach of inductive and deductive coding and theme development’. International Journal of 
Qualitative Methods, 5 (1), 80–92.
Fiennes, C. and Wulf, L. (2014) Getting Better: What education systems in less developed countries 
can learn from evidence-based medicine. Giving Evidence. Online. https://givingevidence.files.
wordpress.com/2014/05/getting-better-full-version-may-20141.pdf (accessed 29 July 2016).
Goldhill, O. (2016) ‘Neuromythology: The concept of different “learning styles” is one of the 
greatest neuroscience myths’. Quartz, 3 January. Online. http://qz.com/585143/the-concept-of-
different-learning-styles-is-one-of-the-greatest-neuroscience-myths/ (accessed 29 July 2016).
Grimshaw, J.M., Eccles, M.P ., Lavis, J.N., Hill, S.J. and Squires, J.E. (2012) ‘Knowledge translation of 
research findings’. Implementation Science, 7 (50), 1–17.
Guest, G., MacQueen, K.M. and Namey, E.E. (2012) Applied Thematic Analysis. Thousand Oaks, 
CA: SAGE Publications.
<<<PAGE=13>>>
264 Ruth Stewart, Harsha Dayal and Laurenz Langer
Research for All 1 (2) 2017
Halpern, D. (2015) Inside the Nudge Unit: How small changes can make a big difference. London: 
W.H. Allen.
Langer, L. and Stewart, R. (2016) ‘The science of using research: Why it starts with the policymaker’. 
The Conversation, 19 May. Online. https://theconversation.com/the-science-of-using-research-
why-it-starts-with-the-policymaker-59265 (accessed 29 July 2016).
Langer, L., Tripney, J. and Gough, D. (2016) The Science of Using Science: Researching the use 
of research evidence in decision-making. Final Report. London: EPPI-Centre, Social Science 
Research Unit, UCL Institute of Education, University College London. Online. http://eppi.ioe.
ac.uk/cms/Default.aspx?tabid=3504 (accessed 25 March 2017).
Meyer, M. (2010) ‘The rise of the knowledge broker’. Science Communication, 32 (1), 118–27.
Newman, K. (2014) What is the Evidence on the Impact of Research on International 
Development? A DFID literature review. Online. https://assets.publishing.service.gov.uk/
media/57a089aced915d622c000343/impact-of-research-on-international-development.pdf 
(accessed 16 April 2017).
Newton, P .M. (2015) ‘The learning styles myth is thriving in higher education’. Frontiers in 
Psychology, 6, Article 1908, 1–5.
Pashler, H., McDaniel, M., Rohrer, D. and Bjork, R. (2008) ‘Learning styles: Concepts and 
evidence’. Psychological Science in the Public Interest, 9 (3), 105–19.
Petrosino, A., Turpin-Petrosino, C., Hollis-Peel, M.E. and Lavenberg, J.G. (2013) Scared Straight and 
Other Juvenile Awareness Programs for Preventing Juvenile Delinquency: A systematic review. 
Campbell Systematic Reviews 2013:5. Online. DOI: 10.4073/csr.2013.5.
Rohrer, D. and Pashler, H. (2012) ‘Learning styles: Where’s the evidence?’ Medical Education, 46 
(7), 634–5.
Sackett, D.L., Rosenberg, W.M.C., Gray, J.A.M., Haynes, R.B. and Richardson, W.S. (1996) ‘Evidence 
based medicine: What it is and what it isn’t’. British Medical Journal, 312 (7023), 71–2.
Stewart, R. (2001) The HIVSA Project Report. London: Social Science Research Unit, Institute of 
Education, University of London.
Stewart, R. (2007) ‘Expertise and multi-disciplinary training for evidence-informed decision-making’. 
PhD thesis, Institute of Education, University of London.
Stewart, R. (2015) ‘A theory of change for capacity building for the use of research evidence by 
decision makers in southern Africa’. Evidence and Policy, 11 (4), 547–57.
Stewart, R. and Liabo, K. (2012) ‘Involvement in research without compromising research quality’. 
Journal of Health Services Research and Policy, 17 (4), 248–51.
Thomas, D.R. (2006) ‘A general inductive approach for analyzing qualitative evaluation 
data’. American Journal of Evaluation, 27 (2), 237–46.