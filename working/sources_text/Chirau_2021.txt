<<<PAGE=1>>>
1
Evidence & Policy • vol xx • no xx • 1–12 • © Policy Press 2021   
Print ISSN 1744-2648 • Online ISSN 1744-2656 • https://doi.org/10.1332/174426421X16104826256918  
Accepted for publication 12 January 2021 • First published online 08 February 2021
practice
Policies for evidence: a comparative analysis of 
Africa’s national evaluation policy landscape
Takunda J Chirau, takunda.chirau@wits.ac.za
Caitlin Blaser-Mapitsa, caitlin.mapitsa@wits.ac.za
Matodzi M Amisi, matodzi.amisi@wits.ac.za
University of Witwatersrand, South Africa
Background:  African countries are developing their monitoring and evaluation policies to 
systematise, structure and institutionalise evaluations and use of evaluative evidence across the 
government sector. The pace at which evaluations are institutionalised and systematised across 
African governments is progressing relatively slowly.
Aims and objectives: This article offers a comparative analysis of Africa’s national evaluation policy 
landscape. The article looks at the policies of Zimbabwe, South Africa, Nigeria, Kenya (not adopted) 
and Uganda. To achieve the aim we unpack the different characteristics taken by the national 
evaluation policies, emerging lessons for countries who wish to develop a national evaluation 
policy, and key challenges faced by countries with regard to evaluation policy development and 
implementation. The article draws on both a desktop review and action research approaches from 
the Centre for Learning on Evaluation and Results Anglophone Africa to build national evaluation 
systems across the region. The approach has included peer learning and co-creation of knowledge 
around public sector evaluation systems.
Key conclusions: The national evaluation policies reviewed share certain common features in terms 
of purpose and composition. They are also struggling with common issues of institutionalising the 
evaluation system across the public sector. However, there are variations in the countries’ guiding 
governance frameworks at a national level that shape the nature and content of policies, as well 
as the ways through which the policies themselves are expected to guide the use of evaluative 
evidence for decision and policymaking, and programming.
Key words national evaluation policy • evidence use • monitoring • evaluation
Key messages
•  Peer-to-peer learning is important for sharing experiences on developing national evaluation 
policy.
•  Countries should develop their policies in line with their state architecture, context and 
relevance to their needs.
•  Policies necessitate new ways of thinking about the practice of monitoring and evaluation.
•  This article fills an important empirical lacuna on evidence use and policy development  
in Africa
To cite this article: Chirau, T., Blaser-Mapitsa, C. and Amisi, M. (2021) Policies for evidence: a 
comparative analysis of Africa’s national evaluation policy landscape, Evidence & Policy,  
vol xx, no xx, 1–12, DOI: 10.1332/174426421X16104826256918
Evidence & Policy
1744-2648
1744-2656
10.1332/174426421X16104826256918
12January2021
xx
xx
1
12
© Policy Press 2021
08February2021
2021
<<<PAGE=2>>>
Takunda J Chirau et al
2
Introduction
In Africa, National Evaluation Policies (NEPs) are regarded as important for the 
building of coherence and capacity across public sector evaluation systems, and also 
as the source of evaluative evidence for decision making. Systems to implement and 
facilitate these policies have proliferated over the past two decades, with objectives 
including performance measurement and impact assessment and planning for future 
programmes and projects. Governments are required to account to their electorate 
on the effectiveness of public sector programmes (Porter and Goldman, 2013). Since 
2011, South Africa has implemented a National Evaluation System (NES) governed 
by the National Evaluation Policy Framework (NEPF) as a component of the 
government-wide monitoring and evaluation system. Through the NES, located in 
the Presidency, the Department of Planning Monitoring and Evaluation (DPME) 
systematically evaluates programmes and policies of national priority, and has widened 
the reach to include provinces and departments by institutionalising mid-term and 
annual planning and budgeting allocation for evaluation (Goldman et al, 2018).
The Sustainable Development Goals (SDGs) include country-led systems, which 
measure the effectiveness, efficiency, relevance, sustainability, coordination, and 
impact of development interventions. Evaluation practice has a long history on the 
continent, but without an enabling policy environment it has not translated into a 
system which enables sound public-sector decision making based on the best available 
evaluation evidence. Supported by international development agencies and donors, 
evaluation has historically taken place outside government systems and has not shaped 
African governments’ reflection and learning. However, the institutionalisation of 
evaluation through NEPs is an opportunity for African governments to demand and 
use evaluative evidence, thereby creating a virtuous cycle that strengthens national 
evaluation capacities. Using examples from the region, this article aims to demonstrate 
how an NEP can foster an evaluation ethos in the public sector and set normative 
expectations, thereby creating a culture that is conducive to the demand for and use 
of evaluative evidence by policymakers and implementers.
This article explores the role that NEPs can play in public sector reform, and shares 
lessons on the policy development process. Certain process elements and features of 
policy development, such as high-level political support, have the potential to shape 
practices within the evaluation system. This article does not provide a detailed analysis 
of the political context in which evaluation policies are formulated and implemented 
or the contested landscape of evidence use in African public sector institutions. 
While these factors are important and interesting, this paper focuses on defining and 
articulating the opportunities and pitfalls of how evaluation policies can promote the 
use of evidence within public sectors. Mwaijande (2018) cautions that the absence 
of such policies can potentially leave programme and policy planning processes up 
to the whims of bureaucrats, thereby compromising the efficiency and effectiveness 
of development policies. However, much of the existing literature on evaluation 
policies and systems relates to the Global North. The context of development and 
public sector evidence use is different in Africa, where the emergence of evaluation 
systems has had a unique trajectory ( Basheka and Byamugisha, 2015 ;  Amisi and 
Chirau, 2019). For these reasons, it is important to specifically consider the process 
of policy development in African countries.
<<<PAGE=3>>>
Policies for evidence
3
We compare the approved NEPs of South Africa, Nigeria, Kenya (not approved but 
awaiting approval), Uganda, and Zimbabwe, examining the respective local elements 
of NEP , and highlighting key components that may be pertinent for countries in 
the region currently developing NEPs or considering ways to institutionalise and 
systematise monitoring and evaluation (M&E) through policy formulation, as has 
been done in Ghana, Ethiopia, Lesotho, Namibia and Tanzania. Because much has 
been written about the South African NES and policy, and one of the authors of 
this article was part of the team that helped to establish that system, the paper draws 
largely on the South African experience. The article uses Holvoet and Renard’s (2007) 
framework for an effective NES to explore the linkages between evaluation policies 
and the development evaluation systems in the various countries. The article further 
examines emerging lessons and key challenges in the development of NEPs and 
their implementation by means of an NES, and draws on action research approaches 
from the Centre for Learning on Evaluation and Results in Anglophone Africa 
(CLEAR-AA) to build the respective NESs across the region.
What is a National Evaluation Policy?
While there is no consensus on the definition of NEP , there is consensus in the 
literature on certain components. Hojlund (2015: 430) defines an NEP as ‘a systematic 
and institutionalised M&E framework in several interdependent organisational entities 
with the purpose of informing decision-making and securing oversight functions’. 
Hojlund adds that NEPs differ from other evaluation policies because they guide the 
evaluation practice for heterogeneous organisations collaborating to achieve common 
national development goals. This distinguishes NEPs from evaluation policies of 
multinational donor organisations like the United States Agency for International 
Development (USAID), Gesellschaft für Internationale Zusammenarbeit (GIZ), or 
the United Nations’ agencies, which may only apply to one bureaucratic system. 
There should be a direct link between NEP and NES, but this varies nationally and 
between institutions. As Naidoo and General (2010 ) point out, evaluation systems 
within the public sector are rarely unified; rather, policies guide a range of different 
systems and approaches to evaluation.
Unlike other public policies, such as the youth policy and poverty alleviation 
policies, NEPs tend to be limited to the conduct of public sector evaluations, with 
the exception of Uganda where attempts were made to coordinate evaluation 
practice across sectors through the government evaluation facility, but the power to 
shape practice in international development agencies is still limited. Most NEPs only 
provide guidance for evaluation within the public service. The 2011 South African 
NEPF , for example, did not prescribe how private sector or Non-Governmental 
Organisations (NGOs) should conduct evaluations, and jurisdiction was limited to 
national departments. The 2019 revised policy framework extended the reach of the 
policy to include parastatal entities. Although provincial government departments 
adopted the NEPF , questions were raised about whether a national cabinet-approved 
policy could be applied to other spheres of government within the context of a semi-
federal political system, as defined in the South African Constitution and governed by 
the Intergovernmental Relations Framework Act of 2005. The scope and application 
of NEPs require careful consideration as increasing numbers of countries move 
towards adopting NEPs. While there are clear drawbacks to a policy of limited scope,
<<<PAGE=4>>>
Takunda J Chirau et al
4
the experiences of the countries investigated suggest that policy interventions, even 
if limited in scope, appear to advance evaluation practice within the public service. 
This will be elaborated upon later (CLEAR-AA, 2019).
The absence of evaluation policies or systems does not necessarily hinder evaluation 
practice. This has been demonstrated in South Africa, where evaluations have been 
conducted by certain government departments, some of which, such as the National 
Department of Human Settlements, formulated their own M&E policy before the 
NEPF was passed (Chirau et al, 2018; Goldman et al, 2018). This is the case in Ghana, 
where evaluation practice is well established in the public sector, despite the absence 
of a prescribed national policy. However, the absence of a policy framework creates 
a vacuum. The public sector functions according to the rules imposed by policy 
and regulation. If public sector initiatives are unregulated or do not adhere to some 
form of regulatory or public sector management instrument, initiatives can easily be 
deprioritised, particularly if budgets are constrained. In addition, as experienced in 
South Africa, the absence of policy guidance on evaluation definitions and practices 
can result in significant variance in the outcomes of evaluation. Many evaluations are 
carried out that fail to meet acceptable definitions of evaluative evidence or rigour.
Compared to the establishment of other components of an NES, there is no correct 
sequence to be followed when a country is developing a NEP . In South Africa, 
for example, the evaluation policy was established prior to the formalisation of an 
evaluation system, while in Uganda, the NES was established prior to the development 
of policy (Goldman et al, 2018). This is a key lesson for countries establishing evaluation 
systems as it suggests that countries do not need to wait for the adoption of an NEP 
before institutionalising evaluation in the public sector. The authors of this paper are, 
however, cognisant of the fact that while NEP is important, is not the only means of 
institutionalising evaluation within the public sector.
In the public sector, policy position can be established through a number of 
instruments, including on the instruction of finance ministries, treasuries, public 
service administration ministries, and so on. Irrespective of the instrument, the same 
objectives – setting evaluation priorities, describing the plan, and formulating rules 
– can be achieved with these instruments as can be achieved with standalone policy. 
Using other public sector policy instruments governments can define evaluation, 
why it is needed, and how it fits into the public service management system, and can 
allow for budget to be allocated for evaluations. Therefore, though this paper focuses 
on standalone national evaluation or M&E policies, the term policy is applied in a 
broad sense.
Why national evaluation policies?
The spread of the New Public Management (NPM) approach in African governments, 
fuelled by the World Bank public sector reform initiatives and demands for aid 
accountability, are largely responsible for the proliferation of government M&E 
initiatives. The NPM, with its emphasis on outputs, outcomes, accountability, and 
transparency, has increased demand for M&E as a means by which governments can 
be informed on the investments they are making and whether investments ultimately 
fulfil their objectives (Basheka and Byamugisha, 2015).  As African countries emerged 
from civil wars and oppressive regimes they became more stable and citizens’ voices 
and demands became increasingly important. Organised civil society has emerged as
<<<PAGE=5>>>
Policies for evidence
5
an important third sector in different countries calling for government and private 
sector accountability and defending human rights. These various developments 
have collectively driven governmental demand for M&E evidence, leading to the 
establishment of monitoring systems largely for financial accountability.
Recently, the need for African governments to demonstrate the results of aid 
initiatives has increased the demand for evaluations, not just monitoring, across all 
spheres of government. Evaluations are a means by which policies, programmes, and 
projects are measured and assessed for effectiveness, impact, and efficiency (Dassah and 
Uken, 2006). The SDGs’ call is for evaluations to measure the effectiveness of initiatives 
geared towards addressing the Agenda 2030 prescripts ( Rugg, 2016). Mwaijande 
(2018: 17) argues that ‘when we talk of increasing demand and use of evaluations, 
we are faced with a chicken vs egg query, which one comes first? Should there be a 
NEP to guide demand for the use of evaluations? Or should there be a culture for 
evaluations that would demand a guiding policy?’ Evidence shows that the adoption 
of a formalised policy incentivises governments and increases the demand for and 
use of evaluative evidence, thereby building a culture of evaluation. In Zambia, the 
7th National Development Plan (NDP) is informed by the evaluation findings of 
the 6th NDP (Ministry of National Development Planning, 2017). In South Africa, 
the introduction of the NEPF consolidated previous efforts of different institutions 
such as the Public Service Commission and national departments, and increased the 
frequency of evaluations from sporadic to systemic, supported by capacity-building 
initiatives promoting demand for and supply of evaluations. Thus, the necessity for a 
policy becomes apparent when an evaluation system that has functioned in an ad hoc 
manner develops to the point that it requires greater clarity in scope and mandate 
(Chirau et al, 2020). Table 1 indicates the number of evaluations influenced by policy.
However, in countries still in the process of policy formation, there is vigorous 
debate among government ministries, departments, agencies, academia, think-
tanks, and development partners on whether an NEP weakens or strengthens the 
undertaking of evaluations (CLEAR-AA, 2019).
Comparative analysis framework
Holvoet and Renard’s (2007 ) six characteristics for effective NES provide the 
conceptual framework for this article, namely, policy, methodology, organisation, 
capacity, the participation of other actors, and the use of evaluation. They are illustrated 
in Figure 1 below.
Table 1: Evaluation influenced country’s national evaluation policy and system
Item Benin Uganda South Africa
Total number of national 
evaluations completed or underway 
as of 31 December 2016
15 (from 2010) 23 (from 2008) 56 + (from 
2012)
No. of evaluations started in 2016 1 4 8
Completed evaluation reports 14 14 32
Source: Goldman et al (2018: 6) Data supplied by government partners
+ includes 2 for 2017/2018
<<<PAGE=6>>>
Takunda J Chirau et al
6
This conceptual tool for evaluation systems, rather than policy analysis, was selected 
because this discussion concerns the ways in which the policy processes currently 
shape national systems.
Comparison and discussion on the five countries’ NEPs
Policy
Of the five countries studied, four NEPs have been approved by their respective 
cabinets. These are: South Africa (2011), Zimbabwe (2015), Uganda (2012/13) and 
Nigeria (2017). The titles of the policies differ; Kenya and Zimbabwe’s policies are 
titled the National Monitoring and Evaluation Policy, Uganda’s is titled the National 
Policy on Public Sector Monitoring and Evaluation, Nigeria’s NEP is titled the 
National Evaluation Policy, and South Africa’s is titled the National Evaluation Policy 
Framework (NEPF). These variations in title indicate that there are diverse purposes 
served by these policies. For example, some policies have a limited focus on evaluation 
and are, in fact, largely national monitoring, reporting, or performance management 
systems. In South Africa only evaluation is considered within the remit of the policy 
framework. These variations point to the important differences in national context 
that shape the use of evaluation evidence and its governing principles.
Annually, Uganda and South Africa, in accordance with their national development 
plans, formulate a cyclic evaluation plan which prescribes priority evaluations for 
that year ( Genesis, 2016), whereas other countries do not follow this process. All 
reviewed NEPs highlighted the need for transparency and impartiality, emphasising the 
importance of independent service providers to ensure that the results of evaluations 
are unbiased and objective.
South Africa, Zimbabwe, Uganda and Kenya’s policies have a system for providing 
feedback and dissemination of information on evaluations. However, dissemination 
of evaluation information remains largely ‘technocratic and can be enhanced to 
widen knowledge of evaluation results, in government, parliament and by the public’ 
(Goldman et al, 2018:6). In the five countries, the linking of evaluation, budgeting 
Figure 1: Characteristics of an effective National Evaluation System by Holvoet and Renard 
(2007), arranged by Jamie Robertson of Genesis Analytics, 2016, used with permission.
1 Policy The evaluation plan M vs. E Autonomy & 
impartiality Feedback Alignment planning 
& budgeting
2 Methodology
Selection of results 
areas to be 
evaluated
Priority setting Causality chain Methodologies 
used Data collection
3 Organisation Coordination& 
oversight Statistical oﬃce Line ministries Decentralised levels Link with 
interventions
4 Capacity Problem 
acknowledged
Capacity building 
plan
5
Participation 
of Other 
Actors
Parliament Civil society Donors Private sector
6 Use Eﬀective use of
evaluation
Internal usage of 
evaluation /f_indings
<<<PAGE=7>>>
Policies for evidence
7
and planning remains a challenge, as the compilation of evaluation information is 
time consuming and planning is done in silos.
Methodology
South Africa and Uganda’s evaluation policies state which types of evaluation should 
be utilised in their M&E systems; Zimbabwe, Kenya and Nigeria are silent on this. 
South Africa’s policy outlines six types of evaluation – diagnosis, synthesis, design, 
implementation, process and impact. The Ugandan policy mentions mid-term, 
impact, summative, and ex ante or baseline evaluations. This indicates the emergent 
professionalism of evaluation practices, and the different ways that vocabulary in the 
evaluation field is standardised. In the South African policy, sources of data collection 
are primarily quantitative, qualitative, or mixed-research methodologies. In most 
cases, programmes are implemented without theories of change, logic models or 
results chains, although both South Africa and Uganda now require these for all 
new programmes. Currently, these are retrospectively developed for evaluations. This 
suggests that both South Africa and Uganda are developing policies that apply to both 
the supply and demand aspects of the evaluation sector, while the other countries 
restrict their focus to public sector consumption and managing of evaluations.
It is important that the NEP identify what is to be evaluated. In South Africa, 
Uganda, and Zimbabwe, the NEPs are aligned to and prioritised according to the 
NDP of each respective country. The DPME in South Africa prioritises 14 outcomes1. 
In Uganda, areas requiring evaluation are identified at the beginning of each budget 
cycle by departments, agencies and ministries in alignment with government priority 
areas. The benefits of assessing the types and objectives of evaluations to be undertaken 
are that the assessment process institutionalises evaluations throughout the government 
and standardises evaluation terminologies.
Organisation
Evaluation practice is enhanced by both the NES and policy. The policies clarify 
the roles and responsibilities of different organisations. Lopez-Acevedo et al (2012: 
8) argue that institutional arrangements differ with respect to the overall leadership 
and centralisation of systems. However, commonalities exist among the policies, 
for example, when the system is managed by a central oversight and coordination 
office within the executive. This demonstrates political and administrative support 
for evaluations and utilisation of evaluation evidence. In South Africa, the DPME 
within the Office of the Presidency and Cabinet is the custodian of governmental 
evaluation. In Zimbabwe, responsibility for M&E rests with the Office of the 
President and Cabinet (OPC). In Uganda, the Office of the Prime Minister (OPM) 
coordinates and oversees M&E activities. In Kenya, the Monitoring and Evaluation 
Department (MED) falls under National Treasury and Planning and plays a crucial role 
in coordinating M&E activities. In Nigeria, the National Monitoring and Evaluation 
Department falls under the Ministry of Budget and National Planning (MBNP). In 
many cases, ministries, departments, and agencies also have their own M&E units 
which are responsible for day-to-day activities.
There are advantages and disadvantages for both centralised and decentralised 
evaluation systems. In centralised systems such as in South Africa and Uganda, the
<<<PAGE=8>>>
Takunda J Chirau et al
8
ministry or institution managing the system controls the procedural framework, 
methodology, and quality assurance of the evaluations. Without centralisation and 
standardised processes, the system will be fragmented ( Genesis, 2016), as has been 
seen in evaluation systems within departments and agencies which have their own 
standards and guidelines, but lack horizontal integration. The DPME and OPM have 
been successful in this regard. Day-to-day operations are undertaken by ministries, 
agencies, and sectors. Decentralisation, however, enables ministries, sectors and agencies 
to own and facilitate the use of evaluation work, as opposed to evaluations being 
regarded as a practice imposed externally by the central institution (Lopez-Acevedo 
et al, 2012; Ahonen, 2015).
Capacity
All five countries report a lack of human resources and financial capacity to support 
policy implementation. While all countries reviewed have M&E officers and M&E 
units, the emphasis of these units is largely on monitoring and not evaluation (Porter 
and Goldman, 2013). In the public sector, there is often little or no availability of 
experience in conducting programme and policy evaluations, though some capacity 
for research and policy analysis exists. The dearth of technical evaluation specialists in 
the government sector is common to all the reviewed countries. Evaluation policies 
focus explicitly on government staff’s function of managing evaluations rather than 
conducting them (Goldman et al, 2018). Furthermore, capacity-building initiatives 
focus on strengthening the commissioning and management of public sector 
evaluations, as opposed to the practice itself (Morkel and Ramasobana, 2017). While 
this is a step in the right direction, a recent study indicated that gaps in the South 
African government’s evaluation resources were not linked to inappropriate skills, but 
rather to ineffective procurement systems that hamper access to these skills (Philips, 
2018). A policy can provide guidance and structure for national evaluation capacity 
development efforts. Capacity-strengthening initiatives have been implemented in 
Uganda where the policy delegates the responsibility for capacity building to the 
Ugandan Evaluation Association and the Ugandan Management Institute. In South 
Africa, the National School of Government, universities, and the South African 
M&E Association offer evaluation capacity building in line with the NEPF . Despite 
capacity-building initiatives in the various countries, evaluation capacity remains a 
challenge, with the bulk of evaluations being undertaken by external consultants.
The policies of South Africa, Kenya and Uganda indicate that there is a need for 
a budget for evaluations, without which implementation of NEPs is problematic. In 
Uganda, when government budgets are inadequate, the government and the use of a 
‘basket of funding’ make decisions on donor resources from donors, and government 
ensures that no single donor influences an evaluation (Goldman et al, 2018:7).
Participation of other actors
The NEPs reviewed acknowledge that successful systems require the support of a 
wide range of stakeholders with assigned roles and responsibilities. South Africa, 
Kenya and Uganda’s policies refer to statistics agencies, for example, the Uganda 
Bureau of Statistics, Kenya Bureau of Statistics and Statistics South Africa, as being
<<<PAGE=9>>>
Policies for evidence
9
the primary sources of credible and quality data. The NEPs of South Africa, Kenya, 
and Uganda emphasise the critical role that the ministry of finance plays in M&E 
and the improvement of feedback mechanisms between budget alignment, planning, 
and learning. This helps to establish an enabling approach for the implementation of 
evaluation recommendations by streamlining processes and providing the budget for 
implementation (Lopez-Acevedo et al, 2012: 10). Lopez-Acevedo et al (2012) argue 
that inclusion allows for independence and minimises the political conflict that often 
arises when one actor implements the system and policy.
The role of non-state actors, for example Civil Society Organisations (CSOs) and 
institutions of higher learning, cannot be underestimated. CSOs play a significant 
role in generating M&E data, however, the use of the data by government ministries 
to improve performance and policymaking is limited, and this was the case in all 
reviewed countries. CSOs act as watchdogs that monitor government accountability to 
citizens and this often leads to mistrust and suspicion. In 2018 and 2019, CLEAR-AA 
conducted workshops in Kenya, Uganda, South Africa and Ghana with the aim of 
strengthening the CSO and government interface by capitalising on their respective 
strengths and encouraging collaboration to improve development outcomes and 
advance social change; their mutual engagement was considered critical by all 
participants.
Quality and use
If evaluations aim to influence and shape the behaviours of policymakers and 
programme implementers, the findings should be widely shared. Despite consensus 
on the importance of disseminating information, Nigeria and Uganda’s policies fail 
to indicate how evaluation findings are to be disseminated. A policymaking cycle is 
complex and, as such, it is important that communication does not only occur at the 
end of the project. The South African NEPF makes it mandatory for all evaluations 
in the National Evaluation Plans to be presented to Cabinet (DPME, 2011), which 
gives weight to implementation for influencing policy and decision making. The 
NEPs of Zimbabwe, Kenya and South Africa state that evaluation information should 
be posted on websites for public consumption.
Political buy-in and ownership of evaluation evidence are important in enforcing 
evaluation recommendations. Briceno (2012: 38) argues that ‘the extreme enforcement 
version, the M&E body has direct power over the evaluation agenda and enjoys a 
prominent position and support from the legislature’. This is the case in South Africa, 
Uganda, and Nigeria. Decision making that is informed and corroborated by reliable 
evaluation evidence enhances the chances of success of policy and programme 
interventions (Rubio, 2012: 133). The policies reviewed have formulated mechanisms 
to ensure quality and credibility and these include evaluation steering committees, 
use of experienced consultants, and standardised guidelines. In South Africa there 
is a quality assessment system which scores the completed evaluations ( Leslie et al, 
2015), the intention of which was to reassure Cabinet that evaluations met acceptable 
standards. Such mechanisms not only ensure methodological rigour but also outline 
evaluation principles, standards and ethics and provide templates for reporting on 
evaluation studies.
<<<PAGE=10>>>
Takunda J Chirau et al
10
Emerging lessons and key challenges
As evaluation policies and plans proliferate in Africa, the emerging lessons can be 
harnessed to better understand the extent to which evaluation policies can support 
the systematic use of evidence in decision making.
Learn, but avoid mimicry 
Countries reviewed have learned mutual lessons during their journeys of developing 
and implementing their NEPs. For example, the T wende Mbele programme was 
established to foster and encourage peer learning between African countries 
institutionalising evaluation. However, the process of learning should be distinguished 
from mimicry. Countries should develop a policy that is relevant and responsive in 
the context of their own country. This includes recognising capacities, government 
incentives and disincentives to use evidence, resources available, and the structure and 
institutional arrangements of government. Failure to do so can result in a futile policy.
Political and administrative support is vital 
As with any policy, political and administrative support is important to ensure that the 
policy shapes behaviour and incentivises meaningful demand and use of evaluative 
evidence. The development of an NEP requires political support, buy-in, ownership 
and inclusivity to prevent the policy becoming yet another layer of compliance and 
programme policing. Political support can be built/cultivated through the use of 
political champions.
Demonstrate the value of evaluation
The value of evaluation is not always apparent to those who are unfamiliar with 
evaluation or who have been implementing programme/policies without evaluative 
evidence. Therefore, to prompt a culture shift towards evaluative evidence, the value 
of the evidence must be clearly demonstrated and this can be achieved by promoting 
evaluation and encouraging buy-in (Amisi and Chirau, 2019). Therefore, it is important 
to carry out strategic evaluations even before a policy is complete and to demonstrate 
the model while building the elements of the policy. Goldman et al (2018:9) argue 
that ‘having the policy in advance (as South Africa did, but Uganda and Benin did 
not) does not seem necessary although there needs to be some definition of how the 
system will work, how it will provide for impartiality, etc.’. The culture of evaluation 
and recognition of the value-add of evaluation precedes the actual establishment of 
the policy.
Take a participatory approach 
Partners are critical to sustain evaluation systems. Key government ministries, civil 
society organisations, and academic institutions are needed to manage a NES. Partners 
could potentially support, benefit, or hinder the process of establishing a national 
evaluation policy; partnerships should be built incrementally and be based on need.
<<<PAGE=11>>>
Policies for evidence
11
Partners, who might not initially see their role in the system, may understand their 
role more clearly as the system matures and begins to conduct evaluations.
The reviewed countries face common key constraints. Firstly, there is a lack of 
evaluators in the public sector with the ability to conduct, commission, and manage 
evaluations. Evaluations are often outsourced to practitioners outside the government 
sector who are internal or external to the country. This poses potential problems on 
how evaluations are defined. This would be the case if western norms and standards are 
applied during programme evaluations in Africa, and would ultimately be impractical 
and costly in the context of many countries in the region (Amisi and Chirau, 2019). 
Countries need to be open to explore institutionalisation of other evaluative evidence 
while committing to systematically invest in strengthening national evaluation capacity. 
Institutionalisation efforts need to be cognisant of what is happening outside of 
government. Information obtained from a CLEAR-AA African evaluation database 
indicates that most evaluations are conducted by international organisations with 
governments frequently unaware of these evaluations and what can be learnt from 
such evaluations.
Secondly, public sector administration focuses predominantly on monitoring 
and is generally unmoved by sound rationale for including evaluation in the policy. 
Monitoring usually takes place during implementation and focuses on inputs, activities, 
and outputs, rather than on results which could improve effectiveness, efficiency, 
and impact. This is partly because monitoring systems are often primarily geared to 
respond to questions of fiscal accountability rather than organisational learning and 
programme adaptation.
Thirdly, the role of CSOs in policy implementation is limited, as CSOs are not able 
to access the information generated by the public sector M&E with which it could 
hold government accountable for its decisions and performance.
Conclusion
A policy is an important vehicle through which governments can influence demand 
and promote the use of evaluative evidence for decision and policymaking. Evaluations 
are likely to improve governance, transparency, accountability, and effectiveness in 
public sector programming. This article compared the policies of South Africa, 
Zimbabwe, Uganda, Kenya and Nigeria. It is clear from the analysis of the policies 
that the structures are largely similar, all having the ultimate purpose of structuring, 
systematising and institutionalising evaluation practice across the government sector. 
The policies are influenced by different factors: national contexts, technical expertise 
available and reform strategies. Each country, however, has taken steps to systematise 
processes of evaluation within the public sector, as well as create coherence within 
existing processes of procurement management, or results-based planning. Peer-to-
peer learning has become important in African countries, particularly those that share 
democratic or developmental similarities. Countries such as Niger and Ghana, which 
are in the process of developing NEPs, can learn from the experiences of South Africa, 
Zimbabwe and Uganda. When developing NEPs, it is critical to understand public 
administration structures, the political economy of evidence use, and the cycles of 
policy development and implementation. For African countries to build country-led
<<<PAGE=12>>>
Takunda J Chirau et al
12
NEPs, a thorough knowledge of existing policies is essential to ensure a wider spread 
of evaluation systems which align with national state architecture and development 
needs. It is not clear from the policies how the demand and use of evidence can 
be strengthened. However, there are entry points which can be strengthened, for 
example improving the nexus between M&E systems and parliamentary evidence 
use systems. The requirement that evaluations must be submitted to Parliament would 
help to inform Parliament and increase the demand for evaluations for oversight 
and legislation purposes ( Chirau et al, 2020). It is key that capacity-strengthening 
initiatives be targeted at senior administrative personnel and parliamentarians who 
make decisions to enable them to ask cogent policy questions, to demand evaluative 
evidence, and to interpret and understand evidence (any type of evidence including 
evaluations), thereby increasing the probability of use of evaluations.
Notes
 1  https://www.gov.za/issues/outcomes-approach
Acknowledgements
We are grateful to Partnersintime for editing the final manuscript.
Research ethics statement
The authors of this paper have declared that research ethics approval was not required 
since the paper does not present or draw directly on data/findings from empirical research.
Contributor statement
TC led the conceptualisation and writing of the first and subsequent drafts of the 
manuscript, CBM responded to the comments from reviewers, and MA further responded 
to other comments towards the finalisation of this article.
Conflict of interest
The authors declare that there is no conflict of interest.
References
Ahonen, P . (2015) Aspects of the institutionalization of evaluation in Finland: basic, agency, 
process and change, Evaluation, 21(3): 308–24. doi: 10.1177/1356389015592546
Amisi, M.M. and Chirau, T.J. (2019) Strengthening regional evaluation capacity-
building through local partnerships: emerging issues and recommendations 
from CLEAR-AA’s 2018 regional strategy workshops, https://hdl.handle.
net/10539/28480.
Basheka, B.C. and Byamugisha, A.K. (2015) The state of monitoring and evaluation 
(M&E) as a discipline in Africa: from infancy to adulthood? African Journal of Public 
Affairs, 8(3):75–95.
Briceno, B. (2012) Defining the type of M&E system: clients, intended users and 
utilisation, in G. Lopez-Acevedo, P . Krause and P . Mackay (eds) (2012) Building 
Better Policies: The Nuts and Bolts of Monitoring and Evaluation Systems, Washington, 
DC: The World Bank.
<<<PAGE=13>>>
Policies for evidence
13
Chirau, T.J., Waller, C. and Blaser-Mapitsa, C. (2018) The National 
Evaluation Policy landscape in Africa: a comparison, Policy Brief for 
T wende Management Committee, https://www.africaportal.org/publications/
national-evaluation-policy-landscape-africa-comparison/
Chirau, T.J., Goldman, I., Watera, J., Gounou, A., Sossou, D. and Segla, E. (2020) 
Chapter X: Links between National Evaluation Systems and Parliaments – current 
practice, future potential, Working Paper for a book chapter.
CLEAR-AA (2019) Compass: Tracking Monitoring and Evaluation Developments in 
Anglophone Africa, Johannesburg: University of the Witwatersrand.
Dassah, M.O. and Uken, E.A. (2006) Monitoring and evaluation in Africa with 
reference to Ghana and South Africa, Journal of Public Administration, 41(4): 705–20.
DPME (Department of Performance Monitoring and Evaluation) (2011) National 
Evaluation Policy Framework, Pretoria: DPME.
Genesis (2016) Evaluation of the National Evaluation System in South Africa, International 
Benchmarking and literature review, Draft Report, Pretoria: DPME.
Goldman, I., Byamugisha, A., Gounou, A., Smith, L.R., Ntakumba, S., Lubanga, 
T., Sossou, D. and Rot-Munstermann, K. (2018) The emergence of government 
evaluation systems in Africa: the case of Benin, Uganda and South Africa, African 
Evaluation Journal, 6(1): 1–11. doi: 10.4102/aej.v6i1.253
Hojlund, S. (2015) Evaluation in the European Commission: for accountability 
or learning?, European Journal of Risk Regulation , 6(1): 35–46. doi: 10.1017/
S1867299X00004268
Holvoet, N. and Renard, R. (2007) Monitoring and evaluation under the PRSP: Solid 
rock or quicksand?, Evaluation and Program Planning, 30(1): 66–81 . doi: 10.1016/j.
evalprogplan.2006.09.002
Leslie, M., Moodley, N., Goldman, I., Jacob, C., Podems, D., Everett, M. and Beney, 
T. (2015) Developing evaluation standards and assessing evaluation quality, African 
Evaluation Journal, 3(1): 1–13,  https://doi.org/10.4102/aej.v3i1.112. doi: 10.4102/
aej.v3i1.112
Lopez-Acevedo, G., Krause, P . and Mackay, P . (2012) Building Better Policies: The Nuts 
and Bolts of Monitoring and Evaluation Systems, Washington, DC: The World Bank.
Ministry of National Development Planning (2017) 7th National Development Plan 
2017–21, Lusaka: Government of Zambia.
Morkel, C. and Ramasobana, M. (2017). Measuring the effect of evaluation capacity 
building initiatives in Africa: a review, African Evaluation Journal , 5(1): a187. doi: 
10.4102/aej.v5i1.187
Mwaijande, F . (2018) Why should countries have National Evaluation Policies?, 
Building Supply and Demand for Evaluation in Africa , 1, https://idev.afdb.org/en/
document/building-supply-and-demand-evaluation-vol-1
Naidoo, I. and General, D.D. (2010) Monitoring and evaluation in South Africa: many 
purposes, multiple systems, in M. Segone (ed) From Policies to Results: Developing 
Capacities for Country Monitoring and Evaluation Systems , New Y ork: UNICEF , pp 
303–20.
Phillips, S. (2018) Diagnostic on the demand and supply of evaluation, T wende Mbele, 
21 January.
Porter, S. and Goldman, I. (2013) A growing demand for monitoring and evaluation 
in Africa, African Evaluation Journal, 1(1): 9. doi: 10.4102/aej.v1i1.25
<<<PAGE=14>>>
Takunda J Chirau et al
14
Rubio, G.M. (2012) Key Choice for a menu of evaluations, in G. Lopez-Acevedo, 
P . Krause and P . Mackay (eds) (2012) Building Better Policies: The Nuts and Bolts of 
Monitoring and Evaluation Systems, Washington, DC: The World Bank.
Rugg, D. (2016) The role of evaluation at the UN and in the new sustainable 
development goals: towards the future we want, Global Policy, 7(3): 426–30. doi: 
10.1111/1758-5899.12346