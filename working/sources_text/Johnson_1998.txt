<<<PAGE=1>>>
EVALUATION 
and PROGRAM PLANNING 
PERGAMON Eculuation and Program Planning 21 (1998) 93-l 10 
Toward a theoretical model of evaluation utilization 
R. Burke Johnson 
University of South Alabama, College qf’Education, BSET. UCOM 3700, Mobile, AL 36688, U.S.A. 
Abstract 
Implicit evaluation utilization process-models were constructed from evaluation theorists, ideas, and explicit evaluation utilization 
process-models (i.e. already developed models) were located in the literature. The meta-model (i.e. a model developed from other 
models) was developed from the implicit and explicit process-models and from important ideas reported in recent research on 
evaluation use (e.g. participation, organizational development and complexity). The model depicts evaluation use as occurring in an 
internal environment situated in an external environment. The three sets of variables in the theoretical model are the background 
variables, the interactional or social psychological variables and the evaluation use variables. It is contended that evaluation-for-use 
will result in longer term effects when ideas from complexity theory, organizational learning and organizational design are employed. 
The meta-model reported here should be viewed as a theoretical model, offered in an attempt to promote theory development in the 
evaluation utilization literature. 0 1998 Elsevier Science Ltd. All rights reserved. 
Keywords: Evaluation utilization; Evaluation use; Evaluation theory; Meta-modeling. 
One area of interest in program evaluation since its mod- 
ern inception has been evaluation utilization. For exam- 
ple, some popular questions have included: How are 
evaluations used?; Why are they used?; What are the 
outcomes of program evaluations?; and What factors 
affect whether or not persons and organizations will actu- 
ally use the results of evaluations? The evaluation uti- 
lization literature has been extensive (Alkin et al., 1979; 
Alkin, 199 1; Cousins and Earl, 1992; Patton, 1978; Stake, 
1990; Weiss, 1984) and slow but steady progress has been 
made in our understanding of evaluation use. 
One weakness of the research literature on evaluation 
utilization is that while many lists of variables that affect 
use are provided (Boyer and Langbein, 199 1; Cousins 
and Leithwood, 1986; Leviton and Hughes, 1981), few 
theoretical process-models have been developed that inte- 
grate variables into systems, showing the inter- 
relationships among variables. There is a need for 
additional research that integrates the past literature and 
offers more holistic and process-focused propositions and 
conclusions. Not surprisingly, many evaluators have sug- 
gested the need for theories and/or models of evaluation 
and evaluation utilization (Alkin, 1991; Chen, 1990; Chen 
and Rossi, 1992, 1989; Cousins and Leithwood, 1993; 
Greene, 1988; Huberman and Cox, 1990; Johnson, 1993, 
1995; Shulha and Cousins, 1996; Wang and Walberg, 
1983). 
The purpose of this paper is to address the need for 
SOl49%7189/98 $19.00 ;c 1998 Elsevier Science Ltd. All rights reserved. 
PII: SO 149-7 189(97)00048&7 
literature integration and theory development. After pro- 
viding definitions of the different types of evaluation uti- 
lization and describing the ‘meta-modeling’ approach 
used in this research study, the research on evaluation 
use is examined. Implicit models of evaluation use are 
developed from the writings of several prominent eva- 
luators, explicit models (i.e. already published models) 
are identified in the literature and reviewed and an inte- 
grative theoretical model is provided in the hope of fur- 
thering the development of theory in the evaluation 
utilization literature and community. The theoretical 
model is abstract enough that it can refer to processes 
that occur in many places. The model is also specific 
enough that it can be used to make predictions and sug- 
gest how to facilitate evaluation utilization at the local 
level. 
1. Types of utilization 
Instrumental, conceptual, process and symbolic use 
are the most common types of evaluation utilization. 
Instrumental use, perhaps the earliest type of use exam- 
ined in the literature, refers to using evaluation findings 
as a basis for action. It refers to concrete use, to behavior 
or action, or to ‘making direct decisions about changing 
programs based on evaluation results’ (Shadish et al., 
1991). Examples of instrumental use include eliminating
<<<PAGE=2>>>
94 R. Burke JohnsonlELduation und Program Planning 21 (1998) 93-I IO 
a program shown to be ineffective, modifying a program 
based on an evaluation, targeting a program to new audi- 
ences, allocating new budget outlays for a program and 
changing the structure of the organization in which a 
program operates. 
Conceptual use, also called ‘enlightenment’, occurs 
when an evaluation influences decision makers’ and 
stakeholders’ cognitive processing (thinking) about a pre- 
sent or future program (Owen, 1992). Weiss (1980) sug- 
gests that over time ‘decision accretion’ occurs, where 
experiences with and thinking about past evaluations 
accrue and influence current decision making. Some 
examples of conceptual use are becoming aware of evalu- 
ation results, becoming aware of features of a program 
from an evaluation, forming attitudes about a particular 
program because of an evaluation and developing 
opinions, attitudes and knowledge about evaluation in 
general. 
A third kind of use is called process use. Process use 
occurs when behavioral and cognitive changes occur in 
persons involved in evaluations as a result of their par- 
ticipation (Patton, 1997; Preskill and Caracelli, 1996; 
Shulha and Cousins, 1996). It results from experiential 
learning and reflection. Patton (1997) defines process use 
as ‘using the logic, employing the reasoning and being 
guided by the values that undergird the profession’ (p. 
88). Process use involves learning to think like an eva- 
luator and it may have long term payoffs through 
improved skills, improved communication, improved 
decision making, increased use of evaluation procedures, 
changes in the organization and increased confidence in 
and sense of ownership of evaluation products. Process 
use partially overlaps with instrumental and conceptual 
use. 
A fourth kind of use is, symbolic use, which occurs 
when individuals use evaluation information for political 
self-interest. The symbolic user has ulterior motives for 
involvement with the evaluation. Pelz (1978) has long 
contended that this kind of use is common. This kind of 
use has also been called ‘conspiratorial’ use (Huberman, 
1987). Two related kinds of use are legitimative use, 
which refers to uses of evaluations that ‘justify decisions 
already made about the program’ (Owen, 1992) and per- 
suasive use, in which individuals use evaluations as part 
of the political process to advocate issues and persuade 
people to act (Patton, 1997). 
2. Meta-modeling 
Meta-modeling is a term I coined to refer to developing 
models from models.’ I view meta-modeling as an induc- 
’ Owen (1992) used the term meta-model to refer to a model he 
developed. Building on Owen, I use the term ‘meta-modeling’ to refer 
to the process of developing a model from other models as described in 
this section. 
tive theory-building approach using a specific data 
source, namely published research studies. Implicit pro- 
cess-models are constructed (i.e. models describing 
researchers’ ideas about a phenomenon are constructed 
during a review of the literature) and explicit process- 
models (i.e. already depicted models) are located in the 
literature and added to the growing set of models. Then, 
systems of concepts, categories, and variables are pulled 
from the implicit and explicit models and are integrated 
into a new, holistic meta-model. Links between the vari- 
ables or concepts should be based on prior research when- 
ever possible. Researchers can also add concepts or 
variables into their meta-models that are either missing 
from published literatures or were present but were not 
included in the implicit or explicit models. The process 
thus involves a mixture of empiricism and rationalism. 
The theoretical process-model developed for this article 
is a meta-model and is shown in Fig. 19. 
After an initial meta-model is developed, model con- 
struction and refinement continue until the researcher is 
satisfied that the best model has been obtained. During 
the process, new variables may be added and others may 
be deleted. The rule of parsimony may be invoked. 
Researchers should refine the model by reflecting and 
returning to the literature and examining the models in a 
new light. They may also look at related models and 
concepts that were not considered relevant during earlier 
runs. Each time a developing meta-model is changed, it 
should be checked for its ‘fit’ with theoretical and empiri- 
cal results reported in the literature. That is, Is the model 
‘grounded in’ the original data? (see Strauss and Corbin, 
1990 for a discussion of grounded theory and the idea of 
model ‘fit’). I think the model should also be checked for 
its ‘viability’; Does the model stand alone as an effective 
and useful model?; Does the model seem to explain its 
domain of interest?; Does the model perform well com- 
pared to other currently available models?; and Can 
someone make predictions using the model? The model 
may also be subjected to journal referees and also to the 
general research community through publication. Over 
time, a published model will probably be modified again 
and new models will be published that challenge the orig- 
inal model. 
Meta-modeling, as it is defined here, has some simi- 
larities to the type of traditional theory building and 
testing called ‘causal modeling’ in that models are 
developed from previous theoretical and empirical results 
(Asher, 1983; Blalock, 1969; Davis, 1984). However, the 
traditional causal modeling approach focuses more on 
model testing (confirmation) than on model discovery 
and development. The three traditional criteria for estab- 
lishing causality in the causal modeling approach should 
also be used when developing a meta-model. They are (1) 
checking for association or covariation to establish the 
existence of a relationship between variables, (2) deter- 
mining the temporal ordering of variables (usually from
<<<PAGE=3>>>
R. Burke Johnson/Evaluation and Program Planning 21 (1998) 93-110 9s 
previous theory, but also through several statistical and 
design techniques) to establish the direction of the causal 
relationship and (3) making attempts to eliminate the 
viability of alternative explanations of the observed 
and/or theoretical relationships. This means that third 
variables are either quantitatively or qualitatively ‘elim- 
inated’ in order to establish that the variables are causally 
related rather than spuriously or coincidentally related. 
The meta-modeling approach, as defined here, is very 
similar to the grounded theory approach popular with 
qualitative researchers (Glaser and Strauss, 1967; Strauss 
and Corbin, 1990). The strategy in grounded theory and 
meta-modeling is to inductively search for categories, to 
describe their properties and dimensions and to order 
the categories. The meta-modeling approach produces 
models that are ‘grounded’ in the research literature. 
The key difference between grounded theory and meta- 
modeling is that the data sources differ. Grounded the- 
ories are usually developed using original qualitative 
data. Meta-models, as defined here, are developed from 
implicit and explicit models that are based on qualitative 
or quantitative data or from purely theoretical models 
not directly based on empirical data. Meta-modeling also 
has some similarity with the technique called ‘meta-eth- 
nography’ where ethnographers construct integrative 
ethnographies based on other published ethnographies 
(Noblit and Hare, 1988). 
3. Implicit evaluation utilization process-models 
As part of the meta-modeling approach, implicit pro- 
cess-models were identified in the literature and from 
these, process-models were constructed. In an implicit 
process-model, variable ordering or process is implied 
but is not directly depicted by the evaluator. In the pre- 
sent study, simplified versions of the implicit process- 
models of several prominent evaluators are given. My 
goal was to draw pictures (i.e. visual models) based on 
the evaluators’ discussions of evaluation utilization that 
are suggestive of their key ideas. In this way we can see 
what their models might look like and we can compare 
their models with the explicit models that are provided 
by other evaluators. It is inevitable that the implicit mod- 
els will not do full justice to the evaluators’ work. For 
example, feedback is not shown in any of the models. To 
learn more about these evaluators’ views on evaluation 
use, the reader is directed to the literature published by 
them. 
The first theorist reported who has an implicit process- 
model is Campbell (1963, 1969). Campbell contends that 
the major responsibility for use of evaluations lies in the 
political process, not with the evaluator. His ‘evol- 
utionary epistemology’ predicts that over time the better 
programs will survive and society will be improved 
(Campbell, 1984, 1988). The evaluator is viewed as a 
Fig. 1. Campbell’s implicit process-model. 
scientist conducting evaluations with the best methods 
possible (especially, but not limited to, ‘true’ experiments; 
Campbell (1988)). She or he does not, however, directly 
promote the use of findings. Campbell sometimes seems 
to optimistically assume that evaluations will be used 
when they are well done. The Campbell model might be 
labeled the traditional model of evaluation use. The 
model does not delineate very clearly what factors result 
in evaluation use. A depiction of Campbell’s ideas is 
shown in Fig. 1. 
Striven (1974, 1983) has implied a summative or ‘con- 
sumer reports’ type model of evaluation in which the 
evaluator examines the comparative strengths and weak- 
nesses of a program and makes a final judgement of 
worth-is the program ‘good’ or is the program ‘bad’? 
Consumers of programs are like consumers of other prod- 
ucts, making rational choices. While Striven also advo- 
cates using many additional ideas from the utilization 
literature (Striven, 1991), only his consumer reports 
implicit process-model is presented in Fig. 2. 
Weiss (1984) perhaps more than any other prominent 
theorist, suggests that little instrumental use occurs. She 
contends that most use is through enlightenment. Over 
time decision accretion takes place, that is: 
Policies are not made at a single point in time; they 
seem to happen as the result of gradual accretions, 
the build-up of small choices, the closing of small 
options and the gradual narrowing of available 
alternatives. (Weiss, 1976, cited in Shadish et al., 
1991, p. 192). 
This important type of use is difficult to measure because 
it is conceptual; it is not tangible. 
Weiss (1983) offers an interesting implicit process- 
model of decision making at the individual level of analy- 
sis called ‘I-I-I Analysis’. She says decisions are the result 
of three major influences: (1) information, (2) ideology 
and (3) interests. The influence of these three factors is 
tempered by the organizational environment in which the 
person resides (Alkin, 1990). Furthermore, when making 
a decision, decision makers conduct ‘truth tests’ (i.e. does 
FHEtBEHZi 
Fig. 2. Striven’s implicit process-model.
<<<PAGE=4>>>
R. Burke Johnson/Evaluation and Program Planning 21 (1998) 93-110 
Fig. 3. Weiss’s implicit process-model 
it conform to prior knowledge?) and ‘utility tests’ (i.e. is 
the evaluation feasible and action oriented?) (Weiss and 
Bucuvalas, 1980). 
A depiction of the process described by Weiss was 
constructed for this research. It is shown in Fig. 3. 
Wholey (1983, 1985) has written extensively about 
instrumental use. He suggests that evaluation should 
directly serve the needs of management. If the potential 
for use of an evaluation does not exist (which he would 
determine from an ‘evaluability assessment’) then the 
evaluation should not be done. Wholey (1985) offers 
advice on how to manage programs and how to use 
evaluative information as part of management. In other 
words, his focus is on instrumental use (i.e. immediate 
tangible use) through effective management. He does not 
question whether a program meets social needs or not; 
he allows managers and policy makers to make that 
decision. Wholey’s focus is on program improvement and 
effective management and it is the evaluator’s job to work 
with management to improve programs. His approach to 
management is quite compatible with modern organ- 
ization development theory (Carnall, 1990; Harvey and 
Brown, 1992). 
Wholey infrequently recommends use of the rigorous 
experimental methods advocated by Campbell (1969, 
1988) for program evaluation. Wholey contends that pro- 
grams are usually characterized by limited resources and 
purposes. Expensive evaluations cannot always be done. 
Wholey (1979), therefore, recommends a ‘sequential pur- 
chase of information’. This involves, in order of increas- 
ing expense, the following: (1) Evaluability assessment, 
(2) Rapid feedback assessment, (3) Performance moni- 
toring and (4) Intensive evaluation. Each of these could 
probably be modeled. In general, however, a model of 
Wholey’s ideas about evaluation use might look like the 
one given in Fig. 4. 
Stake’s favorite research method is the qualitative case 
study (Stake, 1981, 1995). His approach to evaluation, 
/III 
EvaluaMity EvaMica ChiVQ#l CMlblluxa 
buessment 7Men$tkn_ managen’ mw 
llsB 
Fig. 4. Wholey’s implicit process-model. 
called ‘responsive evaluation’, basically means orienting 
evaluations to program activities and stakeholders’ needs 
(Stake, 1975). Through a kind of participatory approach 
to evaluation, a qualitative report based on detailed 
description is produced. The evaluator tells a story and 
after reading this story (i.e. the report), readers should be 
able to make naturalistic generalizations (Stake, 1990). 
That is, readers are said to vicariously experience a pro- 
gram and then generalize, based on their experience, to 
other environments, people and programs-this is a kind 
of conceptual use. The model developed from Stake’s 
implicit process-model of use is depicted in Fig. 5. As 
shown in the model, Stake believes that use of the case 
study method ultimately increases evaluation utilization. 
In contrast to Campbell (1986) who emphasizes local 
molar causal validity and Striven (1980, 1983) who 
emphasizes summative evaluation, Cronbach (1982) con- 
tends that it is the evaluator’s job to show in detail the 
processes going on in a program. The main reason for 
doing this is generalization. Cronbach talked about, for 
example, aptitude-treatment-interactions and multiple- 
variable interactions (Cronbach, 1957). He suggests that 
3-way, 4-way, 5-way or greater interactions are necessary 
for describing the social world (Cronbach and Snow, 
1977). Cronbach wants to examine processes going on in 
a program, and he wants to be able to generalize to 
other, sometimes dissimilar programs, people and places 
(Cronbach et al., 1980; Cronbach, 1982). 
Cronbach et al. (1980) contend that a major purpose 
of evaluation is for conceptual use (or enlightenment). 
To this end, the evaluator may wish to reveal findings 
to stakeholders continuously during an evaluation. The 
evaluator should ‘hang around’, an evaluation site. In 
general, results should decrease stakeholder uncertainty 
about the operation of the program. The evaluator is to 
carry out an ‘educational’ role. Because much use of 
evaluation occurs over the long term and for programs 
other than the one being evaluated, all programs will 
benefit from accumulated theoretical knowledge. Finally, 
Cronbach has advocated the use of ‘standing committees 
of experts’ to synthesize findings from evaluation 
research. A tentative construction of Cronbach’s implicit 
process-model of utilization is shown in Fig. 6. 
Rossi suggests that to increase use evaluators should 
‘tailor’ evaluation activities to local needs (Rossi and 
Freeman, 1993). In general, how this is done depends on 
the stage and kind of program that is being evaluated. 
This process of ‘fitting evaluations to programs’ can be 
viewed as an approach to increasing evaluation use. 
Apparently if one were to conduct the wrong type of 
evaluation, it would be ‘useless’. Further, it is predicted 
that Rossi would suggest that forming a congruence 
between evaluation and need fosters use. Rossi’s col- 
laboration with Chen about the importance of program 
theory can also be seen as suggesting how to facilitate 
evaluation use (Chen and Rossi, 1983, 1989). Theory
<<<PAGE=5>>>
R. Burke Johnson/Evaluation and Program Planning 21 (1998) 93-110 91 
-II- -- 
Fig. 5. Stake’s implicit process-model. 
Fig. 6. Cronbach’s implicit process-model. 
enables one to know what is going on in the ‘black box’ 
and therefore, to understand and do something about the 
operation of a program. 
A tentative depiction of Rossi’s ideas is shown in Fig. 
7. 
The last implicit process-model from the evaluation 
utilization literature was developed from a 1984 paper by 
Weiss. Weiss suggests that evaluators can make the larg- 
est impact on evaluation use when policy is being trans- 
formed into an evaluation formulation and when 
evaluation findings are being put into action. She con- 
tends that there are two major kinds of obstacles to use: 
(1) intellectual-cognitive and (2) social-structural. Weiss 
discusses what an evaluator can do to increase use and 
when he or she should do it. The utilization facilitation 
process is modeled in Fig. 8. 
4. Explicit evaluation utilization process-models 
Explicit process-models are process-models that are 
constructed by researchers and generally, appear in art- 
icles and books. Empirical models that are directly tested 
on empirical data can be also viewed as explicit process- 
models (e.g. a single regression equation which is based 
on a direct effects model or a full structural equation 
model). Some theoretical and empirical explicit models 
that have appeared in the literature are presented now. 
One frequently cited explicit process-model of evalu- 
ation utilization was developed by Greene (1988), see Fig. 
9. Greene empirically generated a grounded theory model 
using qualitative methods (i.e. two field studies). She stud- 
ied two local program evaluations-a youth employment 
program and a day-care information and referral 
program. She suggested that stakeholder participation in 
the evaluation is an effective way to promote evaluation 
use. Based on the program data, Greene categorized 
stakeholders into three groups: (1) VIPs (very involved 
persons), (2) SIPS (somewhat or sometimes involved per- 
sons) and (3) MIPS (marginally involved persons). 
According to Greene’s participatory approach, the eva- 
luator should get people involved in the formulation and 
interpretation phases of the evaluation in order to 
increase evaluation utilization. Therefore, VIPs are to be 
desired (Greene, 1988). As can be seen in Fig. 9, Greene 
views participatory evaluation as a three-stage process. 
The three stages are (1) the ‘Participatory Evaluation 
Process’, (2) ‘Uses of the Evaluation Process’ and (3) the 
‘Process of Utilization’. Elements in stage one are said to 
affect elements in stages two and three; elements in stage 
two are said to affect the ‘greater understanding’ part of 
the stage three utilization process. Therefore, one inter- 
esting feature of this model is that it includes indirect 
effects. Also, it operationalizes utilization as a process. 
Another model of evaluation use was originally 
developed by Cousins and Leithwood in 1986 and was 
updated in 1993. Although the 1993 version is a knowl- 
edge utilization model, the authors point out that it 
should be viewed as a direct extension of their 1986 evalu- 
ation utilization model. The key extension is the inclusion 
of an ‘interactive processes’ factor. The 1993 model, 
Do your 
Modify 
Work with Compare program 
homework program based on 
i.e, review managers 
literature on _ and develop _ 
Collect model 
_ with 
findings 
data 
similar program reatii 
r (instrumental 
use+ 
programs model conceptual 
4 uW 
Fig. 7. Rossi’s implicit process-model.
<<<PAGE=6>>>
98 R. Burke Johnson/Evaluation and Program Planning 21 (I998) 93-110 
I. Intellectul/Cognitivs Doa\ain 
Social/Structural Oar&n 
&I I-Gzl r-=G--l I------ 
Fig. 8. Weiss’s second implicit process-model 
ParUdpatory Evaluation Process 
USUSdUle ConMkrtions ol the Evaluation 
Eveluatbn Process Procsss to uwatbrl 
Fig. 9. Greene’s explicit process-model. 
shown in Fig. 10, divides seventeen utilization factors 
into three more abstract factors called ‘Characteristics 
of the Source of Information’, ‘Improvement Setting’ 
characteristics and ‘Interactive Processes’. All three of 
these sets of factors are shown to directly affect util- 
ization. The first two sets are also shown to have indirect 
effects on utilization through their effects on the inter- 
active processes factor. 
The next model comes from an evaluation-for-use 
applications book by Alkin (1985). Alkin is one of the 
earliest researchers in the evaluation utilization literature 
(Alkin et al., 1979). In the 1985 book, Alkin includes a 
list of variables needing consideration when one is eva- 
luating for use. He suggests that practicing evaluators 
consider human factors, context factors and evaluation 
factors. Listed under human factors are evaluator charac- 
teristics and user characteristics. Listed under context 
factors are pre-existing evaluation bounds (e.g. written, 
contractual and fiscal constraints), organizational fea- 
tures and project characteristics. Finally, listed under 
evaluation factors are evaluation procedures, infor- 
mation dialogue, substance of evaluation information 
and evaluation reporting. Alkin organizes what he sees 
as the most important of these aforementioned factors 
into the concepts shown in the process-model in Fig. 11. 
Patton (1997, 1984) is the founder of ‘Utilization- 
Focused Evaluation’. In this approach, an evaluator only 
conducts an evaluation when it has a realistic potential 
for use. The evaluator is supposed to consider potential 
use at every stage of the evaluation (1988). In the begin-
<<<PAGE=7>>>
R. Burke Johnson/Eualuation and Program Planning 21 (1998) 93-110 99 
Sophistication Qualii 
Cl-Y 
Relevance 
CuNnurllcatKn alAal*/ 
C6fuet-u 
r#?Mness 
kteraahfe Processes 
Fig. 10. Cousins’s and Leithwood’s explicit process-model. 
Setting 
the 
Stage 
Identifying/ Operationalizing 
) Organizing _ > the 
the Interactive 
Participants Process 
Fig. 1 I, Alkin’s explicit process-model. 
Adding 
the 
Finishing 
Touches 
ning stages, the evaluator identifies and works with 
potential users. The most important users are organ- 
izational decision makers and are called ‘primary 
intended users’. The evaluator works with the primary 
intended users in deciding what and how to evaluate. 
Information that would be helpful in decision making is 
collected using a design created through a collaboration 
of the evaluator and the primary intended users. At the 
end of this process, use by intended users is considered 
to be likely. An abridged version of Patton’s process is 
shown in Fig. 12. 
The next researcher, Wollenberg (1986) studied the 
use of evaluation in two school districts using fieldwork 
methodology over an extended period (i.e. a complete 
school year). Extensive qualitative data were collected. 
The data were analysed in three ways: (1) using Alkin’s 
Identie 
Rimuy 
Intended 
Users and 
Stakeholders 
1979 categories, (2) through ‘forms of use’ (including 
direct use, legitimative use, persuasive use, conceptual 
use and anarchic use) and (3) through ‘time periods/cycles 
of program implementation/growth’ (i.e. data were cat- 
egorized into the three cycles, in effect creating a time- 
process variable). The three cycles of program growth 
included a conceptual stage, a developmental stage and 
an institutional stage. Wollenberg’s depiction of the inter- 
action of evaluation users, types of use and program 
cycles is shown in Fig. 13. 
An additional model, developed by Wollenberg (1986) 
is useful because it depicts the evaluator’s communication 
pattern in two different kinds of organizational structures 
(Fig. 14). The two structures are, first, a ‘loosely-coupled’ 
system (i.e. a decentralized, bottom up system) and 
second, a more tightly controlled, but smaller, system 
Fig. 12. Patton’s explicit process-model
<<<PAGE=8>>>
100 R. Burke JohnsonjEcaluarion and Program Planning 21 11998) 93-110 
Fig. 13. Wollenberg’s depiction of interaction of stakeholder, type of utilization and evaluation cycle. 
Fig. 14. Wollenberg’s depiction of coupling in two California School Districts. 
(i.e. a more traditional bureaucratic hierarchical form). In 
the loosely coupled system the evaluator communicated 
mainly with a bilingual director and teachers (see diagram 
for Montemar School District). The bilingual director 
played a ‘gatekeeping’ role in the Montemar School Dis- 
trict (i.e. deciding who obtains information about the 
evaluation). In the ‘tighter’ district (i.e. Alvarado Union 
School District), the evaluator was the gatekeeper and 
hence, communicated with all of the major stakeholder 
groups. The use of these two models is in showing who 
communicates with whom in the overall organization. 
Even though the model is not a utilization model per se, 
Wollenberg pointed out that coupling affects com- 
munication which affects utilization. 
The next explicit process-model was created by Boyer 
for his dissertation (Boyer, 1989). He carried out a quan- 
titative study and used multiple regression analyses to 
compare the importance of variables known, from the 
previous literature, to influence utilization. Boyer exam- 
ined utilization of evaluation reports by members of 
Congress and key Congressional staff-persons. Rec- 
ommendations were the unit of analysis. The study was 
based on a sample of 100 Congressional staff who had 
direct evaluation use experience. 
The results were based on several regression equations. 
When using one simple or multiple regression equation 
for explanatory purposes, one is using a model of ‘direct 
effects only’; that is, it is hypothesized that each of the 
explanatory variables has a direct effect on the outcome 
variable (controlling for the other explanatory variables). 
Before conducting the regression analyses, Boyer found 
that all of the independent variables had statistically sig- 
nificant bivariate relationships with utilization. The 
regression tables can be found in Boyer (1989) and in 
Boyer and Langbein (1991). Some of the explanatory 
variables that were important in predicting utilization 
were clarity, absence of a detractor, communication, rel- 
evance to Congress, character of the timing, credibility 
of methodology and reputation of performer. 
Another quantitative study of evaluation utilization 
was conducted by Johnston for his dissertation (John- 
ston, 1986, 1988). In this study, Johnston drew arrows 
depicting his assumption of direct effects. One of several 
sub-models reported is provided in Fig. 15. 
Johnston reported nine empirical models in his dis- 
sertation; for each model he changed one or more of the 
outcome or explanatory variables. The model in Fig. 15 
is given here only as one example. 
Johnston found that his variable ‘type of change’ was 
the best predictor of evaluation utilization. In order, from 
easiest-to-achieve utilization to hardest, the change 
dimensions were (1) behavior changes, (2) changes in
<<<PAGE=9>>>
R. Burke Johnson/Emluation and Program Planning 21 (1998) 93-110 101 
Methodology 
Factor 
Acceptance 
Variable 
p5F-y Fig. 15. Johnston’s direct-effects model of evaluation acceptance. 
rules, (3) organizational structure changes and (4) chan- 
ges in goals and purposes. The other two variables found 
to be statistically significant predictors of utilization were 
influence (i.e. pressure from politically powerful external 
agencies) and study methodology (e.g. conceptualization, 
generalizability, reliability and validity, and use of litera- 
ture). 
In the next study reviewed, Huberman developed a 
series of models to depict the process of research uti- 
lization (Huberman, 1987, 1990, 1994-1995). He 
developed a ‘general model’ which integrated the other 
models in his paper. The other models included (1) an 
‘organizational model: researchers’, (showing how 
researchers influence the dissemination and utilization 
process), (2) an ‘organizational model: users’, (showing 
how users of research are influenced by dissemination 
effort and the ‘predictors of local impact/use’) and (3) a 
‘dissemination effort model’ (which showed the sets of 
variables affecting dissemination efforts). Huberman’s 
general model is shown in Fig. 16 (Huberman, 1987, 
1990). 
As seen in the Figure, Huberman’s explicit process- 
model is the most extensive of the models reviewed here. 
The model was used to depict the causal process opera- 
ting in a series of local projects that were part of a national 
program created by the Swiss National Research Council. 
In the 1987 article, the projects were ongoing and the 
‘integrative’ models were used as preliminary depictions 
of project processes. In the 1990 article, the projects had 
been completed and Huberman used a ‘multiple-case, 
tracer study’ design. Basically, Huberman followed the 
eleven projects (from a population of 25 projects) from 
beginning to 18 months after completion. 
Huberman’s results showed, empirically, that linkages 
between researchers and practitioners were important for 
utilization (especially conceptual). Furthermore, he 
found that new collaborative relationships frequently 
developed as a result of practitioner participation in the 
evaluation. It was important that practitioners par- 
ticipated in the evaluation before its conduct, during its 
conduct and at the end. Participation during the ongoing 
evaluation was especially important. 
The next explicit process-model was developed and 
empirically tested by Johnson (1980). This is the only 
path-analytic study of evaluation utilization found in 
the utilization literature. To empirically test the model, 
Johnson collected data from 75 decision makers who 
worked for 25 organizations. The decision makers had 
been exposed to ‘one or two of 19 evaluation products 
produced by university personnel and students’ (Johnson, 
1980). In-person interviews and paper-and-pencil ques- 
tionnaires were used for data collection. 
Johnson’s model is shown in Fig. 17 with the stan- 
dardized partial regression coefficients placed on the 
arrows. Each coefficient shows the standard deviation 
change in the variable receiving the arrow (i.e. endogen- 
ous variable) given a one standard deviation change in 
the causal/independent variable. 
As seen in Fig. 17, contact and involvement had the 
most important influence on evaluation use (using stan- 
dardized coefficients as the ‘relative importance’ indices). 
Transfer intensity and compatibility of results also had 
direct influences on evaluation. Linkage roles had a clear 
indirect influence on evaluation use through direct effects 
on two intervening variables of evaluation use (i.e. trans- 
fer intensity and compatibility of results). 
The last explicit process-model was developed by Owen 
(1992). Delineating, obtaining and reporting refer to the 
conduct of an evaluation in Owen’s model (Fig. 18). 
Following an evaluation, evaluation findings are trans- 
mitted and enlightenment occurs. As a result of enlight- 
enment, additional forms of use occur. A strength of the 
model is that conceptual use or enlightenment is shown 
to occur before instrumental or behavioral use. 
5. A meta theoretical process-model 
In reviewing the implicit and explicit models of evalu- 
ation utilization and other published lists of utilization 
variables, several key categories and themes emerged as 
facilitators of evaluation utilization. Participation by pro- 
gram evaluators, participants and practitioners was con- 
sistently found to be an important facilitator of
<<<PAGE=10>>>
102 R. Burke Johnson/Evaluation and Program Planning 21 (1998) 93-110 
Organizationd 
Dlssemlnatlon Effort 
.lntensnyd dfoft 
.qu8lnydexeaRbn 
Effects of the study 
.inm I#. 
.rnmd- 
.ralathm~ 
.rmtrgkum 
.htomrgMlzanond uw 
Fig. 16. Huberman’s explicit process-model. 
Proximity to 
Evaluatbn Us0 
Fig. 17. Johnson’s explicit process-model. 
utilization (Alkin, 1985; Cousins and Earl, 1992; Cousins 
and Leithwood, 1993; Greene, 1988; Owen et al., 1994; 
Patton, 1997). Some properties of participation include 
the type of participation (e.g. autocratic or democratic 
and open or closed) and the degree of participation (vary- 
ing from great to small). Organizational process and 
ongoing commmunication were also important com- 
ponents of the utilization process (Huberman, 1987, 
1990; Johnson, 1995; Wollenberg, 1986). Some possible 
properties or subcategories are quality of communication 
(e.g. clear/not clear; high fidelity or low fidelity), openness 
of organization to communication and change (e.g. open 
or not open), timeliness of communication, dissemination 
(e.g. ‘Was feedback given during the program or at the 
end as in a traditional report?’ and number of people, 
organizations, and people included in the dissemination/ 
discussion), type and direction of communication (e.g. 
vertical, horizontal and diagonal; high bandwidth or low 
bandwidth) and distribution of power (e.g. power 
through status or position vs widespread empowerment 
of employees and managers using information and idea 
power) (Greene, 1988; Huberman, 19941995; Hub- 
erman and Cox, 1990; Siegel and Tucker, 1985). Feed- 
back, which is probably part of the organization/ 
communication variable just listed, also appeared to be 
part of the utilization process (e.g. some possible dimen-
<<<PAGE=11>>>
R. Burke Johnson/Evaluation and Program Planning 21 (1998) 93-110 103 
Major further use Effects on 
Professional 
+ development 
Delincatin~ Obtaining Reporting Tmnsmission of 
--) evaluation 4. Enlightenment 
fmdings 
Program logic 
Structures 
- Policy 
Fig. 18. Owen’s explicit process-model. 
+ Justification 
sions are timeliness, frequency, depth and consensus 
building). Politics and self-interested decision making 
were important utilization factors (e.g. formal and infor- 
mal political cultures, and individuals’ self-interests, 
ideology, utility and power) (Chelimsky, 1987; Newman 
et al., 1987; Weiss, 1983, 1984). Use management was 
another important variable (e.g. evaluability assessment, 
selection of appropriate methodology, management com- 
mitment and use of organizational design and devel- 
opment principles) (Dibella, 1990; Patton, 1997; Wholey, 
1983,1985). 
A multidimensional conceptualization of the outcome 
variable (i.e. evaluation utilization) was developed, with 
cognitive use (e.g. general cognitive processing based on 
evaluation participation), behavioral use (e.g. instru- 
mental use) and organizational learning (e.g. changes in 
the organizational culture) being especially important. 
Process use is often part of cognitive use. For example, a 
stakeholder may develop an ‘evaluation schema’ while 
participating in an evaluation (i.e. he or she learns how 
to think like an evaluator). Conceptualizing cognitive use 
as affecting behavioral use is in line with current social 
and cognitive theory. That is, some form of cognitive 
processing virtually always precedes behavior. Cognitive 
use is viewed as including awareness of an evaluation, 
thinking about a program or evaluation and the devel- 
opment of attitudes, beliefs and opinions about a pro- 
gram as a result of an evaluation and participation in it. 
It also includes the development of skills and ways of 
thinking as a result of participating in an evaluation. 
These cognitive schema may affect behavior toward par- 
ticular programs currently being evaluated and behavior 
toward future programs via enlightenment. Behavioral 
use involves action and it is closely aligned with instru- 
mental use, but it may also include symbolic and legi- 
timative use (Owen, 1992) persuasive use and process 
use. All of the forms of use occur as iterations occur 
through the theoretical model during and after a program 
evaluation. 
Organizational learning (Fiol and Lyles, 1985; Levitt 
and March, 1988; Senge, 1990) is another important con- 
cept that is gaining in popularity in the evaluation uti- 
lization literature (Forss et al., 1994; Cousins and Earl, 
1992; Preskill, 1994). Organizational learning is an inter- 
disciplinary change theory integrating ideas from, but 
not limited to, learning theory, organizational behavior, 
organizational theory, organizational development, con- 
structivism, complexity theory (defined below) and cog- 
nitive science (Coveny and Highfield, 1995; Guba and 
Lincoln, 1989; Mainzer, 1994; Senge, 1990; Waldrop, 
1992). It focuses on how people operate in a dynamic 
learning system, how they come to create and understand 
new ideas, how they adapt to constantly changing situ- 
ations and how new procedures and strategies are incor- 
porated into an organization’s culture (defined below), 
structure, policies, memory and individual human minds. 
It is a theory of complex and dynamic systems (Coveny 
and Highfield, 1995; Senge, 1990). 
Forss et al. (1994) found that organizational learning 
resulting from an evaluation sometimes takes place 
through organizational members becoming involved in 
the evaluation process and communicating about it. 
Organizational learning involves changes in the people 
of an organization as well as changes in organizational 
structure, operating procedures and culture. The theory 
of organizational learning should provide evaluators with 
a helpful framework for understanding and creating cul- 
tural and structural change and promoting long-term 
adaptation and learning in complex organizations opera- 
ting in dynamic environments. One goal would be to
<<<PAGE=12>>>
104 R. Burke JohnsonlEt~aluaIion and Program Planning 21 (1998) 93-110 
create an active learning organization (i.e. an organ- principles to help increase the amount and quality 
ization with a culture and structure supportive of learning of participation, dissemination, utilization and 
and growth). Organizational learning was not in any organizational learning. 
implicit or explicit models reviewed above. It is, however, 
part of the final theoretical model in Fig. 19. In grounded theory development a general theme like 
It seems clear, in review, that evaluation use is a con- this is called a story line (Strauss and Corbin, 1990). 
tinual process that evolves and changes shape over time. During the review of the implicit and explicit process- 
Models showing relationships among variables, there- models, I developed several integrative models. Because 
fore, are needed and these models should specify change earlier versions included too many variables for parsi- 
and temporal development. The general theme from the mony, later versions became smaller and slightly more 
above analysis goes like this: abstract. Using an iterative approach of going back to 
the literature, making model changes (i.e. eliminating and 
Evaluation utilization is a continual and diffuse consolidating variables) and deciding on model fit, the 
process that is interdependent with local contextual, theoretical model shown in Fig. 19 was developed. The 
organizational and political dimensions. Par- model includes the general factors considered most 
ticipation by program stakeholders is essential and important in evaluation utilization. 
continual (multi-way) dissemination, communi- Definitions of the constructs used in Fig. 19 and defi- 
cation and feedback of information and results to nitions of selected terms used in the discussion are given 
evaluators and users (during and after a program) now. The constructs used in the theoretical model are 
help increase use by increasing evaluation marked with asterisks. 
relevance, program modification and stakeholder 
ownership of results. Evaluators, managers and Belmioral Use* 
other key stakeholders should collaboratively Once becoming aware of an evaluation and the 
employ organizational design and development program, behavior or action may result. This 
(Formative Feedback and Process Loop) I 
Interactional 
Variabbs 
Utiliiation 
Variables 
Internal Environment and Context of the Evaluation 
External Environment and Context of the Evaluation 
Fig. 19. A theoretical model of evaluation utilization
<<<PAGE=13>>>
R. Burke JohnsonlEt~aluarion and Program Planning 21 (1998) 93-110 105 
behavior mainly includes instrumental use, but may 
also include symbolic use, legitimative use, and 
action oriented process use. 
Cognitive use* 
This is the cognitive processing construct in the 
model. It refers to the degrees to which people 
involved in or directly related to the program are 
aware of the evaluation, think about the infor- 
mation and form attitudes, beliefs, knowledge, 
skills and opinions about the program being evalu- 
ated. It also includes changes not related to the 
evaluation findings but result from participating in 
the evaluation (i.e. process use). People may cog- 
nitively develop an evaluation schema or mental 
model (i.e. they are able to think like an evaluator 
thinks). The schema may activate, for example, pro- 
cedural and tacit knowledge. Cognitive use includes 
cognitive oriented process use, enlightenment and 
conceptual use, and individual learning. 
Complexity theory 
A term which includes, but is not limited to chaos 
theory. It is used here to suggest a complex world 
of open, dynamic systems, based on non-linear (and 
linear) relationships, self-organization principles, 
feedback loops, strange attractors, perturbations, 
bifurcations, phase shifts and sensitivity to initial 
and changing conditions. One example is the ‘but- 
terfly effect’ where a very small perturbation to a 
system may lead to major consequences that would 
have been unpredictable based on estimates derived 
from an analysis of the initial conditions. 
Competing infomation* 
Information types and loads in organizations that 
may affect participation in and decisions about pro- 
grams being evaluated (e.g. it varies from low 
amount of directly competing information to high 
amount of directly competing information). 
Dissemination* 
Interpersonal and intergroup communication of 
information. This may take place formally (e.g. via 
memos, reports and meetings) and/or informally 
(e.g. through social networks and ‘grapevine com- 
munication’). 
Evaluator characteristics* 
Two contrasting types are evaluation-for-use/p- 
erson-focused evaluators and research-focused eva- 
luators. The person-focused evaluator believes that 
evaluation is a person activity (e.g. working directly 
with managers, participants and other stake- 
holders) rather than a research activity. Ideas of 
people related to the evaluation are sought out. 
The term evaluation-for-use may include Patton’s 
Utilization-Focused Evaluation approach but it is 
not limited to this formulation. The research- 
focused evaluator is the ‘expert’ who emphasizes 
conducting a scientific study but does not actively 
promote use. 
Expectations* 
Individuals’ expectations about the purpose of a 
program, how it should operate, what program out- 
comes will be and what evaluation can do. It 
includes summative or formative thinking. It may 
be based on attitudes about evaluation, a program 
and persons in an organization. It may include short 
term expectations and/or long term expectations 
about a program and/or an evaluation. It is a key 
psychological variable that affects how people think 
and behave. 
External environment and context of the evaluation* 
The total outside environment in which the organ- 
ization exists. It includes external stakeholders 
(who may enter into the internal environment of the 
evaluation once the program is started), regulations 
and laws, current political ideas, the macro-culture 
of society and community (which, for example, 
increasingly stresses individualism, competition 
and self interests), sub-cultures (e.g. religious 
and/or ethnic groups), the economic system (e.g. 
level of technology, capitalism, competition) and 
human resources available to the organization. 
Individual characteristics* 
Characteristics of individuals. Two contrasting 
types of individuals are (1) change seekers, who 
thrive on learning and like to try out new ideas and 
(2) bureaucratic individuals, who like order and 
may equate change with instability. May include 
personality characteristics as well as cognitive and 
behavioral characteristics (e.g. knowledge about 
evaluation and knowledge about and participation 
in program). This applies to all individuals in the 
system (e.g. organization members such as man- 
agers and program participants, as well as other 
stakeholders). 
Internal environment and context of the evaluation* 
Includes the internal political environment (see 
Politics) and the organizational culture and struc- 
ture (see organizational culture) operating as a 
dynamic system (see complexity theory). 
Interests and ideology* 
Do the evaluation findings support values and 
beliefs held in the organization and is it in potential 
users’ self-interests to use evaluation results and 
recommendations?
<<<PAGE=14>>>
106 R. Burke JohnsonlEt>aluation and Program Planning 21 (1998) 93-110 
Network organization 
A concept that I generated based on this research 
study. It can be viewed as a web or social network. 
It is based on the interactions and structures con- 
necting all individual stakeholders and organ- 
izations (internal or external) that participate in or 
have a stake in an evaluation or program. It is a 
relatively emergent and fluid type of organization 
that appears each time an evaluation takes place. It 
comes about through communication and par- 
ticipation. 
Organizational characteristics* 
Organic vs mechanistic types. An organic organ- 
ization is a relatively flat organization with high 
levels of vertical and horizontal communication. 
Power is located more in ideas and performance 
than in position. A mechanistic organization is the 
traditional Weberian bureaucracy where power is 
located in the position and communication travels 
through the chain of command typically shown in 
organization charts (Tosi, 1992). 
Organizational culture 
An organizational culture is the unique set of norms 
(accepted and expected behavior), values, attitudes, 
beliefs, traditions, language, ‘ways of doing things’, 
folklore and artifacts of an organization. Indi- 
viduals are socialized into organizational cultures 
when they enter and continuously interact during 
their tenure; they also reciprocally impact the cul- 
ture in the process of culture making. Cultures tend 
to support change or support homeostasis. 
Organizational learning* 
Involves adjustments on the part of the program 
and organization, including, for example, changes 
in the culture, strategy (e.g. how things are done in 
the organization, including how the organization 
‘learns’) , formal and informal structures/patterns, 
‘important stories’, ‘myths’ and current operating 
and behavioral norms. It is based on learning by 
individuals in an organization. 
Participation* 
Degree to which people involved in or directly 
related to a program (e.g. organization members, 
stakeholders and evaluators) participate in a pro- 
gram and/or evaluation. It includes communi- 
cation, reflection, discourse and dissemination 
during a program. It results in continuous process 
use. Formative feedback adds information to and 
affects the nature of participation. 
Politics* 
What is the nature of the political environment 
in the organization or network organization? Do 
individuals with formal or informal power support 
changes suggested in formative and summative 
evaluations? How quickly can new practices be 
adopted without creating too much resistance to 
change? Can individuals change their practices 
without fear of political retribution? Will indi- 
viduals’ power affect how they view a program and 
evaluation? Will individuals’ personal power 
increase or decrease if changes are made? 
Truth and utifity tests* 
Do potential users believe the evaluation results 
(formative or summative)? Do they view the eva- 
luator as being credible? Is the evaluation of high 
quality? And is the information seen as useful in 
individuals’ jobs? Truth and utility testing varies 
along a continuum with the poles labeled ‘affirm- 
ative truth and utility testing’ and ‘negative truth 
and utility testing’. 
Several comments are made at this point about the 
theoretical model shown in Fig. 19. First, the model is 
intended to apply to any evaluation, formative or summ- 
ative, internal or external. Often, but not always, in an 
internal evaluation the internal environment will be com- 
posed of a single organization with an in-house evaluator. 
In an external evaluation the evaluator generally enters 
the internal environment from an organization outside 
of the organization where the program is being developed 
and evaluated. In both cases, iterations occur through the 
model during and after any evaluation; that is, feedback 
continually occurs and the status of the variables in the 
model change over time. The term network organization 
(see definition) was coined to add focus to the idea that 
in many evaluations, multiple stakeholder groups become 
involved in program development and evaluation. In 
essence, a social network evolves around the program 
and evaluation forming the network organization. It is 
the persons in this network that have the most to gain, 
learn and add to program development, refinement, and 
evaluation. Evaluators are situated in a nexus of internal 
and external influences. 
Next, the theoretical model with an emphasis on pre- 
sent and emergent structures and process draws on Gid- 
dens’ idea of structuration. That is ‘social structure is 
used by active agents; and in so using the properties 
of structure, they transform or reproduce this structure’ 
(Turner, 1986). Last, the type of theory provided in Fig. 
19 is what Reynolds (197 1) calls the ‘causal process form’ 
of theory. Probable causal links are proposed in a visual 
model rather than as a list of propositions. 
It is suggested in Fig. 19 that evaluation use occurs 
through an open system of interrelated background, 
interactional and use variables operating in an internal 
environment situated in an external environment (defi-
<<<PAGE=15>>>
R. Burke JohnsonlEwluation and Program Planning 21 (1998) 93-110 107 
nitions above). The model should not be viewed simply 
as a set of stagnant concepts. It is the process implied that 
makes up the ‘model in action’. The theoretical model is 
based on the assumption that the utilization process needs 
to be viewed as a dynamic and open, complex system 
(see definition of complexity theory) (Bertalanffy, 1968; 
Coveny and Highfield, 1995; Mainzer, 1994; Schoderbek 
et al., 1990). Iterations (i.e. feedback) occur as individuals 
participate, think, act, learn and adjust to changing con- 
ditions. This is shown by the causal arrows, feedback 
loops and input-output points. If any factor in the system 
changes, other changes may occur as a result of feedback. 
The point is that evaluation use is not a static, linear 
process. 
The external environment and context is the broadest 
dimension of the model. It is assumed that the external 
environment is constantly changing and is complex. The 
external environment affects and is reciprocally affected 
by the internal environment of the evaluation, as well as 
the explicit process variables in action (via input and 
output). The external environment and context set up 
certain constraints as well as opportunities for an organ- 
ization or program by providing, for example, knowl- 
edge, a labor pool, national and local political beliefs, new 
organization/business/service opportunities and laws and 
regulations. It is contended that, in order not to overlook 
the external environment, an external environmental 
analysis needs to be done by evaluators attempting to 
maximize utilization of results. That is, we need to scan, 
study, and capitalize on the characteristics of the external 
environment and context of an evaluation. 
The next broadest component of the model is the 
internal environment and context of an evaluation. Eva- 
luators enter a particular internal environment at a par- 
ticular time and place. A ‘social reality’ is present, but it 
is also emergent, holistic and situational; it is continually 
reconstructed and changing. This is the nature of organ- 
izational change and learning. Politics (definition above) 
permeates all organizations’ internal environments and 
may result in a propensity for change (or not); this will 
be reflected in the organizational culture. Although poli- 
tics is virtually omnipresent, the political variable was 
inserted into the model at two key points. 
The internal environment also includes the program 
and organizational history and culture, in addition to the 
formal bureaucratic structure. If long term organization 
or program changes are to be expected, evaluators need 
to become aware of the manifold characteristics of an 
internal environment. Therefore, an internal environ- 
mental analysis needs to always be carried out (Wholey, 
1979). 
Ultimately, cultural as well as structural changes must 
be made if evaluation use and organizational learning 
are to occur, become institutionalized and last over time 
(Goodman and Dean, 1982). A promising structural 
change for facilitating lasting change is to develop a col- 
lateral organization (also called a parallel learning struc- 
ture) (French and Bell, 1995; Zand, 1974). This is a 
relatively permanent group of individuals in an organ- 
ization who become knowledgeable about a program and 
about program evaluation and continue to be a source 
for people in the organization to go to with thoughts 
and ideas to be shared and discussed. Members of the 
collateral organization continue to monitor, evaluate and 
facilitate program improvement and organizational 
learning over the long term. They also train or help others 
in the organization to become more knowledgeable about 
the conduct of evaluation, as well as organizational devel- 
opment and change. 
The explicit process-variables form the third major 
component of the theoretical model. The first three vari- 
ables, at the left of the model, are called background 
variables because they, combined with the external and 
internal environments of the organization, set the stage in 
which evaluation utilization and organizational learning 
occur. They are: organizational characteristics, indi- 
vidual characteristics and evaluator characteristics (defi- 
nitions above). Note that these variables can change and 
modify as feedback takes place in the model. 
Participation and dissemination are called the process 
or interactional variables because they represent the 
ongoing activity of people in an organization. These are 
social psychological variables and involve the process of 
social interaction (Cousins and Leithwood, 1993). The 
forms of participation and dissemination are directly 
affected by the background variables. 
It is hypothesized that participation will be highest for 
organic organizational forms (as contrasted to mech- 
anistic), for change-oriented ‘learning’ individuals (as 
contrasted to bureaucratic individuals) and for evalu- 
ation-for-use evaluators (as contrasted to research- 
focused evaluators). Participation is also affected by the 
utilization variables as iterations through the model take 
place (via feedback). For example, as part of organ- 
izational learning, changes in the culture may facilitate 
increased evaluation participation. 
Dissemination and participation are shown to have 
a reciprocal relationship because dissemination (as it is 
operationalized here) includes informal and formal com- 
munication (in addition to formal reports) and par- 
ticipating and communicating feed on one another in 
organizational behavior (e.g. as positive communication 
occurs, participation increases and as participation 
increases, communication increases more). Dis- 
semination is also affected by the same three background 
variables affecting participation and by politics. In an 
organic organization, informal networks are especially 
common and may result in a good deal of ‘grapevine’ 
and relatively open communication and dissemination, 
in addition to formal dissemination. While, a research- 
focused evaluator may view a final report as sufficient 
(‘my job is done now’), an evaluation-for-use evaluator
<<<PAGE=16>>>
108 R. Burke JohmonlEvaluation and Program Planning 21 (1998) 93-I IO 
will likely consider informal (in addition to formal) com- 
munications before, during and after an evaluation as 
important for facilitating evaluation use. Change-ori- 
ented individuals are hypothesized to become interested 
in an evaluation (because it may represent positive self 
and organizational growth and learning) and, they are 
therefore likely to participate and formally and infor- 
mally disseminate evaluation information and results. In 
sum, it is hypothesized that participation and dis- 
semination are key interactional variables in the evalu- 
ation utilization process. These variables directly affect 
cognitive use and are, in turn, affected by cognitive use 
via feedback. Participation and dissemination are con- 
stantly in motion, and they feed on each other, as well as 
change as iterations through the model occur. 
Competing information, expectations, truth and utility 
testing, interest and ideology, the interactional variables 
(i.e. participation and dissemination) and politics are 
shown to have direct effects on cognitive use. (Definitions 
of the variables are given above.) Cognitive use is also 
indirectly affected by the background variables and 
directly and indirectly affected by behavioral use and 
organizational learning through feedback. Cognitive use 
does not stop at a particular point in time. Individuals 
become aware of formative and/or summative results 
prior to using the results behaviorally. They conduct 
truth and utility testing, consider if use is in line with 
their interests and ideology, relate what they are learning 
to their expectations and consider competing information 
while in the organization (Weiss, 1980, 1984). Cognitive 
use also occurs when individuals develop an ‘evaluation 
schema’. This is a type of process use, where changes 
in individuals result not from any particular evaluation 
findings but from their participating in an evaluation and 
learning to think like an evaluator. 
It is specifically hypothesized that cognitive use will be 
high when participation and dissemination are high, 
when there is only a small amount of competing infor- 
mation, when truth and utility testing are affirmative (i.e. 
when they see an evaluation as credible and useful), when 
individuals’ interests and ideology are supported (i.e. 
when the evaluation supports values and beliefs com- 
monly held in the organization and fits the individuals’ 
self-interests) and when positive and realistic expectations 
are held about the evaluation and program. 
As shown in the model, cognitive use occurs and then 
individuals act (i.e. behavioral use). The proposition here 
is that cognitive use is a prerequisite for behavioral and 
organizational learning. Therefore, the cognitive use fac- 
tor in the model is an important node because it leads to 
behavioral use and organizational learning. 
It is hypothesized that the type of organization affects 
the nature of participation (‘who does what?‘, ‘who talks 
to whom?’ and ‘who makes decisions?‘) resulting in 
different degrees and types of utilization (i.e. cognitive, 
behavioral, and organizational). In a tightly coupled, 
mechanistic organization, many potential stakeholders 
may not be aware of the conduct of an evaluation because 
they are not in the appropriate information network; 
therefore, they do not participate. It is the evaluators’ 
and managers’ role not to let this happen. Further, in a 
mechanistic structure it may not be in non-participators’ 
interests to participate because of their lack of infor- 
mation, concern, power, and intense job specialization 
(Tosi, 1992). 
Organizational learning from the program and evalu- 
ation is shown in the model to result from cognitive use, 
behavioral use, and politics. Organizational learning is 
also affected by all of the other variables in the model 
through feedback and the internal and external contexts 
of the evaluation. It is well known that organizations can 
change and learn over time (Fiol and Lyles, 1985; Frey, 
1990) and it is hypothesized that evaluation utilization 
(e.g. cognitive and behavioral) will affect this process over 
time (Cousins and Earl, 1992). Forss et al. (1994) have 
recently argued that evaluations may lead to organ- 
izational learning via involvement and communication. 
In Fig. 19, these factors are shown to affect organizational 
learning indirectly through cognitive and behavioral use. 
Furthermore, new social constructions of reality in 
organizations and programs result from program evalu- 
ations and the concomitant interpersonal interactional 
process (Bandura, 1986; Gergen, 1985; Berger and Luck- 
mann, 1967; Pitre and Sims, 1987). Change in knowledge 
structures is one form of organizational learning. 
There will, however, always be some resistance to 
change (Couch and French, 1948; Klein, 1984; Neumann, 
1989) as well as misutilization (e.g. selective or inap- 
propriate use of findings) (Alkin, 1990; Shulha and Cous- 
ins, 1996). Nonetheless, as participation increases in a 
program evaluation, it is hypothesized that evaluation 
utilization (cognitive, behavioral and organizational 
learning) will occur, especially, given an open environ- 
ment (i.e. in an organic organization with an open cul- 
ture) where organizational members can grow and feel 
free to participate in the organizational change process. 
In fact, people often enjoy being part of the organ- 
ization/program change process, and many will, there- 
fore, join into the evaluation utilization process (Pasmore 
and Fagans, 1992; Neumann, 1989). 
6. Conclusion 
There has been a call for theory development in evalu- 
ation utilization. Therefore, a tentative meta theoretical 
process-model was provided in this study (Fig. 19). The 
hypothesized predictions that would be made from the 
model were discussed. Each arrow or combination of 
arrows in the model (i.e. direct; indirect, or feedback 
relationships) implies a theoretical proposition which is 
subject to additional empirical testing. Future research
<<<PAGE=17>>>
R. Burke Johnson/Ecaluation and Program Planning 21 (1998) 93-110 109 
should focus on testing propositions from the models 
presented in this paper and on developing superior 
models. 
References 
Alkin, M. C. (1985). A guide for evaluation decision makers. Newbury 
Park, CA: Sage. 
Alkin, M. C. (1990). Debates on evaluation. Newbury Park, CA: Sage. 
Alkin, M. C.. Daillak, R., & White, P. (1979). Using eoaluations: Does 
evaluation make a difference. Beverly Hills, CA: Sage. 
Alkin. M. C. (1991). Evaluation theory development: II. In M. W. 
McLaughlin & D. C. Phillips (Eds), Ecaluation and rducarion: At 
yuar/rr centur)’ (pp. 91-l 12) Chicago: University of Chicago Press. 
Asher, H. B. (1983). Causal modeling. 2nd ed. Beverly Hills, CA: Sage. 
Bandura, A. (1986). Social,foundations ofthought andaction. Englewood 
Cliffs, NJ: Prentice Hall. 
Berger, P., & Luckmann (1967). The social construction of reality. 
Garden City, NY: Doubleday. 
Bertalanffy. L. von (1968). General systems theorem. New York: George 
Braziller. 
Blalock, H. M. (1969). Theory construction: From cerbal to mathematical 
,/ormulafion.s. Englewood Cliffs, NJ: Prentice Hall. 
Boyer, J. F. (1989). Factors associated with congressional sfaffuse of; 
eraluation research. Unpublished doctoral dissertation, The Amer- 
ican University. Washington, DC. 
Boyer, J. F.. & Langbein, L. I. (1991). Factors influencing the use of 
health evaluation research in congress. Evaluation Reuiev. IS, 5077 
532. 
Campbell, D. T. (1963). E.uperimmtal and quasi-experimental designs 
for research. Chicago. IL: Rand McNally. 
Campbell, D. T. (1969). Reforms as experiments. American Psychol- 
ogist, 24. 409429. 
Campbell, D. T. (1984). Toward an epistemologically relevant sociology 
of science. Science, Technolog), and Human Values, 10, 38-48. 
Campbell, D. T. (1986). Relabeling internal and external validity for 
applied social scientists. In D. W. Fiske & R. A. Shweder (Eds). 
Adt~ances in quasi-erperimental design and anal?;sis (pp. 67-77). San 
Francisco: Jossey-Bass. 
Campbell, D. T. (1988). Methodology and epistemologJ,/or social xi- 
ence: Selected papers (E. S. Overman. Ed.). Chicago: University of 
Chicago Press. 
Carnall, C. A. (1990). Managing change in organizations. New York: 
Prentice Hall. 
Chen, H. (1990). Theorydriarn evaluations. Newbury Park, CA: Sage. 
Chen. H., & Rossi, P. H. (1983). Evaluating with sense: The theory- 
driven approach. Evaluation Recierv, 7, 2833302. 
Chen, H., & Rossi. P. H. (1989). Issues in the theory-driven perspective 
(Special issue). Evaluation and Program Planning, 12, 299-306. 
Chen, H.. & Rossi, P. H. (1992). Using theory to improve program and 
polic!, evaluations. New York, NY: Greenwood Press. 
Chelimsky, E. (1987). What have we learned about the politics of 
program evaluation? Educational Evaluation and Polic? Am&is, 9, 
1999213. 
Couch, L., & French, J. P. (1948). Overcoming resistance to change. 
Human Relaiions, I, 5 122532. 
Cousins, J. B., & Earl, L. M. (1992). The case for participatory evalu- 
ation. Educational Evaluation and Policy Analysis, 14, 397418. 
Cousins, J. B., & Leithwood, K. A. (1986). Current empirical research 
on evaluation utilization. Retien, of Educational Research, 56, 331- 
364. 
Cousins, J. B., & Leithwood, K. A. (1993). Enhancing knowledge 
utilization as a strategy for school improvement. Ktunvledge: 
Creation, Diffusion, Utilization, 14, 3055333. 
Coveny, P., & Highfield, R. (1995). Frontiers of comple.uit,r: The search 
,for order in a chaotic ,vor/d. New York: Fawcett Columbine. 
Cronbach, L. J. (1957). The two disciplines of scientific psychology. 
American Psychologist. 12, 671-684. 
Cronbach. L. J. (1982). Designing eraluations of educational and sociul 
programs. San Francisco: Jossey-Bass. 
Cronbach, L. J., Ambron. S. R., Dornbusch, S., M., Hess, R. D., 
Hornik. Phillips. D. C.. Walker, D. F., & Weiner, S. S. (1980). 
Tou,ard reform ofprogram etaluution. San Francisco: Jossey-Bass. 
Cronbach, L. J., & Snow, R. E. (1977). Aptitudes and instructional 
methods: A handbook,for research on interactions. New York: Irning- 
ton. 
Davis, J. A. (1984). The logic ofcausal order. Beverly Hills, CA: Sage. 
Dibella, A. (1990). The research manager’s role in encouraging evalu- 
ation use. Evaluation Practice, /l(2), 115-l 19. 
Fiol. C. M., & Lyles. M. A. (1985). Organizational Learning. Acadent>. 
of Management Reviebv, 10, 80338 13. 
Forss. K., Cracknell, B., & Samset, K. (1994). Can evaluation help an 
organization to learn? Etaluution Rerierr, 18, 57459 I, 
French, W. L., & Bell, C. H. (1995). Organixztion development. Engle- 
wood Cliffs. NJ: Prentice Hall. 
Frey, K. (1990). Strategic planning: A process for stimulating organ- 
izational learning and change. Organization Development Journal. 
Fail, 74-8 I. 
Gergen, K. J. (1985). The social constructionist movement in modern 
psychology. American PsJ,chologist. 40, 2666275. 
Glaser, B. G., & Strauss. A. L. ( 1967). The discovery ofgrounded theor!.: 
Strategiesfor Qualitutirr research. Chicago: Aldine. 
Goodman, P., & Dean, J. (1982). Creating long-term organizational 
change. In P. Goodman, (Ed.). Change in organizations (pp. 226 
79). San Francisco: Jossey-Bass. 
Greene, J. C. (1988a). Communication of results and utilization in 
participatory program evaluation. Evaluation und Program Plan- 
ning, II. 341-351. 
Greene, J. C. (1988b). Stakeholder participation and utilization in pro- 
gram evaluation. Evaluation Rerien, 12, 91-l 16. 
Cuba, E. G. & Lincoln. Y. S. (1989). Fourth generation evaluation. 
Newbury Park, CA: Sage. 
Harvey, D. F., & Brown, D. R. (1992). An experiential approach to 
organi:ation development. Englewood Cliffs, NJ: Prentice Hall. 
Huberman, M. (1987). Steps toward an integrated model of research 
utilization. Kno\vledge: Creation, D@ision, Utilixrtion, 8, 586-61 I. 
Huberman, M. (1990). Linkage between researchers and practitioners: 
A qualitative study. American Educational Research Journal, 27, 
363-391 
Huberman, M. (19941995). Research utilization: The state of the art. 
Knowledge and Policy, 7(4), 13-33. 
Huberman, M., & Cox, P. (1990). Evaluation utilization: Building links 
between action and reflection. Studies in Educutional Et>aluation, 16, 
157-179. 
Johnson, R. B. (1993). Models oferaluation utilization: A meta-modeling 
qnthesis of the literature. Paper presented at the annual meeting of 
the American Educational Research Association, Atlanta. 
Johnson, R. B. (1995). Estimating an eealuation utili:ation model using 
conjoint measurement and anai!xis. Evaluation Rez+e,v, 19(3), 3 I3- 
338. 
Johnson. K. W. (1980). Stimulating evaluation use by integrating aca- 
demia and practice. Kno\cledge: Creation. Diffusion. Utilization. 2, 
237-262. 
Johnston, W. P. (1986). A studs of the acceptance ofmanagement per- 
,formance ecaluation recommendations hy .federal agencies.. Lessons 
,fiom GAO resort.y issued in FY 1983. Unpublished doctoral disser- 
tation, George Mason University, Fairfax, Virginia. 
Johnston, W. P. (1988). Increasing evaluation use: Some observations 
based on the results at the U.S. GAO. In J. A. McLaughlin, L. J. 
Weber, R. W. Covert. & R. B. Ingle (Eds), Evaluation Urili:ation:
<<<PAGE=18>>>
110 R. Burke JohnsonlEvaluation and Program Planning 21 (1998) 93-110 
New Directionsfor Program Evaluation (pp. 7584). San Francisco: on educational and human services evaluation (pp. 229-260). Boston: 
Jossey-Bass. Kluwer-Nijhoff. 
Klein, J. A. (1984). Why supervisors resist employee involvement. Har- 
vard Business Review, September-October, 87-95. 
Leviton, L. C., & Hughes, E. F. (1981). Research on the utilization of 
evaluations: A review and synthesis. Evaluation Review, 5.497-519. 
Levitt. B., & March, J. G. (1988). Organizational learning. Annual 
Review, of Sociology, II, 3 19-340. 
Mainzer, K. (1994). Thinking in complexity: The complex dynamics of 
matter, mind and mankind. Berlin: Springer-Verlag. 
Newman, D. L., Brown, R. D., & Rivers, L. S. (1987). Factors influ- 
encing the decision-making process: An examination off the effect 
of contextual variables. Studies in Educational Evaluation, 13, 199- 
209. 
Neumann, J. E. (1989). Why people don’t participate in organizational 
change. Research in Organizational Change and Development, 3. 
181L212. 
Striven, M. S. (1991). Evaluation thesaurus. Newbury Park, CA: Sage. 
Senge, P. M. (1990). TheJfth discipline: The art andpractice of organ- 
izational learning. New York: Doubleday. 
Shadish, W. R., Cook, T. D., & Leviton, L. C. (1991). Foundations of 
program evaluation. Beverly Hills, CA: Sage. 
Shulha, L. M., & Cousins, J. B. (1996). Recent developments in theory 
and research on evaluation utilization, Paper presented at American 
Evaluation Association, Atlanta, Georgia. 
Siegel, K., & Tucker, P. (1985). The utilization of evaluation research. 
Evaluation Review, 9(3), 3077328. 
Stake, R. E. (1975). To evaluate an arts program. In R. E. Stake (Ed), 
Evaluating the arts in education: A responsive approach (pp. 13-3 1). 
Columbus, OH: Merrill. 
Noblit, G. W., & Hare, R. D. (1988). Meta-ethnography: Synthesizing 
Qualitative studies. Beverly Hills, CA: Sage. 
Owen, J. M. (1992). Towards a meta-model of evaluation utilization. 
Paper presented at the Annual Meeting of the American Evaluation 
Association, Seattle, WA. 
Stake, R. E. (198 I). Case study methodology: An epistemological advo- 
cacy. In W. Welch (Ed.), Case study methodology in educational 
evaluation. Minneapolis: Minnesota Research and Evaluation 
Center. 
Owen, J. M., Lambert, F. C., & Stringer, W. N. (1994). Acquiring 
knowledge of implementation and change. Knowledge: Creation, 
Diffusion. Utilization, 15(3), 273-284. 
Pasmore, W. A., & Fagans, M. R. (1992). Participation, individual 
development, and organizational change: A review and synthesis. 
Journal of Management, 18, 375-397. 
Patton, M. Q. (1978). Utilization-focused evaluation. Beverly Hills, CA: 
Sage. 
Stake, R. E. (1990). Situational context as influence on evaluation 
design and use. Studies in Educational Evaluation, 16,231-246. 
Stake, R. E. (1995). The art of case study research. Newbury Park, CA: 
Sage. 
Strauss, A., & Corbin, J. (1990). Basics of qualitative research. Beverly 
Hills, CA: Sage. 
Tosi, H. (1992). The environment/organization/person contingency 
model: A MESO approach to the study of organizations. Greenwich, 
Connecticut: JAI Press. 
Turner, J. H. (1986). The structure of sociological theory. Chicago: 
Dorsey. 
Patton, M. Q. (1984). An alternative evaluation approach for the prob- 
lem solving training program: A utilization-focused evaluation pro- 
cess Evaluation and Program Planning, 7, 189-192. 
Patton, M. Q. (1997). Utilization-focused evaluation (3rd ed.). Beverly 
Hills, CA: Sage. 
Patton, M. Q, (1988). The evaluator’s responsibility for utilization. 
Evaluation Practice, 9, 5-24. 
Pelz, D. (1978). Some expanded perspectives on use of social science in 
public policy. In M. Yinger & S. Cutler (Eds), Major social issues: 
A multidisciplinary view (pp. 346357). New York: Free Press. 
Pitre, E. &Sims, H. P. (1987). The thinking organization: How patterns 
of thought determine organizational culture. National Productivity 
Review, Autumn, 34&347. 
Waldrop, M. M. (1992). Complexity: The emerging science at the edge 
of order and chaos. New York: Touchstone. 
Wang, M. C., & Walberg, H. J. (1983). Evaluating educational pro- 
grams: An integrative, causal-modeling approach. Educational 
Evaluation and Policy Analysis, 5, 347-366. 
Weiss, C. H. (1976). Using research in the policy process: Potential and 
constraints. Policy Studies Journal, 4, 224228. 
Weiss, C. H. (1980). Knowledge Creep and decision accretion. Knowl- 
edge: Creation, Diffusion, Utilization, I, 381404. 
Weiss, C. H. (1983). Ideology, interest, and information: The basis of 
policy decisions. In D. Callahan & B. Jennings (Eds), Ethics, the 
social sciences, and policy analysis (pp. 213-245). New York: 
Plenum. 
Preskill, H. (1994). Evaluation’s role in enhancing organizational learn- 
ing. Evaluation and Program Planning, 17,291-297. 
Preskill, H., & Caracelli, V. J. (1996). The past, present and future 
of evaluation use: Results from a survey on current conceptions of 
evaluation use. Paper presented at American Evaluation Associ- 
ation, Atlanta, Georgia. 
Reynolds, P. D. (1971). A primer in theory construction. New York: 
Macmillan. 
Rossi, P. H., & Freeman, H. E. (1993). Evaluation A systematic 
approach. (5th ed). Newbury Park, CA: Sage 
Schoderbek, P. P., Schoderbek, C. G., & Kefalas, A. G. (1990). Man- 
agement systems: Conceptual considerations. Boston: BPI-Irwin. 
Striven, M. S. (1974). Standards for the evaluation of educational 
programs and products. In W. J. Popham (Ed.), Evaluation in edu- 
cation. Berkeley, CA: McCutchan. 
Weiss, C. H. (1984). Increasing the likelihood of influencing decisions. 
In L. Rutman (Ed.), Evaluation research methods: A basicguide (pp. 
1599190). Beverly Hills, CA: Sage. 
Weiss, C. H. (1988). Evaluation for decisions: Is anybody there? Does 
anybody care? Evaluation Practice, 9, 5519. 
Weiss, C. H. & Bucuvalas, M. J. (1980). Truth tests and utility tests: 
Decision-makers, frames of reference for social science research. 
American Sociological Review, 45, 30223 13. 
Wholey, J. S. (1979). Evaluation: Promise and performance. Wash- 
ington, DC: Urban Institute. 
Wholey, J. S. (1983). Evaluation: Promise and performance. Wash- 
ington, DC: Urban Institute. 
Striven, M. S. (1980). The logic of evaluation. Iverness. CA: Edge Press. 
Striven, M. S. (1983). Evaluation ideologies. In G. F. Madaus, M. 
Striven, & D. L. Stufflebeam (Eds), Evaluation models: Viewpoints 
Wholey, J. S. (1985). Managing for high performance: The role of 
evaluation. Evaluation News, 6,40-50. 
Wollenberg, J. E. (1986). Evaluation utilization: Processes and influences. 
Unpublished doctoral dissertation, University of California, Santa 
Barbara. 
Zand, D. (1974). Collateral organization: A new change strategy. Jour- 
nal of Applied Behavioral Science, IO, 63-89.