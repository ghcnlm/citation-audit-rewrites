<<<PAGE=1>>>
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/337013966
Implications of Evaluation Trends for Capacity Development
Chapter · September 2019
DOI: 10.18820/9781928480198/08
CITATIONS
3
READS
447
2 authors:
Candice Morkel
University of the Witwatersrand
17 PUBLICATIONS   66 CITATIONS   
SEE PROFILE
Neville Mangwiro
University of the Witwatersrand
2 PUBLICATIONS   3 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Neville Mangwiro on 09 January 2020.
The user has requested enhancement of the downloaded file.
<<<PAGE=2>>>
Implications of Evaluation Trends for Capacity Development
191
8
IMPLICATIONS OF 
EVALUATION TRENDS FOR 
CAPACITY DEVELOPMENT
Candice Morkel & Neville Mangwiro
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=3>>>
192
EVALUATION LANDSCAPE IN AFRICA
Introduction
Evaluation capacity development (ECD) remains one of the most sought-after 
yet elusive outcomes in the evaluation sector (Picciotto, 1998:39; Stockdill, 
Baizerman & Compton, 2002:16). Although there are extensive efforts to 
strengthen evaluation capacity in Africa, the precise mechanisms by which 
evaluation capacity development strengthens evaluation practice is largely 
unknown. The absence of empirical evidence which confirms the link between 
ECD and improvements in evaluation practice exacerbates the challenge 
(Preskill & Boyle, 2008; Tarsilla, 2014). Regardless of the challenges, the sector 
continues to recognise ECD as one of the primary solutions to the persistent 
weaknesses and limitations in the demand and supply of monitoring and 
evaluation (M&E) capacity on the continent (Wao, Onyango, Kisio, Njatah & 
Onyango, 2017:1; Porter & Goldman,
 
2009).
Research and practice have shown that there has been an absence of coherent 
strategies or models for ECD in Africa, resulting in fragmentation and piecemeal 
approaches to capacity development (Basheka, 2016:115; Tarsilla, 2014:8; 
CLEAR
-
AA, 2016c:25). The lack of harmony in short-term training or formal 
graduate education in M&E across the continent could, in part, be attributed 
to the absence of agreed-upon norms and standards for M&E education 
provision, and uncertainties around how to build capacity for evidence use, or 
capacity development in general (Stewart, 2015:555; Denney & Mallet, 2017:1; 
Preskill,
 2014:118). 
This is despite the existence of guidelines for evaluator 
competencies provided by, for example, the International Development 
Evaluation Association (IDEAS), the African Evaluation Association (AfrEA) and 
the American Evaluation Association’s (AEA) Joint Committee on Standards for 
Educational
 
Evaluation. 
A superficial analysis of the problems associated with current approaches to 
ECD will lead some to wrongly assume that the problem rests with those who 
are responsible for conducting evaluations – commonly termed the “supply 
side”. However, proponents of systems-thinking include the “demand side” 
in their approach to ECD – which incorporates the institutional capacity and 
appetite for conducting, institutionalising, and using evaluations. The terms 
‘supply’ and ‘demand’ in evaluation have therefore become widely applied 
(Porter & Feinstein, 2014:7; Merkle, 2016:2; Porter & Goldman, 2013; Mulenga & 
Porter,
 2
013:3; Development Bank of Southern Africa, African Development Bank 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=4>>>
Implications of Evaluation Trends for Capacity Development
193
and World Bank, 2000:53) and the matching of the two has become common in 
the discourse on ECD (Smith & Morkel,
 
2018:41). However, assumptions that 
evaluation supply and demand mechanisms work in the same way as those of 
other products, services or commodities in capitalist markets may not hold. 
It is critical to understand the interplay between the drivers and contextual 
issues that mediate between supply and demand that can assist in developing 
effective solutions to ECD challenges (Blaser Mapitsa & Khumalo, 2018:4). 
This chapter examines empirical results of evaluation reports from the AfrED 
database in order to unpack the relationship between the demand for 
evaluations and the capacities needed to meet that demand. The analysis 
further explores ways in which current M&E training and education provision 
can be enhanced to respond to capacity development needs. In achieving its 
objectives, the chapter also draws evidence from a secondary analysis of the 
results of a survey of evaluation practitioners’ perceptions of ECD challenges in 
the sector. The primary question guiding the chapter is: How can a snapshot of 
trends in evaluation supply, demand and practice provide an indication of the 
kinds of skills and capabilities that may be required of evaluators in the future 
and serve as a guide for those tasked with ECD? 
Brief background to evaluation capacity development 
Capacity development has been on the African agenda for the past six decades, 
yet contestations persist around the concept itself (Taut, 2007:120; Denney & 
Mallett,
 2017:5; 
Kwang
-
Ho, 2009:201; Kotvojs & Hurworth, 2013:5; Compton & 
Baizerman,
 
2007:118). ECD in particular has been on the evaluation agenda for 
at least two decades, despite not yielding the expected results (Kotvojs,
 
2017:14). 
Many continue to highlight the weaknesses in evaluation ‘supply’, which is 
generally defined as the individuals who must produce evaluations (Wao, Kisio, 
Njatha & Onyango, 2017:1; Merkle, 2016:4; Holvoet, 2012:9; UNICEF ,
 
2008:121; 
Podems,
 
2015:10). Findings from a 2005 study involving evaluation practitioners 
in South Africa, for example, illustrated the widely held opinion that both 
individual evaluation capacities and the quality of evaluation training were weak 
(Abrahams,
 
2015:4). Almost ten years later, a survey conducted on evaluation 
capacity in the South African government yielded similar results (Podems, 
Goldman & Jacob, 2014:76). Despite more than three decades of capacity 
building in Africa, and an increasing intensity in building evaluation capacity 
in particular, research has shown that the problems of capacity continue to 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=5>>>
194
EVALUATION LANDSCAPE IN AFRICA
persist (Wao, Onyango, Kisio, Njatha & Onyango,  2017:1; Basheka,  2016:95). 
It is plausible t
hat inadequate diagnoses of capacity needs may result in ever-
increasing ECD interventions, without the concomitant increase in evaluation 
capacity (Morkel & Ramasobana, 2017:7; Tarsilla, 2014:5). The scant use of 
diagnostic information to inform ECD interventions may also have a role to 
play in this (Kotvojs & Hurworth, 2013:5; Kotvojs, 2017:13). Although there is 
insufficient evidence to point to the exact reason for the mismatch, there is 
evidence that many ECD programmes do not implement systemic approaches 
that are known to be more effective, but instead continue to rely on short-term 
training programmes (Tarsilla, 2014:5).
The African Capacity Building Foundation, in its Africa Capacity Report 2019, 
defines capacity as the “ability of people, organizations, and society as a whole 
to manage their affairs successfully” (ACBF ,
 
2019:1). Capacity development, 
therefore, is broadly defined as a process by which “people, organizations, 
and society as a whole unleash, strengthen, create, adapt, and maintain 
capacity over time”. The contestation around the concept of ECD has added 
to the challenge. Some scholars and practitioners prefer to use the term 
‘evaluation capacity building’ (ECB), whilst others prefer ‘evaluation capacity 
development’ (ECD). Stockdill, Baizerman and Compton (2002) use the term 
ECB, and define it as: “a context-dependent, intentional action system of guided 
processes and practices for bringing about and sustaining a state of affairs 
in which quality programme evaluation and its appropriate uses are ordinary 
and ongoing practices within and/or between one or more organisations/
programs/sites”. Others (Preskill & Boyle, 2008:444; Brinkerhoff & Morgan, 
2010:3) argue that ECB is a systems-level intervention that incorporates both 
individual and institutional components. Tarsilla (2014:2), on the other hand, 
proffers that evaluation capacity building usually refers to the limited types of 
interventions targeting individuals, whilst evaluation capacity development 
focuses on effecting institutional and systems-wide changes. In this chapter, 
the term evaluation capacity development is used. It encompasses the entire 
spectrum of initiatives an organisation undertakes to strengthen evaluation 
systems, including the enabling environment. Where the intention is to refer 
to the building of individual human capacity in particular, the term ‘training’ is 
used
 
explicitly. 
According to Wubneh (2003:169
-
170), there are four dimensions to what 
is termed “capacity building”, which would fit the definition of capacity 
development used in this chapter. The first dimension is the development 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=6>>>
Implications of Evaluation Trends for Capacity Development
195
of human capacity, and focuses on the development of individual skills and 
abilities; the second is value system re
-
organisation; the third is institutional 
restructuring, whilst the fourth is the review of organisational development 
practices. Though Wubneh (2003:169
-
170) focuses on the development sector 
in general, the typology easily translates to the evaluation sector, as the aim of 
ECD is to change the way an individual and an organisation behaves, in order 
to observe positive changes in practice. 
Taking guidance from these definitions, it is clear that short
-
term training 
becomes a necessary, but not sufficient, part of ECD Interventions. Such 
interventions need to be combined with an integrated strategy to enhance 
individual and organisational capability if they are to be effective at a systems 
level. It is argued within the evaluation community that ECD is multi-faceted 
and systemic (Lennie, Tacchi, Wilmore & Koirala, 2015:326; Stockdill, Baizerman 
& Comption, 2002; Preskill & Boyle, 2008). Research has shown that training 
constitutes only one component of building competence as well as confidence in 
evaluation (Bamberg, Perlesz, Mckenzie & Read, 2010). Volkov and King’s
 
(2007) 
Checklist for Building Organisational Evaluation Capacity is instructive in this 
respect. It outlines eight guidelines for organisational evaluation capacity 
building, which have little to do with training. Examples of the guidelines 
include: cultivating a positive, ECD-friendly internal organisational context; 
purposefully creating structures to facilitate the development of evaluation 
capacity; building and reinforcing the infrastructure to support evaluations 
and securing financial resources to support an evaluation programme in the 
organisation (Volkov & King, 2007:1
-
3). The definition provided by Labin, Duffy, 
Meyers, Wandersman & Lesesnen (2012:308) also illustrates how ECD needs 
to be pursued beyond the individual, with their definition focusing on “an 
intentional process to increase individual motivation, knowledge, and skills, 
and to enhance a group or organisation’s ability to conduct or use evaluation”. 
Leviton (2013:93), in a paper on evaluation capacity building, poses a reminder 
that “training and technical assistance are a process, just as the experience 
of doing an evaluation is a process”. Proponents of ECD would likely agree 
that training is only one aspect of developing evaluation capacity, however, a 
much narrower view is often adopted in the planning and execution of ECD 
interventions. In many cases, training (typically in the form of short
-
term courses) 
becomes the lodestar of ECD. There is a need for more integrated approaches 
to ECD, which apply existing knowledge about the effectiveness of combining 
multiple strategies and tactics and incorporating them into ECD interventions. 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=7>>>
196
EVALUATION LANDSCAPE IN AFRICA
A more long-term view must also be adopted in thinking about ECD if efforts 
at str
engthening evaluation capacity are to be sustainable
Looking back to move ahead: What capacity 
development in Africa has taught us
History of capacity development in Africa and the  
influence on ECD
The historical roots of capacity development in Africa are well-documented 
(Basheka,
 
2016; Denney & Mallett, 2017; Development Bank of Southern Africa, 
African Development Bank and World Bank, 2000). The post-independence 
Africa of the 1960s was characterised by structural adjustment programming 
and the influence of Bretton Woods institutions, which shaped the interventions 
undertaken to ensure the continent’s growth and development (Basheka,
 
2016). 
Assisting the African continent to emulate the growth trajectories of the 
‘West’ became the focus of the international development community as the 
ideal model of addressing the continents’ poverty and social welfare crisis 
(Denney & Mallett, 2017:6). The decade of the 1960s was also punctuated by 
the growing popularity of the modernisation theory of development, which 
incorrectly assumed that Western modes of materialism, economic growth and 
the principles of the “free market economy” would release the continent from 
the grip of under-development (Davis, Theron & Maphunye, 2009; Denney 
& Mallett, 2017). Technical assistance or “technical cooperation” therefore 
became synonymous with building the capacities of developing countries 
through “international experts” who were meant to transfer their skills to local 
counterparts (Demongeot, 1994:479). 
Tarsilla (2014:2) argues that ECD remains donor centric and reproduces the old, 
self
-
serving pattern of strengthening local staff of international organisations 
rather than strengthening local, contextually relevant ownership of evaluation. 
Evaluation capacity development in Africa has its history in the 1980s and 
1990s (Basheka & Byamugisha, 2015:79
-
80). A tidal wave of initiatives solidified 
the importance of evaluation capacity development in Africa and a host of 
institutions, such as the African Evaluation Association (AfrEA) and others were 
established in order to focus on building the capacity of evaluation in Africa. 
Preskill and Boyle (2008:443) suggest that the first decade of the 21st century 
marked the beginning of the wave of interest in and focus on evaluation 
capacity building, particularly in North America. 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=8>>>
Implications of Evaluation Trends for Capacity Development
197
Despite the rising tide of criticism against the weaknesses in capacity 
development in Africa and the absence of evidence that these have any 
impact on building national capacity at all (Demongeot, 1994:479), capacity 
development initiatives (and their budgets) continue to increase (Wubneh, 
2003:166; Denney & Mallett, 2017:v; Kotvojs, 2017:13). For example, the 
International Monetary Fund (IMF) spends about one quarter of its budget on 
capacity development activities, with direct spending on capacity development 
growing by 6% from 2015 to 2016 (IMF , 2017:1
-
2). Between 1991 and 2016, 
the African Capacity Building Foundation (ACBF) committed US$700 million 
to capacity development in sub-Saharan Africa alone (ACBF , 2016:5), and the 
global investment in capacity development now exceeds US$30 billion annually 
(Kotvojs, 2017:13), despite yielding less than stellar results (Kotvojs & Hurworth, 
2013:5; Kotvojs, 2017:14). Programmes such as the CLEAR Initiative, the 
International Programme on Development Evaluation Training (IPDET), capacity 
building by voluntary organisations for professional evaluation (VOPEs) and 
universities have flooded the development community – yet the weaknesses in 
evaluation capacity and evidence use in Africa persist (Stockdill, Baizerman & 
Compton, 2002:16; Stewart, 2015:550; Denney & Mallett, 2017:v). Despite the 
misalignment between the proliferation of ECD interventions and strengthened 
evaluation practice, ECD is still seen as key to improving institutional capacities 
for evaluation planning and to produce high quality evaluations (Merkle,
 
2016) 
Therefore it is necessary to continue to produce evidence on “what works” in 
ECD so as to move beyond the impasse.
Challenges in the landscape of ECD efforts in Africa –  
what capacity, for whom?
Whereas twenty years ago it would have been less easy to find a suitable 
M&E training programme on the African continent, in recent times training 
opportunities
 
have rapidly expanded for anyone desiring professional 
development in this area. There is an ever-growing variety of ECD
 
actors 
in Africa, including private training organisations, individual consultants, 
international funding agencies, international development institutions, 
voluntary
 or
ganisations for professional evaluation (VOPEs) as well as 
institutions of higher learning. Some attempts have been made to develop 
databases of the various evaluation education opportunities available globally 
as well as in Africa. For example, in 2017, a specialised task force of the Global 
Goals for Sustainable Development identified 31
 
institutions across the world 
that provide capacity development programmes in evaluation, ranging from 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=9>>>
198
EVALUATION LANDSCAPE IN AFRICA
academic institutions to multilateral organisations and civil society organisations 
(Agrawal, Rao & Kumra, 2017). A similar mapping exercise in
 2016 
charted 
45
 
academic institutions across Anglophone Africa with programmes in M&E 
(CLEAR
-
AA, 2016a:24). However, there is no single, consolidated database or 
inventory of the myriad ECD opportunities and service providers in Africa or 
elsewhere, with little efforts at harmonisation and few attempts to measure 
their effectiveness.
Measuring and defining evaluation capacity is also an area of contestation. 
Competency frameworks,  such as the Canadian Evaluation Society Competencies 
for Canadian Evaluation Practice and the Evaluation Competency Framework 
of the South African Department of Planning Monitoring and Evaluation 
provide an indication of what constitutes a good evaluator (Buchanan & Kuji-
shikatani, 2014:34; Podems, Goldman & Jacob, 2014:81). However, these are 
not necessarily universally applicable and usually focus on the competency of 
the individual evaluator. Just over a decade ago, there was a disproportionate 
focus on individuals’ technical skills in making M&E systems work (Blaser 
Mapitsa & Khumalo, 2018:2) which may have influenced the dominance of 
individualised technical training to this day. However, it is widely recognised 
that evaluators need a broad range of skills, and that evaluation capacity is 
not limited to the individual, but applies to organisations as well (Wao et
 
al., 
2017:1; Lucas, 2013:6; Mapitsa Blaser & Khumalo, 2018:3; Edwards, Stickney, 
Milat, Campbell & Thackaway, 2016:265). Although thinking has transformed 
from an individualistic perspective on evaluation capacity towards a more 
organisational one (Labin, 2014; Volkov & King, 2007; Bourgeois, Toews, Whynot 
& Lamarche, 2013; Lennie, Tacchi, Wilmore & Koirala, 2015), individualised 
training programmes, focusing on technical aspects of evaluation continue to 
dominate ECD interventions, particularly in Africa (Tarsilla, 2014:4). 
Two additional ECD short-comings in Africa were identified by Tarsilla (2014). 
One of these is the ‘parachuting’ of evaluation expertise from outside the 
continent, which strangles the development of local evaluation capacities in 
Africa (Tarsilla, 2014:6; CLEAR
-
AA, 2016a:3). Tarsilla also laments that most of 
the continents’ ECD programmes are based on content developed outside 
of Africa, most notably North America, Europe and Australia, with little or no 
contextualisation to the African context. 
It is beyond the scope of this chapter to analyse ECD interventions in Africa in its 
entirety, as it cannot do justice to its complex and emergent nature. The systemic 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=10>>>
Implications of Evaluation Trends for Capacity Development
199
nature of ECD is highlighted here so as to reinforce the point that Africa’s 
evaluation capacity problems do not lie solely in improvements in evaluation  
education and training. The limitation, therefore, is that the data do not provide 
for an organisational or systemic perspective of ECD, highlighting the need 
for more empirical evidence on what needs to be done to improve evaluation 
capacity in Africa. 
This chapter assesses the landscape of evaluation demand from selected 
AfrED evaluations to understand the kind of capacity required to conduct 
evaluations in Africa for the period between 2005 and 2015. An analysis of a 
survey conducted by CREST (Centre for Research on Evaluation, Science and 
Technology) with over 500 M&E practitioners in 2016 during the compilation of 
the AfrED provides a snapshot of the evaluation skills and capabilities that are 
available on the continent. Although the data may not be generalisable, it does 
provide an account of evaluation practitioners’ view of ECD, and opens up the 
opportunity for further research on how ECD providers need to respond to the 
growing demand for evaluation services on the continent.
Approaches and methods
This chapter adopts a mixed-methods approach to explore trends about the 
effectiveness of evaluation capacity development interventions in Africa. Both 
qualitative and quantitative data were used, namely (i)
 
literature on ECD and 
ECB; (ii)
 t
he results of an online survey conducted in 2016 with 564
 
evaluators/
M&E practitioners as part of the African Evaluation Database (AfrED) project; 
and (iii)
 
the repository of evaluations on the AfrED. 
An online survey was conducted between 10 and 29 November 2016 by CREST 
to 3
 032
 
individuals via an e
-
mailed link. These included e
-
mails listed in the 
evaluation reports on the African evaluation database; e
-
mail addresses from 
the South African, Ethiopian, Ghanaian and African evaluation associations; and 
e
-
mail addresses from CREST’s own internal database. A total of 564
 
individuals 
completed the survey, amounting to a 22% response rate. The survey contains 
items relevant to this study (e.g. M&E qualifications and training; evaluation as 
a professional identity; and primary involvement in evaluation). Quantitative 
data relevant to the research questions were analysed using descriptive 
statistics. The open-ended questions in the survey were included as part of the 
qualitative data and analysis. In the survey, an open-ended question was posed 
to participants, asking respondents what they thought is the key to large scale, 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=11>>>
200
EVALUATION LANDSCAPE IN AFRICA
quality evaluation capacity development in Africa. A total of 407 responses were 
r
eceived for this question. Inductive coding was used to analyse the qualitative 
data and determine the main codes, categories, and themes emerging from 
the
 
data.
The AfrED currently has a total of 2
 635 documents, wit
h a geographical 
scope of twelve sub-Saharan Anglophone countries. A random sample of 
evaluations was retrieved from the database using systematic sampling. Every 
30th
 
evaluation report on the complete list of evaluations, per country, were 
selected using a random start. A total of 59 (n=59) evaluation reports were 
analysed using deductive coding. A set of a priori codes was developed, which 
included the following: evaluation type; evaluation design; data-collection 
methods; data-analysis method; evaluand country; sector; funder; government 
as client; lead evaluator local/international; lead evaluator consultant/private; 
and role of local stakeholders. The literature review on ECB and ECD provided 
the framework for the analysis and discussion of the survey responses as well 
as the evaluation reports. The subsequent section addresses the findings of 
this
 
study. 
Results and discussion: Snapshots of supply and demand  
and trends to guide the future of ECD
Evaluation supply trends – findings from the survey
The need for more training in technical competencies, with 
practical experience and opportunities for real-world practice
Evaluators in Africa appear to be highly qualified, yet most of the responses to 
the question on the key to large scale, quality ECD in Africa revolved around 
accessing more training. Despite the fact that more than half of the respondents 
were in possession of a master’s degree (55%), and a smaller percentage 
possessed a doctorate (17%), the stated need for additional training was 
significantly high (123
 out 
of 407
 r
esponses). Further exploration is needed 
to determine whether the content of ECD interventions are appropriately 
responsive to the differentiated needs of evaluation practitioners on the 
continent, as 76
 
respondents possessed a master’s degree in evaluation, whilst 
126
 
respondents possessed a diploma in evaluation studies. Respondents were 
actively working in evaluation practice, and either conducted (n=185), managed 
(n=113) or provided technical assistance in evaluations (n=91). These individuals 
were more likely to have a diploma in evaluation studies (between
 
48 and
 
60%) 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=12>>>
Implications of Evaluation Trends for Capacity Development
201
than any other qualification. This was followed by a master’s degree in evaluation 
(between
 
37% and
 
51%). Those who trained others in evaluation (n=99) were 
equally as likely to have a diploma or a master’s in evaluation studies
 
(47%). 
Respondents also reflected on a need for training in technical skills for evaluators 
such as project management, data management as well as quantitative and 
qualitative methods. This negates the view that the pendulum has swung 
in favour of ECD interventions that are contextual, and less technical. There 
appears to be a continued demand for training in fundamental technical 
aspects of M&E. For example, one respondent stated a need for training in: 
“evaluation design, outcome and impact evaluation, theory of change and 
results based evaluation methodologies, and using evaluation as evidence 
for decision-making”. Another respondent stated that what was required 
is a “model that combines training, practice and reflection over a sustained 
period of time”. This last statement is reflective of research that has shown that 
experiential learning is an important but insufficiently practiced component of 
ECD (Leviton, 2013:93; Tarsilla, 2014:6). More significantly, African practitioners 
who participated in Tarsilla’s 2014 study viewed the practical utility of 
training courses as ‘minimal’, and despite facilitators’ expertise, the training 
programmes provided no opportunity to put their learning into practice. It 
comes as no surprise, therefore, that respondents preferred training that 
provided opportunities for “real-world practice”. 
A second cluster of statements emerged from the data specifically around 
the development of skills in management and analysis of data. It appears that 
there are concerns amongst evaluators around the quality of data, and a need 
for training on the production and analysis of data as well as management 
information systems to support these. Examples of statements from respondents 
in response to the question on how to remedy this include: 
Provide capacity on techniques of monitoring database management that can fit into 
evaluation process”; “Recruitment of competent data collectors, use of real-time data-
collection devices, effective training, co-ordination and close monitoring and follow-
up”; and “In my opinion the key lies in having access to quality of data which assist in 
arriving at evidence-based conclusions.
Significantly, the second highest cluster of responses (110  out of  407) was 
categorised under t
he broad category of “Continuous professional develop
-
ment of t
he demand and supply side” (including leadership and evaluation 
culture), which also links to training. The statements on the need for more 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=13>>>
202
EVALUATION LANDSCAPE IN AFRICA
training, combined with this set of statements, represent more than half of the 
statements responding to the question on what Africa needs for high-quality 
ECD. This seems to indicate a great need for ongoing capacity building of 
both evaluators as well as those commissioning and managing evaluations. It 
is possible that the need for continuous professional development drives the 
popularity and unceasing demand for evaluation training in Africa. Therefore, 
a more nuanced view of ECD needs is required. ECD providers need to be 
cognisant of the needs of individuals in order to respond appropriately. 
Differences between sectors (particularly the state and civil society) and the 
practice and utilisation of M&E also matter. ECD providers therefore need 
to ensure that the design of ECD programmes are appropriately targeted to 
participants, particularly at post-graduate level. For example, whereas the 
fundamentals of research methodology, sampling and data-processing may be 
necessary for those who have not yet been trained in these areas, it could be an 
ill fit for participants with master’s and PhD qualifications. Ongoing professional 
development for the latter cohort would need to provide learning opportunities 
on advanced, innovative and emerging issues in evaluation, rather than courses 
focusing on technical skills development of this nature. Evaluation theory and 
specialised curriculum design would be important for more advanced ECD. 
The importance of appropriate targeting is therefore emphasised, and the use 
of diagnostic exercises, pre
-
tests, training-needs analyses or skills audits would 
be useful before the design of any ECD programme.
Communities of practice
The remaining categories of statements were comparatively smaller than the 
statements around training and professional development, yet were large 
enough to categorise and compare with similarly smaller categories. This 
also assisted in exploring the hierarchy of ECD needs amongst evaluation 
practitioners. Forty statements were about collaboration and partnerships. 
Respondents reflected on the need for professional exchanges between 
the global North and South, and between senior and emerging evaluators. 
Mentorship, skills transfer and the building of communities of practice also 
emerged as important actions that needed to be taken, pointing to a possible 
realisation amongst respondents that training on its own is not sufficient in 
building evaluation capacity. Considering that the survey data also showed 
that the vast majority of respondents in the age category 31
 
to 40
 
identified 
themselves as managers, trainers and commissioners of evaluations, this 
could be an indication of an emerging need amongst mid
-
career evaluation 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=14>>>
Implications of Evaluation Trends for Capacity Development
203
practitioners to form part of professional communities where learning takes 
place. This is also the cohort that self-identified as evaluators, and considered 
evaluation as their primary discipline. They are mainly involved in managing, 
supervising or consulting on evaluations rather than carrying out all evaluation 
activities. Evaluation education should therefore be cognisant of the particular 
needs of this cohort of professionals, and steer more ECD interventions towards 
supporting the strengthening of evaluation management activities, and also 
consider more innovative means of ECD through, for example, professional 
exchanges and peer learning.
Country and place of work, professional identity, 
professionalisation and accreditation
The majority of respondents (62%) self-identified as evaluators in any capacity 
(this included trainers, students and managers). Evaluation was considered the 
primary professional identity of a large proportion of the respondents
 
(43%), 
whilst for 31% of respondents it was considered their secondary professional 
identity. However, it is significant that most self-identified evaluators were 
responsible for managing evaluations (39%) or commissioning evaluations
 
(14%). 
The largest employer for these evaluators was the state (24%), closely followed 
by the NGO sector (19%). This has important implications for ECD. Primarily 
identifying as an evaluator implies a much more concentrated focus on 
evaluation as a discipline, than managing it as a secondary discipline. Similarly, 
the contextual differences in evaluation practice between the state, national 
NGOs, international organisations and other sectors also have important 
implications for ECD. The skills and capabilities required to commission 
evaluations are different from those required of an evaluator, or someone who 
designs management information systems to support evaluation practice. For 
example, a director of agricultural projects who needs to evaluate the success 
of a micro-finance project may require only foundational theoretical knowledge 
in order to manage such an evaluation, versus the director of evaluation in 
the same state department. This once again points to the need for extensive 
diagnostics in order to differentiate between the various ECD needs and focus 
areas, whether at an individual or organisational level.
Respondents reported moderate levels of experience in evaluation-related 
activities. Fewer than half of the respondents (41%) had less than five years 
of experience in doing evaluation-related work. On average, respondents 
had about eight years of experience working in evaluation-related activities. 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=15>>>
204
EVALUATION LANDSCAPE IN AFRICA
Most respondents had about 6–10 years’ experience in the various types of 
evaluat
ion activities such as conducting evaluations (n=135), planning/contracting 
evaluations (n=87), technical assistance in evaluation (n=67), training others in 
evaluation (n=68) and in writing about evaluation (n=51).
Education and training institutions would also need to consider how newly 
established institutional arrangements for M&E in late-adopting countries 
would need to shape ECD. Approximately half of those between the ages of 
31 and 49 (50%) were more likely to self-identify as primarily evaluators. A much 
smaller proportion (less than 30%) of those between the ages of 50 and 69 
felt that they were primarily an evaluator. This is to be understood in relation 
to the dominance of South African respondents and the fact that evaluation 
is considered a relatively ‘new’ profession in the country. More professional 
opportunities to work in evaluation have emerged in the last decade and a half 
in South Africa, and therefore the results may represent this state of affairs. The 
need for foundational training and development may be more pronounced 
in these circumstances. How individuals identify their roles and functions 
in respect of their institutional structure has implications for how evaluation 
education may need to be shaped. 
In terms of place of work, 38% of respondents worked primarily in South Africa as 
M&E practitioners in various organisations and in a range of capacities, followed 
by Ghana (9%). Those who worked in South Africa did mostly contractual work 
with national non
-
governmental organisations (NGOs) (78% of respondents), 
whilst 70% were contracted by the state (national) and 69% by international 
development agencies (some may have multiple clients). 
The implications for ECD in the case of evaluation practitioners (who have 
some experience in conducting a range of evaluation activities) would be 
around what would constitute effective continuous professional development 
in strengthening evaluation practice. The challenge in designing curriculum 
and capacity development strategies is that there are variations amongst 
professionals in the type of work that they will do, and the amount of time 
spent on it. For example, a senior public official who is tasked with planning or 
contracting evaluations has different capacity-building needs from those who 
are involved in providing technical assistance. ECD programmes need to take 
these variations into account in designing capacity development interventions, 
which poses a challenge to the ideal of harmonising competencies and 
curriculum across the continent (particularly for institutions of higher learning). 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=16>>>
Implications of Evaluation Trends for Capacity Development
205
The ideal curriculum and ECD programme would need to find the right balance 
between generic skill development and knowledge building, and specialised 
training
 
subjects.
The issue around professionalisation and certification was certainly a concern 
to respondents. The fourth largest category of statements was around 
professionalisation, certification and building a pool/cohort of professionals (35 
statements), whilst the fifth was around strengthening university programmes 
(34 statements). Respondents were also concerned about cultural representivity 
in building a pool of professionals. There may be a link between the desire for 
professionalisation and certification and the statements reflecting a demand 
for improved university-based evaluation education programmes. A major 
challenge for the evaluation sector is that few countries have been able to 
address the contestations that have dominated the professionalisation debate 
over the decades, as well as the binary positions that have been adopted 
around whether or not to ‘professionalise’ or ‘credentialise’. A study by 
Podems (2014) indicated the highly contested and sometimes political nature 
of the development of competency criteria for evaluators, and has an effect on 
moving forward on the issue of professionalisation. 
The desire for academic institutions to adopt a more definitive role in ECD 
amongst respondents is congruent with the rising demand amongst evaluation 
professionals in Africa for more university-based qualifications. It is also aligned 
to the increase in the number of academic institutions that are building post-
graduate qualifications in M&E (as opposed to or in addition to executive short 
courses). Some of the statements included, for example: “Professionalising the 
evaluation field and expanding evaluation training institutions” and “Evaluation 
should be professionalised and made part of university curriculum”. Building 
a cohort of professional evaluators who produce high-quality evaluations (and 
other related services) is key to addressing the perceived shortage of African 
evaluators on the continent. 
Evaluation practice
In terms of evaluation purpose, 94% stated that they frequently or occasionally 
used evaluations for decision-making, followed by evaluations to improve 
practice. Although it is commonly thought that accountability and compliance 
too often drive an organisation’s evaluation purpose, the findings show that this 
is the least common purpose of evaluation for respondents (24%
 
never
 
used). 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=17>>>
206
EVALUATION LANDSCAPE IN AFRICA
In terms of evaluation approaches used, respondents most commonly used 
participatory evaluation (88% frequently or occasionally used). The least 
common evaluation approach was feminist evaluation (76% never used). In 
terms of evaluation types, programme monitoring and process evaluation were 
most commonly used (93% frequently or occasionally used). The least common 
type of evaluation was evaluability assessment (44% never used). The most 
common evaluation method utilised was face
-
to
-
face interviews (98% frequently 
or occasionally used). In respect of the use of methods, various qualitative 
methods tended to dominate amongst respondents. These included individual 
interviews, focus groups and stakeholder interviews. Individual face
-
to
-
face 
interviews and document review were the methods most frequently used by 
most respondents with up to fifteen years of experience. 
ECD interventions need to strike a balance between the depth and scope of 
subject matter covered in training and development programmes. Training 
programmes, in particular, need to weigh up whether the more pressing need 
is to develop a broad range of technical skills in specific methods, types, and 
approaches to evaluation, how much time to spend on evaluation management 
or whether the need is to provide skills development in only a limited number of 
specialist areas. The findings show that there is a need to broaden the repertoire 
of evaluation types, approaches and methods used in evaluation practice, 
which can be addressed by ECD institutions. The dominant use of interviews 
must also be more closely examined, as it is a common misconception that it 
is easier to collect and analyse qualitative data. The limited use of quantitative 
methods and those which rely on statistical tests may point to an absence of 
the technical skills that are required to perform these. 
Although the second and third most frequently undertaken studies were 
process and impact evaluations (n=259), it is also commonly known that ‘impact’ 
is understood in different ways amongst evaluation practitioners. For example, 
the findings show that two quantitative approaches (surveys and testing) are 
less frequently employed, which are a large part of impact evaluation. It would 
be important that ECD providers pay attention to the need to harmonise the 
understanding of the concept to ensure that impact evaluations are rigorous, 
and that commissioners/managers of evaluation have a working understanding 
of conducting these.
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=18>>>
Implications of Evaluation Trends for Capacity Development
207
Contextualisation of evaluation practice
An important cluster of statements emerged around participatory methods, 
localisation and diversity (26 statements). Examples of the statements made 
by respondents to improve evaluation capacity building on the continent 
included a need for training participatory impact assessment and participatory 
monitoring and evaluation as well as capacity building to national evaluators 
on participatory and result-based monitoring and evaluation. It is important 
to evaluators that local stakeholders are involved throughout the evaluation 
process. The illusion of participation of stakeholders in evaluations is often 
created by the articulation of participatory data-collection methods in 
evaluation, such as focus groups. These, however, are not sufficient to address 
the challenge of ensuring that local communities (often the beneficiaries of 
programmes being evaluated) are sufficiently empowered to participate in 
meaningful ways in evaluations, beyond data collection. ECD interventions 
need to intentionally create awareness and capabilities that will strengthen 
practice in this regard. An illustrative statement from a participant reinforces 
this
 
notion: 
Furthermore, there is a need for increased capacity to conduct evaluations but equally 
important is the ability to understand the context of the major stakeholders in the 
evaluation. In the African context technical knowledge of constructing evaluations 
will not be enough unless trust is established and relationships are managed at every 
stage of the evaluation. So leadership ability plays an important role.
 
 (Sur
vey respondent, 2016)
These responses are linked to a number of recurring themes that emerged 
which were not repeated as frequently as the above (less than twenty 
statements each), but nevertheless are important issues that have resonated in 
the discourse on ECD. One of these was the need for training in Made in Africa 
Evaluation (MAE) and understanding context and culture (sixteen statements). 
An example of a statement by a respondent echoed the need for ECD content 
that is rooted in African axiology: “Development of professional competencies 
for evaluators, development of a body that oversees the evaluation profession 
and the development of accredited, affordable evaluation courses. All of 
these must be rooted in African philosophies.” Scholars such as Chilisa, 
Major, Gaothlobogwe and Mokgolodi (2016) posit that there is a need for a 
greater focus on indigenised, culturally relevant methods and approaches to 
evaluation in ECD. An endogenous approach to ECD, emerging from African-
centric approaches to evaluation, is missing from the current range of ECD 
interventions (Tarsilla, 2014:8). The absence of this in ECD interventions on 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=19>>>
208
EVALUATION LANDSCAPE IN AFRICA
the whole, but curriculum and instructional design in particular, needs to be 
addressed in order for evaluators to remain relevant and effective in doing 
evaluation work in
 
Africa. 
Quality assurance
Two sets of statements that appeared with equal frequency (21 statements each) 
wer
e clustered around standardised protocols across Africa as well as funding/
resources. The issue of standardisation has been on the evaluation capacity 
development radar for a number of years. The defining of competencies and the 
standardisation of evaluation practice is a contested space, and no continent 
has been able to fully resolve it. Greater efforts by the ECD sector are required 
to provide an enabling environment for evaluators in Africa to improve the 
professionalism and quality of evaluation practice. This includes moving past 
the impasse around whether or not to standardise or to focus instead on the 
harmonisation of evaluation quality standards. A related and recurring theme 
deemed important (and linked to the concern over the weaknesses in evaluation 
competencies on the African continent and in some way to the standardisation of 
evaluation practice) was clustered around requiring high quality evaluations and 
quality assurance (ten statements). Examples of statements from respondents 
included the need for “training in quality assurance”; “Better evaluation quality 
control”; “More dedicated commitments by evaluators to quality evaluations”; 
and “Strong evaluation bodies that regulate quality of evaluations and maintain 
a pool of continuously learning evaluators”. With regards to the resource 
challenge, respondents offered a number of solutions to what they perceive 
will lead to effective evaluation capacity and practice on the continent. These 
include: resource pooling, making training more affordable to Africans, as well 
as scholarships for African evaluators. 
Evaluation demand – findings from the evaluation database
The review of the evaluations in the AfrED was an attempt to identify trends in 
the commissioning and conducting of evaluation to help provide an indication 
of the “demand environment” for evaluation. This provided insights into the 
kinds of skills that have been required to meet the demand, and what that 
means for ECD in Africa. 
Donor-led evaluations
Trends in the data emerging from the AfrED point to the practice of placing a 
premium on the use of external evaluators from the West as opposed to local 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=20>>>
Implications of Evaluation Trends for Capacity Development
209
consultants (CLEAR-AA, 2016a:3). The data revealed that international evaluators 
cont
inue to lead the production of evaluation reports on the continent (40
 
out 
of the 59
 
reports that were examined, were led by evaluators from outside of 
the country). Only one of the cases observed had a combination of local and 
international evaluators involved. Only five of the cases studied were led by 
local evaluators. In the vast majority of cases (n=28) local stakeholders were 
involved only in data collection, closely followed by no role (n=23). This aligns 
with Tarsilla’s (2014) findings on the ‘parachuting’ of international evaluation 
expertise onto local
 
communities.
What does this mean for ECD in Africa? It is still unclear why international 
evaluators dominate the local evaluation landscape. There is some speculation 
of an enduring perception that there is a lack of qualified, experienced evaluators 
on the continent. This may not hold water, considering that the data in this 
chapter revealed the high levels of experience and qualifications (general and 
evaluation specific) of evaluators in Africa. Another speculation is that African 
evaluators may not be au fait with the specific forms and frameworks that are 
used by individual donors for evaluating their interventions. There is a need, 
therefore, to empower African evaluators to respond to terms of reference 
and bids in a way that will enable them to compete on an equal footing 
with international evaluators, who may have (through experience) become 
familiar with donor requirements for evaluations. This may require bilateral 
organisations and donors to invest in local, country-led evaluation systems and 
to ensure that there is alignment between international needs for information 
about performance, and local needs for evidence for decision-making.
Evaluation types, design and methods of data collection 
In the vast majority of cases observed (n=21), impact evaluation was the type 
of evaluation undertaken. The type of evaluation conducted appears to be 
related to the commissioner of the evaluation. For example, impact evaluations 
appear to be mostly commissioned by donors. Impact evaluation was also the 
type of evaluation least undertaken by respondents to the survey, whereas 
impact evaluations were the most common type of evaluations conducted by 
donors in the sample of evaluation reports observed. Whether or not a lack of 
experience and skills in impact evaluation amongst African evaluators exists, 
must be further investigated. However, the space for international evaluators to 
dominate local evaluation practice is evident in the area of impact evaluation. 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=21>>>
210
EVALUATION LANDSCAPE IN AFRICA
Further research is needed to understand whether these patterns are more 
generalisable and,
 
if
 
so,
 
why. 
A fairly even spread of other types of evaluation were undertaken such
 
as form
-
at
ive evaluation, summative evaluation, process evaluation, and performance 
evaluation (between one and six cases spread across the various types). 
Document reviews (n=20), interviews (n=24) and observation or field work (n=15) 
remained the dominant methods of data collection amongst the evaluations 
conducted. In a vast majority of cases, a mix of these qualitative methods were 
used. It is expected, therefore, that thematic content analysis was used as the 
primary method of analysis in the vast majority of evaluations
 
(n=46).  
ECD interventions need to focus on the ability of evaluators to make observa
-
t
ions about data that will produce high-quality analyses and evaluation reports 
that have a high utilisation value. The robustness of evaluation practice is 
important to commissioners and users of evidence. Evaluation training and 
education must begin to re
-
shape the content of curriculum and instructional 
design towards producing these kinds of results. Much of the curriculum and 
training in evaluation education (particularly short courses) tend to focus on the 
fundamentals of M&E, such as designing theories of change, logic modelling, 
and results-based management. This is important foundational training that is 
needed for all M&E practitioners, yet evaluators in particular need training that 
is focused on developing the technical and analytical skills revolving around the 
“how to” of evaluation. From the two sets of data analysed above, a case may 
also be made for greater variations in the types of evaluation education and 
ECD provided to different cohorts of M&E practitioners (for example, those who 
commission and manage evaluations – who may be more likely be employed 
in government – and those who conduct evaluations). The standardisation (or 
harmonisation) of ECD interventions can become a reality if stakeholders and 
role-players agree on the competencies that different M&E practitioners may 
need to have in various roles, and the pathways to professional development 
in each of these cases. 
Conclusion
The evidence points to the continued need for ECD. However, proper assess -
ments and d
iagnostics must be prioritised to recognise the differentiated 
capacity building and capacity development needs of individuals and institutions 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=22>>>
Implications of Evaluation Trends for Capacity Development
211
if evaluation practice is to be strengthened effectively and sustainably. The 
relatively young evaluator or evaluation practitioner population, both in terms 
of experience and age, points to a need for ECD to focus on building the 
profession “from the ground up” in Africa. A large proportion of evaluators 
have less than five years’ experience, and although they are highly qualified 
in a range of disciplines, there is a gap in their experience in performing a 
range of M&E-related functions. ECD providers must respond by offering many 
more opportunities for experiential learning, in addition to classroom-based 
knowledge building and capacity development. ECD must move beyond 
classroom-based training and incorporate a broader range of interventions 
such as mentorships, create institutional structures and provide the resources 
that facilitate building individual capacities, and provide a supportive culture 
and context for individual and organisational learning.
There is a need to ensure that ECD programmes become more adept at 
targeting the right participants, particularly in programmes that would
 
include 
professionals with a first (or higher) academic qualification. The need for 
foundational training and development may be more pronounced in some 
circumstances, whereas more advanced levels of training may be more 
appropriate in others. Furthermore, evaluation practitioners self-identify 
differently within the sector (e.g. primarily as evaluators or commissioners 
of evaluations, public sector or civil society). There is a need for continuous 
professional development (CPD) programmes that are clear about what 
learning outcomes are being offered, who is being targeted and which sector 
the programme would best suit. Practitioners and scholars can then select 
to participate in programmes and be confident about the results that they 
will
 
achieve. 
University-based training programmes are increasingly becoming important to 
evaluation practitioners, so academic institutions need to remain relevant in 
meeting the ‘supply-side’ desire for more robust, university-based qualifications 
in M&E. However, these cannot afford to ossify in theory, as respondents clearly 
articulated the need for opportunities in the “real-world”. The inclusion of a 
practical component which simulates the realities in evaluation practice can no 
longer be considered a novelty for evaluation education.
Whether it is best to develop a broad evaluation knowledge-base or specific 
technical skills in conducting evaluations, and how much needs to be known 
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=23>>>
212
EVALUATION LANDSCAPE IN AFRICA
around evaluation management, is not cast in stone. ECD providers need to 
strike the ideal balance between foundational/generic capacity development 
and building specialised capabilities (and may even further disaggregate 
according to specific sectors). Much more effort must, therefore, be put into 
building diagnostic studies, training-needs analyses and skills audits as a 
non
-
negotiable part of the process of the design of ECD interventions. 
The limited range of evaluation types, approaches and methods used in 
evaluation practice observed in the evaluations examined in this sample of the 
AfrED perhaps points to a need to broaden the repertoire of evaluators. This 
includes the limited use of quantitative methods in impact evaluation, which is 
particularly concerning. 
The impasse around standardisation/professionalisation/certification within 
the sector needs to be transcended if evaluation capacity is to experience 
a fundamental shift in the right direction in Africa. Evaluation practitioners 
have clearly articulated their desire for the ECD sector to provide an enabling 
environment and to take the lead in the professionalisation of evaluation 
practice in
 
Africa. 
The localisation of evaluation practice as well as the empowerment of local 
evaluators and indigenous peoples (or the ‘beneficiaries’ of development 
interventions) beyond tick-box ‘participation’ in the global South is rapidly 
gaining momentum. This is taking place within a broader African debate 
around the decolonisation of curriculum in the academe in general. In order to 
remain relevant and to respond to this demand for indigenisation and cultural 
responsiveness, the ECD environment needs to re
-
calibrate its curriculum and 
instructional design. Related to this is the need to break free of the decades-
long pattern of international evaluators dominating the local evaluation 
scene. African evaluators need to be empowered to develop the knowledge, 
experience, and skill to respond to specific donor frameworks and templates for 
evaluation-related activities in order to compete equally with the international 
evaluator community. Rather than bemoan the state of affairs, more disruptive 
ways of addressing the challenges are called for. For example, commissioners 
of evaluations (particularly donors) could, for example, require as part of 
evaluation contracts that local evaluators be ‘inducted’ into the process of an 
evaluation from the inception of the project as part of a skills-transfer initiative, 
or that a certain percentage of local evaluators be required to jointly lead 
the
 
evaluation.
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=24>>>
Implications of Evaluation Trends for Capacity Development
213
The role of HEIs in evaluation capacity development is crucial, in so far as 
they are the locus of individual learning and development, and the creation 
of bodies of knowledge that support the growth of a discipline. However, it 
is important that a collaborative posture is adopted between all actors who 
play a role in ECD, as improved qualifications and proficiencies only tell part 
of the evaluation capacity story. As more organisations raise the demand for 
academic qualifications in M&E, either in recruitment practices or as part of 
their internal capacity-building strategies, higher education institutions will 
need to pay more attention to consensus around the design of competency-
based evaluation education programmes, the standardisation of qualifications, 
and curriculum
 
design.
References
Abrahams, M.A. 2015. A review of the 
growth of monitoring and evaluation 
in South Africa: Monitoring and 
evaluation as a profession, an industry 
and a governance tool. African 
Evaluation Journal, 3(1):1
-
8. https://doi.
org/10.4102/aej.v3i1.142
ACBF . 2016. African Capacity Building 
Foundation in action: 25 Years of 
capacity development impact across 
Africa. Unpublished report.  
https://elibrary.acbfpact.org/
acbf/collect/acbf/index/assoc/
HASH1d44/8f6b094e/cd2f4197/26.
dir/25%20Years%20of%20Capacity%20
Development%20impact.pdf
Bamberg, J.; Perlesz, A.; McKenzie, P . & 
Read,
 
S. 2010. Utilising implementation 
science in building research and 
evaluation capacity in community 
health. Australian Journal of Primary 
Health, 16(4):276
-
283. https://doi.
org/10.1071/PY10027
Basheka, B. 2006. Evaluation capacity 
building (ECB) in Uganda. Administratio 
Publica, 24(2):95
-
121. 
Bourgeois, I.; Toews, E.; Whynot,
 
J. & 
Lamarche,
 
M.K. 2013. Measuring 
organizational evaluation capacity in 
the Canadian Federal government. 
The Canadian Journal of Program 
Evaluation, 28(2):1
-
19.
Brinkerhoff, D. & Morgan, P .J. 2010. 
Capacity and capacity development: 
Coping with complexity. Public 
Administration and Development, 
30(1):2
-
10. https://doi.org/10.1002/
pad.559
Buchanan, H. & Kuji
-
shikatani, K. 2014. 
Evaluator competencies: The Canadian 
experience. The Canadian Journal 
of Program Evaluation, 28(3):29
-
47. 
https://doi.org/10.3138/cjpe.29.3.70
Chilisa, B.; Major, E.T.; Gaotlhobogwe,
 
M. 
& Mokgolodi,
 
H. 2016. Decolonizing 
and indigenizing evaluation practice 
in Africa: Toward African relational 
evaluation approaches. The Canadian 
Journal of Program Evaluation, 30(3): 
313
-
328. https://doi.org/10.3138/
cjpe.30.3.05
CLEAR
-
AA. 2016. The state, locus, focus 
and nature of current monitoring and 
evaluation education and training 
opportunities in Anglophone Africa. 
Johannesburg. Unpublished  
scoping report. 
CLEAR
-
AA & Twende Mbele. 2016a. 
Strengthening monitoring and 
evaluation education and training 
in Anglophone Africa: A regional 
workshop. Accra, Ghana. Unpublished 
scoping
 
report.
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=25>>>
214
EVALUATION LANDSCAPE IN AFRICA
CLEAR-AA & Twende Mbele. 2016b. 
Str
engthening monitoring and 
evaluation education and training 
in Anglophone Africa: A regional 
workshop. Nairobi, Kenya. Unpublished 
scoping report.
Compton, D.W. & Baizerman, M. 2007. 
Defining evaluation capacity 
building. American Journal of 
Evaluation, 28(1):118
-
119. https://doi.
org/10.1177/1098214006298172
Demongeot, P . 1994. Market-oriented 
approaches to capacity building in 
Africa. Public Administration and 
Development, 14(5):479
-
488. https://
doi.org/10.1002/pad.4230140504
Denney, L.; Mallett, R. & Benson, M.S. 2017. 
Service delivery and state capacity: 
Findings from the Secure Livelihoods 
Research Consortium. London: Secure 
Livelihoods Research Consortium.
Development Bank of Southern Africa, 
African Development Bank and World 
Bank. 2000. Monitoring and evaluation 
capacity. Conference discussion paper. 
Johannesburg, 25
-
29
 
September. 
Dewatcher, S. & Holvoet, N. 2016. Facing 
up to (online) fashion and fads
 … 
Face
-
to
-
face contact is here to stay in 
M&E capacity building. Evidence from 
35
 
national evaluation societies. African 
Evaluation Journal, 4(1):1
-
11. https://
doi.org/10.4102/aej.v4i1.158
Edwards, B.; Stickney, B.; Milat, A.; 
Campbell, D. & Thackway, S. 2016. 
Building research and evaluation 
capacity in population health: The NSW 
Health approach. Health Promotions 
Journal of Australia, 27(3):264
-
267. 
https://doi.org/10.1071/HE16045
IMF (International Monetary Fund). 2017. 
2018 Quinquennial review of the 
fund’s capacity development strategy: 
Concept note. Unpublished paper. 
Washington,
 
DC. 
Kotvojs, F . 2017. The capacity development 
evaluation framework: Tested on 
three initiatives. Evaluation Journal 
of Australasia,14(4):13
-
24. https://doi.
org/10.1177/1035719X1701700403
Kotvojs, F . & Hurworth, R. 2013. Valuating 
capacity development: What do we 
really want to know? Evaluation Journal 
of Australasia, 3(1):4
-
14. https://doi.
org/10.1177/1035719X1301300102
Kwang
-
Ho, S. 2009. Theoretical and 
strategic frameworks for national 
evaluation capacity building 
(NECB). International Area 
Review, 12(3):199
-
218. https://doi.
org/10.1177/223386590901200312
Labin, S.N. 2014. Developing common 
measures in evaluation capacity 
building: An iterative science and 
practice process. American Journal  
of Evaluation, 35(1):107
-
115. https:// 
doi.org/10.1177/1098214013499965
Labin, S.N.; Duffy, J.L.; Meyers, D.C.; 
Wandersman, A. & Lesesne,
 
A. 2012. 
A research synthesis of the evaluation 
capacity building literature. American 
Journal of Evaluation, 33(3):307
-
338.  
https://doi.org/10.1177/10982140 
11434608
Lennie, J.R.; Tacchi, J.; Wilmore,
 
M. & 
Koirala,
 
B. 2015. A holistic, learning-
centred approach to building 
evaluation capacity in development 
organizations. Evaluation, 21(3):325
-
343. 
https://doi.org/10.1177/13563890 
15590219
Leviton, L.C. 2013. Some underexamined 
aspects of evaluation capacity 
building. American Journal of 
Evaluation, 35(1):90
-
94. https://doi.
org/10.1177/1098214013502844
Lucas, B. 2013. Current thinking on 
capacity development. GSDRC 
Helpdesk Research Report No.
 
960. 
Birmingham,
 
UK: GSDRC, University 
of Birmingham. http://www.gsdrc.org/
docs/open/hdq960.pdf [Accessed 
7
 
May
 
2017].
Mapitsa Blaser, C. & Khumalo, L. 2018. 
Diagnosing monitoring and evaluation 
capacity in Africa. African Evaluation 
Journal, 6(1):1
-
10. https://doi.org/ 
10.4102/aej.v6i1.255
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=26>>>
Implications of Evaluation Trends for Capacity Development
215
Merkle, C. 2016. UN women’s experience 
with strengthening evaluation systems 
in Africa: Enhancing quantity, quality 
and use of evaluations. African 
Evaluation Journal, 4(1):1
-
8.  
https://doi.org/10.4102/aej.v4i1.127
Morkel, C. & Ramasobana, M. 2017. 
Measuring the effect of evaluation 
capacity building initiatives in Africa: 
A
 
review. African Evaluation Journal, 
5(1). https://doi.org/10.4102/aej.
v5i1.187
Mulenga, O. & Porter, S. 2013. Study on the 
demand for and supply of evaluation 
in Zambia. Centre for Learning on 
Evaluation and Results Anglophone 
Africa (CLEAR
-
AA). Johannesburg: 
University of the Witwatersrand. 
Podems, D. 2014. Evaluator competencies 
and professionalizing the field: Where 
are we now? Canadian Journal of 
Program Evaluation, 28(3):127
-
136.
Podems, D. 2015. Road map to: Feasibility 
study on professionalisation of 
evaluation in South Africa or 
strengthening evaluators in South 
Africa. Unpublished consultancy 
report submitted to South African 
Monitoring and Evaluation Association 
(SAMEA) and Department of Planning, 
Monitoring and Evaluation (DPME). 
https://ucarecdn.com/0094d98b-46ba-
43b2-aff7-3cbb0915050a/ 
Podems, D.; Goldman, I. & Jacob, C. 2014. 
Evaluator competencies: The South 
African government experience. 
The Canadian Journal of Program 
Evaluation, 28(3):71
-
85.
Porter, S. & Feinstein, O. 2014. Demand for 
and supply of evaluation in selected 
sub
-
Saharan African countries.  
https://www.gov.uk/dfid-research-
outputs/demand-for-and-supply-of-
evaluations-in-selected-sub-saharan-
african-countries 
Porter, S. & Goldman, I. 2013. A growing 
demand for monitoring and evaluation 
in Africa. African Evaluation Journal, 
11(1):1
-
9. https://doi.org/10.4102/ 
aej.v1i1.25
Preskill, H. 2008. Evaluation’s second act: 
A
 
spotlight on learning. American 
Journal of Evaluation, 29(2):127
-
138.  
https://doi.org/10.1177/10982140 
08316896
Preskill, H. & Boyle, S. 2008. A multi-
disciplinary model of evaluation 
capacity building. American Journal 
of
 
Evaluation, 29(4):443
-
459. https:// 
doi.org/10.1177/1098214008324182
Smith, L. & Morkel, C. 2018. Trends in  
supply and demand for evaluation 
in Africa, a view from CLEAR
-
AA. 
Evaluation Matters. Abidjan:  
African Development Bank. 
Stewart, R. 2015. A theory of change 
for capacity building for the use of 
research evidence by decision makers 
in southern Africa. Evidence & Policy, 
11(4):547
-
557. https://doi.org/10.1332/1
74426414X1417545274793
Stockdill, S.H.; Baizerman, M. & 
Compton,
 
D.W. 2002. Toward a 
deﬁnition of the ECB process: A 
conversation with the ECB literature. 
New Directions For Evaluation, 93:7
-
26. 
https://doi.org/10.1002/ev.39
Tarsilla, M. 2014. Evaluation capacity 
development in Africa: Current 
landscape of international partners’ 
initiatives, lessons learned and the way 
forward. African Evaluation Journal, 
2(1):1
-
13. https://doi.org/10.4102/aej.
v2i1.89
Taut, S. 2007. Defining evaluation 
capacity building utility 
considerations. American Journal 
of Evaluation, 28(1):120. https://doi.
org/10.1177/1098214006298062
UNICEF . 2008. The role of monitoring and 
evaluation in evidence-based policy 
making bridging the gap. Evaluation 
Working Papers, Issue
 
12. Geneva: 
UNICEF Regional Office for Evaluation. 
Volkov, B.B. & King, J.A. 2007. A checklist 
for building organisational evaluation 
capacity. http://dmeforpeace.org/sites/ 
default/files/Volkov%20and%20King_ 
Checklist%20for%20Building%20
Organizational%20Evaluation%20
Capacity.pdf [Accessed 8
 
February
 
2017].
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
<<<PAGE=27>>>
216
EVALUATION LANDSCAPE IN AFRICA
Wao, H., Onyango, R., Kisio, E., Njatha, M. & 
Onyango, N.O. 2017. Str
engthening 
capacity for monitoring and evaluation 
through short course training in Kenya, 
African Evaluation Journal, 5(1):1
-
9. 
https://doi.org/10.4102/aej.v5i1.192
Wilcox, Y. & King, J. 2014. A Professional 
Grounding and History of the 
Development and Formal Use of 
Evaluator Competencies. The Canadian 
Journal of Program Evaluation, 
28(3):1
-
28. 
Wubneh, M. 2003. Building Capacity in 
Africa: The impact of institutional, 
policy and resource factors. African 
Development Review, 15(2
-
3):165
-
198. 
https://doi.org/10.1111/j.1467-8268. 
2003.00070.x
C.B. Mapitsa, P. Tirivanhu & N. Pophiwa (eds). 2019. Evaluation Landscape in Africa - Context, Methods and Capacity. Stellenbosch: African Sun Media.
DOI: 10.18820/9781928480198/08
© 2019 AFRICAN SUN MeDIA
View publication stats