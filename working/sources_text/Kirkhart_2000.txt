<<<PAGE=1>>>
This chapter recasts evaluation use in terms of influence 
and proposes an integrated theory that conceptualizes 
evaluation influence in three dimensions-source, intm- 
tion, and time. 
Reconceptualizing Evaluation Use: 
An Integrated Theory of Influence 
Karen E. 
Kirkhart 
Use of evaluation has been a concern since the earliest days of the profes- 
sion. Although some challenge the centrality of use in the identity of the 
profession (see Henry, this volume; 
Mushkin, 1973; Scriven, 1991), the pri- 
macy of use as a focus of evaluation is well recognized. It has bounded the- 
ories of evaluation, marked debates, and framed meta-evaluation inquiry. 
Historically, the evolution of evaluation use has been marked by an increas- 
ing recognition of its multiple attributes (Cousins and 
 Leithwood, 1986; 
Johnson, 1998; Leviton and Hughes, 1981; Preskill and Caracelli, 1997; 
Shulha and Cousins, 1997). Nevertheless an inclusive understanding of the 
influence of evaluation has been hampered by the scope and language of 
past approaches. This chapter argues that it is time to step back and recon- 
ceptualize the terrain of evaluation’s influence by mapping influence along 
three dimensions-source, intention, and time. Each of these is defined, 
justified, and illustrated in the sections that follow, acknowledging histor- 
ical antecedents. The chapter highlights related issues raised by the three- 
dimensional conceptualization and closes with reflections on the potential 
utility of an integrated theory of influence. 
Historical Context 
Evaluators have shown a long-standing interest in the nature and extent of 
their works impact. Historically, conversations about influence have 
occurred under several themes-internal and external evaluation, evalua- 
tor roles, evaluation as a profession, ethics and values, and use of results 
(Anderson and Ball, 1978; 
Suchman, 1967). As the profession grew, these 
NEW DIKECTIONS FOR EVALUATION, no. 88, Winler 2000 
 @ Jossey-Bass 5
<<<PAGE=2>>>

<<<PAGE=3>>>

<<<PAGE=4>>>

<<<PAGE=5>>>

<<<PAGE=6>>>

<<<PAGE=7>>>

<<<PAGE=8>>>

<<<PAGE=9>>>

<<<PAGE=10>>>

<<<PAGE=11>>>

<<<PAGE=12>>>

<<<PAGE=13>>>

<<<PAGE=14>>>

<<<PAGE=15>>>

<<<PAGE=16>>>
20 THE EXPANDING SCOPE OF EVALUATION USE 
development. For example, this theory could be used to test Shulha’s 
hypothesis (this volume) that evaluation and evaluation inquiry exhibit dif- 
ferent patterns of influence or to extend the work of theorists such as John- 
son (1998) to differentiate logic models underlying specific types of 
influence. 
In closing, it is important to recognize how a more inclusive view of 
evaluation influence has positive implications for the evaluation profes- 
sion as a whole (Shulha and Cousins, 1997). Although construct under- 
representation has been previously addressed as a validity issue, its 
pragmatic effect is that evaluation influence is underestimated. As con- 
struct underrepresentation is corrected, not only does validity improve, 
but also the full scope of evaluation influence becomes increasingly visi- 
ble. For example, understanding long-term evaluation impact builds cred- 
ibility for the profession and generates support for evaluation among 
service delivery professionals. This integrated theory of influence helps us 
recognize that evaluation practice has had a more pervasive impact than 
heretofore perceived. 
Notes 
1. The first two dimensions were addressed by Kirkhart (1995). The model presented 
here reflects a revision of that earlier work. This chapter is the product of many thought- 
ful conversations among the coauthors of this volume, energetic debates with early col- 
laborators David M. Fetterman, Jean A. King, and William R. Shadish Jr., and continuing 
dialogue with Nick L. Smith. 
2. Here the term developmental is used to acknowledge that these distinctions are not 
defined by the passage of time alone. They also stand in relation to the evaluation 
process, which itself moves through stages. For example, end-01-cycle influence is tied 
to the length of an evaluation cycle, not fixed at a certain number of weeks or months. 
3. The logic here is analogous to that underlying interval recording procedures (Bloom, 
Fischer, and Orme, 1999). A time period is taken as the frame of reference, and any 
activity of interest-in this case, influence-that occurs during that interval is noted. 
When one shifts one’s attention to a subsequent time period, evidence of influence 
would be noted again, even if it represented a continuation from the previous interval. 
4. Although developmental use can occur within and across the time frames discussed 
in this chapter, Patton’s (1997) example of developmental evaluation as reflective prac- 
tice provides a clear instance of end-of-cycle influence. He describes a reflective cycle in 
which an issue is identified, a response is tried, the trial is observed, observations are 
reported and reflected on, patterns and themes are identified, action implications are 
determined, and the process is repeated. Note that this example includes data -based 
reflection, bridging process use and results-based use. 
References 
Alkin, M. C. Debates on Evaluation. Thousand Oaks, Calif.: Sage, 1990. 
Alkin, M. C., Daillak, R., and White, P. Using Evaluations: Does It Make a Difference? 
Thousand Oaks, Calif.: Sage, 1979. 
Anderson, C. D., Ciarlo, J. A., and Brodie, S. F. “Measuring Evaluation-Induced Change 
in Mental Health Programs.” In J. A. Ciarlo (ed.), Utilizing Evaluation: Concepts and 
Measurement Techniques. Thousand Oaks, Calif.: Sage, 1981.
<<<PAGE=17>>>
RECONCEPTUALIZING EVALUATION USE 2 1 
Anderson, S. B., and Ball, S. The Profession and Practice 
ofprogram EvahQtion. San Fran- 
cisco: Jossey-Bass, 1978. 
Argyris, C., Putnam, R., and Smith, D. M. Action Science. San Francisco: Jossey-Bass, 
1985. 
Bloom, M., Fischer, J., and Orme, J. M. Evaluating Practice: 
 Guidehesfor the Account- 
able Professional. (3rd ed.) Needham Heights, Mass.: Allyn & Bacon, 1999. 
Brisolara, S. “The History of Participatory Evaluation and Current Debates in the Field.” 
In E. Whitmore (ed.), 
 Understanding and Practicing 
 Purticipatory Evaluation. New 
Directions for Evaluation, no. 80. San Francisco: Jossey-Bass, 1998. 
Caro, F. G. “Leverage and Evaluation Effectiveness.” Evaluation and Program Planning, 
Christie, C. A., and Alkin, M. C. “Further Reflections on Evaluation Misutilization.” 
Studies in Educational Evaluation, 1999,25, 1-10. 
Ciarlo, J. A. “Editor’s Introduction.” In J. A. Ciarlo (ed.), Utilizing Evaluation: Concepts 
and Measurement Techniques. Thousand Oaks, Calif.: Sage, 1981. 
Conner, R. F. “Measuring Evaluation Utilization: A Critique of Different Techniques.” 
In J. A. Ciarlo (ed.), Utilizing Evaluation: Concepts and Measurement Techniques. Thou- 
sand Oaks, Calif.: Sage, 1981. 
Conner, R. F. “Toward a Social Ecological View of Evaluation Use.” 
ArnericanJournal of 
Evaluation, 1998, 19(2), 237-241. 
Cousins, J. B., and Leithwood, K. A. “Current Empirical Research on Evaluation Uti- 
lization.” Review of Educational Research, 
1986,56(3), 33 1-364. 
Cousins, J. B., and Whitmore, E. “Framing Participatory Evaluation.” In E. Whitmore 
(ed.), Understanding and Practicing Participatory Evaluation. New Directions for Eval- 
uation, no. 80. San Francisco: Jossey-Bass, 1998. 
Cronbach, L. J. Designing Evaluations of Educational and Social Programs. San Francisco: 
Jossey-Bass, 1982. 
Cronbach, L. J., and Associates. Toward Reform of Program Evaluation: Aims, Methods, 
and Institutional Arrangements. San Francisco: Jossey-Bass, 1980. 
Fetterman, D. M. “Empowerment Evaluation.” Evaluation Practice, 1994, 
IS( l), 1-15. 
Greene, J. C. “Communication of Results and Utilization in Participatory Program Eval- 
uation.” EvaIualion and Program Planning, 1988a, 11 (4), 341-351. 
Greene, J. C. “Stakeholder Participation and Utilization in Program Evaluation.” Evalu- 
ation Review, 1988b, 12(2), 91-116. 
Henry, G. T., and Rog, D. J. “A Realist Theory and Analysis of Utilization.” In G. T. 
Henry, G. Julnes, and M. M. Mark (eds.), Realist Evaluation: An Emerging Theory in 
Support of Practice. New Directions for Evaluation, no. 78. San Francisco: Jossey-Bass, 
1998. 
Hopson, R. K. “Editor’s Notes.” In R. K. Hopson (ed.), How and Why Language Matters 
in Evaluation. New Directions for Evaluation, no. 86. San Francisco: Jossey-Bass, 2000. 
Huberman, M., and Cox, P. “Evaluation Utilization: Building Links Between Action and 
Reflection.” Studies in Educational Evaluation, 1990, 16, 157-1 79. 
Johnson, R. B. “Toward a Theoretical Model of Evaluation Utilization.” Evaluation and 
Program Planning, 1998,21 (l), 93-110, 
Julnes, G., and Mark, M. “Evaluation as Sensemaking: Knowledge Construction in a 
Realist World.” In G. T. Henry, G. Julnes, and M. M. Mark (eds.), Realist Evaluation: 
An Emerging Theory in Support of Practice. New Directions for Evaluation, no. 78. San 
Francisco: Jossey-Bass, 1998. 
Kirkhart, K. E. “Consequential Validity and an Integrated Theory of Use.” Paper pre- 
sented at Evaluation 1995, international evaluation conference cosponsored by the 
Canadian Evaluation Society and the American Evaluation Association, Vancouver, 
B.C., November 1995. 
Kirkhart, K. E. ‘‘ Multifaceted Dimensions of Use: Intended and Unintended Influences.” 
Paper presented at the annual meeting of the American Evaluation Association, 
Orlando, Fla., Nov. 1999. 
1980,3(2), 83-89.
<<<PAGE=18>>>
22 THE EXFAND~NG SCOPE OF EVALUATION USE 
Knorr, K. D. “Policymakers’ Use of Social Science Knowledge: Symbolic or Instrumen- 
tal?” In C. H. Weiss (ed.), Using Social Research in Public Policy Making. Lexington, 
Mass.: Heath, 1977. 
Leviton, L. C., and Hughes, E.F.X. “Research on the Utilization of Evaluations: A Review 
and Synthesis.” Evaluation Review, 
1981,5(4), 525-548. 
McClintock, C., and Colosi, L. A. “Evaluation of Welfare Reform: A Framework for 
Addressing the Urgent and the Important.” Evaluation Review, 
1998,22(5), 668-694. 
Messick, S. ‘Validity of Psychological Assessment: Validation of Inferences from Per- 
sons’ Responses and Performance as Scientific Inquiry into Score Meaning.” American 
Psychologist, 1995, SO@), 741-749. 
Mushkin, S. J. “Evaluations: Use with Caution.” Evaluation, 1973, 1 (2), 30-35. 
Patton, M. Q. Utilization -Focused Evaluation. Thousand Oaks, Calif.: Sage, 1978. 
Patton, M. Q. Utilization -Focused Evaluation. (2nd ed.) Thousand Oaks, Calif.: Sage, 
Patton, M. Q. “Developmental Evaluation.” Evaluation Practice, 1994, 15(3), 31 1-319. 
Patton, M. Q. Utilization -Focused Evaluation: The New Century Text. (3rd ed.) Thousand 
Patton, M. Q. “Discovering Process Use.” Evaluation, 1998,4(2), 225-233. 
Patton, M. Q. “Overview: Language Matters.” In R. K. Hopson (ed.), How and Why Lan- 
guage Matters in Evaluation, New Directions for Evaluation, no. 86. San Francisco: 
Jossey-Bass, 2000. 
Preskill, H., and Caracelli, V. J. “Current and Developing Conceptions of Use: Evalua- 
tion Use Topical Interest Group Survey Results.” Evaluation Practice, 1997, 
18(3), 
Rein, M., and White, S. H. “Can Policy Research Help Policy?” Public Interest, 1975,49, 
Rich, R. F. “Use of Social Science Information by Federal Bureaucrats: Knowledge for 
Action Versus Knowledge for Understanding.” In C. H. Weiss (ed.), Using Social 
Research in Public Policy Making. Lexington, Mass.: Heath, 1977. 
Rippey, R. M. (ed.). Studies in Transactional Evaluation. Berkeley, Calif.: 
McCutchan, 
1973. 
Rodman, H., and Kolodny, R. “Organizational Strains in the Researcher -Practitioner 
Relationship.” In C. H. Weiss (ed.), Evaluating Action Programs: Readings in Social 
Action and Education. Needham Heights, Mass.: Allyn & Bacon, 1972. 
1986. 
Oaks, Calif.: Sage, 1997. 
209-225. 
119-136. 
Scriven, M. Evaluation Thesaurus. (4th ed.) Thousand Oaks, Calif.: Sage, 1991. 
Shadish, W. R., Jr., Cook, T. D., and Leviton, L. C. Foundations of Program Evaluation: 
Theories of Pructice. Thousand Oaks, Calif.: Sage, 1991. 
Shulha, L. M., and Cousins, J. B. “Evaluation Use: Theory, Research, and Practice Since 
1986.” Evaluation Practice, 1997, 18(3), 195-208. 
Smith, M. F. “Evaluation Utilization Revisited.” In J. A. 
McLaughlin, L. J. Weber, R. W. 
Covert, and R. B. Ingle (eds.), Evaluation Utilization. New Directions for Program Eval- 
uation, no. 39. San Francisco: Jossey-Bass, 1988. 
Smith, N. L,, and Chircop, S. “The 
Weiss-Patton Debate: Illumination of the Funda - 
mental Concerns.” Evahalion Practice, 1989, 10(1), 5-13. 
Suchman, E. A. Evaluative Research: Principles and Practice in Public Service and Social 
Action Programs. New York: Russell Sage Foundation, 1967. 
Tornatzky, L. G. “The Triple-Threat Evaluator.” Evaluation and Program Planning, 1979, 
2(2), 111-115. 
Weiss, C. H. “Knowledge Creep and Decision Accretion.” Knowledge: Creation, Utiliza - 
tion, Diffusion, 
1980,1(3), 381-404. 
Weiss, C. H. “Measuring the Use of Evaluation.” In J. A. Ciarlo (ed.), Utilizing Evalua - 
tion: Concepts and Measurement Techniques. Thousand Oaks, Calif.: Sage, 1981. 
Weiss, C. H., and 
 Eucuvalas, M. J. “Truth Tests and Utility Tests: Decision -Makers’ 
Frames of Reference for Social Science Research.” American Sociological Review, 1980, 
45,302-313.
<<<PAGE=19>>>
RECONCEPTUALIZING EVALUA-LION USE 23 
Whitmore, E. “Evaluation and Empowerment: It’s the Process That Counts.” Empower - 
ment and Family Support Networking Bulletin (Cornell University Empowerment Pro- 
ject), 1991, 2(2), 1-7. 
Wholey, J. S. “Assessing the Feasibility and Likely Usefulness of Evaluation.” In J. S. 
Wholey, H. P. Hatry, arid K. E. Newcomer (eds.), Handbook 
ufPractical Program Eval- 
uation. San Francisco: Jossey-Bass, 1994. 
KAREN E. 
KIRKHART is 
projcssor of social work at Syracuse University. Her inter- 
ests include 
evaluation and social justice and the validity of evaluation in mul- 
ticultural contexts.