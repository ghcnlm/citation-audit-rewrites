<<<PAGE=1>>>
CHAPTER 5
How Policy Appetites Shape, and Are
Shaped by Evidence Production and Use
Kathryn Oliver
“New York is not a City of Alleys”, Nick Carr, Location Scout
“I’ve worked on more ﬁlms that want to ﬁnd the imaginary version
of New York than the real. The big thing I always get asked to ﬁnd
are dank dilapidated alleys, and New York City has, like, 5 alleys that
look like that. Maybe four. You can’t ﬁlm in three of them. So what
it comes down to is there’s one alley left in New York, Cortlandt
Alley, that everybody ﬁlms in because it’s the last place.
I try to stress to these directors in a polite way that New York is not
a city of alleys. Boston is a city of alleys. Philadelphia has alleys. I
don’t know anyone who uses the ‘old alleyway shortcut’ to go home.
It doesn’t exist here. But that’s the movie you see. Your impression
of New York is that it is the city of alleys, and then directors will
K. Oliver ( B)
Faculty of Public Health and Policy, London School of Hygiene and Tropical
Medicine, London, UK
e-mail: Kathryn.Oliver@lshtm.ac.uk
© The Author(s) 2022
P . Fafard et al. (eds.), Integrating Science and Politics for Public Health ,
Palgrave Studies in Public Health Policy Research,
https://doi.org/10.1007/978-3-030-98985-9_5
77
<<<PAGE=2>>>
78 K. OLIVER
come here, they’ve seen movies set in New York and they want their
movies to have alleys.
And it’s this self-perpetuating ﬁctional version of New York that
just kills me because movies are so much more interesting when
you show a side of New York that actually exists but isn’t regularly
highlighted”.1
1 Introduction
For many years, researchers have advocated for greater research impact
on policy. This advocacy has often, in an attempt to be helpful, taken
the form of specifying preferred types of evidence (the randomised
controlled trial or systematic review, for example) and preferred direc-
tions of policy change. A simplistic model, often termed the technocratic
or rationalist model, of knowledge uptake is presented: a problem is iden-
tiﬁed, the most ‘robust’ research evidence possible is created to solve this
problem, the research recommendations are implemented, and the policy
problem is solved. Policymakers—who draw on many and varied kinds
of evidence—have responded to this advice by funding and supporting
particular versions of knowledge (e.g., trials units, systematic review facil-
ities such as the What Works Centres and the National Institute for Health
Care and Excellence in the UK).
Yet the production of evidence, and its use are far from simple
processes. From discussions about the plurality of evidence (Parkhurst &
Abeysinghe, 2016; Petticrew & Roberts, 2003), to the politicisation of
research systems and the role research plays in the world, many have
argued against the simplistic view of ‘best’ evidence put forward above,
which ignores both values and people. So why does this narrative remain
so powerful, even though even many of its proponents would agree that it
is an overly simplistic way of understanding how evidence informs policy?
Unfortunately, the dominance of the rationalist model really matters
because it affects how policy appetites for evidence, and the actual
production of evidence. It narrows the range of evidence available to
policymakers in shaping and framing problems and solutions, and conse-
quently there is less support for research which sits outside these framings.
This in turn has led to misunderstandings, methodological in-ﬁghting,
1 https://www.citylab.com/design/2011/11/ﬁlm-location-scout-pet-peeves/521/
<<<PAGE=3>>>
5 HOW POLICY APPETITES SHAPE … 79
misuse of evidence by decision-makers, and vested interests. What might
an alternative be?
To better understand how policymakers ﬁnd and use evidence, we
need a broader lens to examine the political economy of knowledge. By
understanding what knowledge is, and ‘its forms of extraction, points
of commodiﬁcation, how it is reﬁned as intellectual property’ (Tilley,
2017), we can better conceptualise its role in decision-making, and begin
to imagine the broader evidence-policy system within which knowledge
is exchanged. Between 2014 and 2019 in the UK, I conducted 91
interviews with researchers and policymakers in the UK, discussing the
challenges of evidence use in policy. I draw on these interviews in this
chapter to explore how knowledge production, mobilisation and use
shape and are shaped by policy appetites. This offers a new way to begin
thinking about how to creatively shape a more helpful environment for
both policy and evidence.
2 The Rationalist Model
Researchers and policymakers alike have sought to conquer the challenge
of improving health and social outcomes by implementing improved
decision-making. Evidence and data use have for decades, if not centuries,
been at the heart of this drive. For many, the relationship between these
processes is a straightforward, linear one of problem deﬁnition, solution
creation, and implementation. In this vision, research is there to provide
solutions for real-world problems faced by policymakers and practitioners.
For their part, policymakers and funders have made investments in applied
research tied to explicit policy priorities, with an emphasis on disciplines
deemed likely to produce ‘economic value’ (Bastow et al., 2015). This has
broadly meant spending money on solutions-oriented evidence, or to put
it another way, research which assists policymakers in selecting options for
policy implementation.
From the very beginnings of the evidence-based medicine and
evidence-based policy movements (in the UK, usually agreed to be
Cochrane’s,1972 ‘Effectiveness and Efﬁciency’ report (Cochrane, 1972;
Oliver & Pearce, 2017), through the growth of evidence-synthesis organ-
isations, to the training of individual researchers to increase their impact,
this has come to mean a particular form of evidence and research. Under-
pinning this is the notion of the hierarchy of evidence, which is a heuristic
describing the strength of different methodological designs. Although
<<<PAGE=4>>>
80 K. OLIVER
extensively critiqued, it has also been translated into outcomes-focused
decision-making tools such as GRADE (Movsisyan et al., 2018; Shen-
derovich et al., 2019). This hierarchy afﬁrms and assigns value to different
pieces of evidence on the basis of research design; in particular, the
randomised controlled trial (RCT), and the systematic review or evidence
synthesis. RCTs are valued because this research design minimises the
chance that random chance has led to the research ﬁnding, meaning that
they have high internal validity, and that readers can have conﬁdence that
the research ﬁnding is reproducible. The systematic review exhaustively
brings together evidence on a particular question, assessing the strengths
and weaknesses in a body of work. Systematic reviews of RCTs are consid-
ered particularly robust—the peak of the evidence hierarchy—but both
are prioritised and highly valued (de Souza Leão & Eyal, 2019;P e a r c e&
Raman, 2014;W h i t e , 2019). As ably recounted elsewhere, RCTs have
been around for almost a century, but in the last 20 years there has been
a huge increase in their funding and use (Deaton & Cartwright, 2018;
Pearce & Raman, 2014).
The seductively simple process offers an attractive vision of a world
where, if only enough research evidence were available, acted on by
willing and capable decision-makers, life in general would be better. We
could summarise this view as:
 Policy is best made using research evidence.
 RCTs and systematic reviews are the best kind of evidence.
 Researchers should do more, better RCTs and SRs, and maximise
their use by policymakers.
This is a very technocratic, rationalised view of policymaking which is still
widely held (Wood, 2019). Thus, we ﬁnd researchers offering aspirational
views of their possible impact:
For our department, [impact] means having certain policies and practices
put in place because of our research. (Academic, criminal justice)
Large national bodies who would then take our research and maybe them-
selves translate it into guidance, which might be used by non-scientists and
non-researchers. (Social scientist)
<<<PAGE=5>>>
5 HOW POLICY APPETITES SHAPE … 81
Well it’s good to feel actually that the policy is becoming more evidence
based, as long as it doesn’t turn into some sort of matrix based thing
where you think you measure something and we should change the world
to increase that. Everything is matrix based and you can’t do anything if
you can’t ﬁnd a matrix. But I think that rational view is welcomed. (Social
scientist)
I would like our research to ultimately result in some change in the energy
system and since we are not in control of the energy system and we do
not build energy systems ourselves that means that we will have to have
our impact through working with partners. (Engineer)
The version of policy decision-making which these researchers share is
quite clear:
It’s really important that policy is based on the best evidence that’s
possible. (Engineer)
Policymaking is seen to be optimised by easy access to high-quality,
systematically identiﬁed and analysed evidence, which then forms the
primary “input” to the policy process (see, e.g., Oxman et al., 2009:1 ) .
The steps within this process are then laid clear for all to see, to enable
‘accountability’ and revision.
2.1 The Dominance of the Rationalist Model
Given the perceived simplicity of this process, it may be puzzling to its
proponents why these improvements often fail to materialise in the real
world. Scholars—particularly from the social and political sciences—have
problematised the relationship between evidence and policy, recognising
its complexity. The policy sciences have demonstrated convincingly that
policy operates in a complex, even chaotic fashion. The linear model
(positing problems, solutions, evaluations of these solutions, and thus
improved policy) bears little resemblance to the multi-level complex adap-
tive governance that characterises most legislative systems. These days,
most policymakers and many (especially social and political researchers)
believe that a pluralist, diverse evidence base is the ideal starting point
for decisions to be made about public policy and practice (Head, 2008).
As a way of achieving this aim, it is now fairly common to see calls
for more deliberative, democratic approaches to knowledge production
<<<PAGE=6>>>
82 K. OLIVER
and use (Degeling et al., 2017; Stewart, 2017); see Chapter 13 (Cassola
et al., 2022) and Chapter 4 (Kothari & Smith, 2022). This approach
recognises that all forms of knowledge are social, in the sense that they
are interpreted by humans within social settings, and therefore driven by
and subject to societal and political values and interests (Douglas, 2009;
Fafard, 2015; Jasanoff & Polsby, 1991). However, despite these efforts,
all too frequently the ‘problem’ is seen as being a ‘lack of data’, ‘lack of
evidence’, or perhaps ‘poor evidence uptake’ (where the evidence exists,
but is not acted on).
2.2 Why Has This Rationalist Model
Held Strong, and Does It Matter?
One reason may be because (ironically) social and political scientists have
tended to emphasise the complex nature of policymaking and the intran-
sigent nature of the challenges facing decision-makers. While not wishing
to argue with either of these characterisations, a lack of clear, informed
lessons for other researchers and decision-makers may have meant that
many relied on simplistic, easy-to-understand models of the policy world.
So, for many researchers new to this ﬁeld, their only way of getting
a handle on what policy is, is to learn from the informal discussions
between academics, from funders, or from university-led training courses.
These tend to produce generic discourses of a simpliﬁed version of how
evidence and policy interact, drawing on the misleading advice of unusu-
ally successful academics, or otherwise aiming to equip researchers with
the idea (and tools to help) of maximising their impact. For example,
many universities in the UK and internationally seek to increase their
inﬂuence in the policy world. To do this, they encourage researchers
to engage with government through in-house courses and incentive
structures (Fafard & Hoffman, 2020; Hopkins et al., 2021). While well-
intentioned, universities tend to rely on the rationalist model of research
impact—perhaps because their teaching and examples are often derived
from high-status researchers and projects from the faculties of medicine
and science, not the political and social sciences. The alternative to the
rationalist model is a highly complex evidence-policy ecosystem. For many
this is hard to conceptualise, and researchers may feel is too difﬁcult to
engage with and inﬂuence as a whole. Thus, researchers and policymakers
have formed, and are able to continue to promote, an unrealistically
<<<PAGE=7>>>
5 HOW POLICY APPETITES SHAPE … 83
simple view of the nature of policy and evidence which is both rationalist
and technocratic.
From the political and social sciences, attempts to challenge this over-
simpliﬁed story have resulted in better conceptualisations of the nature
of the problem, but not really in actionable next steps for those wishing
to improve the situation. For example, challenges to the evidence hier-
archy mostly took the form of methodological debate about the quality
of different social research methods (Hammersley, 2005) and the appro-
priateness of different research designs for use in public policy (Cairney &
Oliver, 2017;H e a d , 2008). These limitations to the RCT are well-
documented. RCTs may not be appropriate where complex outcomes
may be of interest (such as patient preferences or experiences); policies
may operate at a different (e.g., whole-nation) level which is impossible
to be randomised; or, as is the case for most public policy interventions,
operate within a complex, ever-changing social environment with multiple
competing policy interventions inﬂuencing individuals at different levels.
These limitations to the RCT are of course extremely well-documented
and understood by the research and policy community at large. However,
this wide understanding of what they can, and cannot tell us has not
prevented the even wider uptake of the hierarchy of evidence as a rule of
thumb within policy and policy-research circles.
If we look at the three aspects of knowledge production, mobilisation
and use together we can see they are a system over and through which
we work as individuals and institutions. We operate within a system of
funding, institutional roles and activities which incentivises certain activ-
ities and behaviours. Any radical approach would need to reimagine this
system, but would in doing so challenge deeply held views about how
decisions should be made (i.e., based on expertise and/or ‘best’ evidence)
and indeed about the role of science in society.
Yet technocratic and normative polemics are hardly rare. In recent
years, there has been a slew of talks and publications in the UK alone
calling for more data-driven, technocratic decision-making (Haynes et al.,
2012;W h i t e , 2019). It is not hard, for instance, to ﬁnd examples
of people advocating for data-driven policymaking with no recogni-
tion of the social (and thus non-objective) nature of this data; for the
need for more RCTs to inform public policy; and for the importance
of strengthening technocratic decision-making structures (Watts, 2019;
White, 2019). The COVID-19 pandemic which began in early 2019
<<<PAGE=8>>>
84 K. OLIVER
is a good example of this cognitive dissonance. It has impacted virtu-
ally every population on the planet, with governments adopting a slew
of different policy responses to the huge challenge, with different goals
(virus transmission suppression, containment, elimination, management),
and different strategies to reach these goals (investment in vaccines,
additional healthcare resources, public safety announcements, population
control measures such as lockdowns, new legislative powers). Yet many in
the public health research world continued to insist that evidence needed
to be “robust enough” before acting on (meaning, it needed to be RCT
evidence). As has been argued, this is simply not an appropriate form
of knowledge required to answer the questions raised by the pandemic
(Greenhalgh, 2020). Most governments did of course use other forms of
evidence, but tended to rely on highly quantitative and—by necessity—
reductionist modelling techniques to inform decision-making, rather than
on, for example, discussions with anthropologists or sociologists (Cairney,
2021).
The key lesson from the many analyses already written about the covid
pandemic, and indeed other disasters, crises, and challenges of more ordi-
nary policymaking, is that multiple forms of knowledge are required
(Jasanoff & Polsby, 1991; Sarewitz, 2018; Wynne, 2013). A mixed
economy of knowledge and expertise enables a more honest conversa-
tion about what implications there are for decision-making. And focusing
on how political and social pressures shape our evidence base allows us all
to better understand how problems and solutions are framed. How might
we do that?
3 An Alternative: The Political
Economy of Knowledge?
Tilley deﬁnes the political economy of knowledge as studying ‘its forms
of extraction, points of commodiﬁcation, how it is reﬁned as intellectual
property’ (Tilley, 2017). In short, focusing attention on what is presented
and preferred allows us to ask critical questions about who is able to
participate in knowledge production and why, what is valued and why,
and the impact of these relationships. Using this lens, one can begin
to see how misunderstandings, methodological in-ﬁghting, and vested
interests shape the evidence available to policymakers, and how this land-
scape shapes the environment for knowledge production. This offers a
<<<PAGE=9>>>
5 HOW POLICY APPETITES SHAPE … 85
new way to begin thinking about how to creatively shape a more helpful
environment for both policy and evidence.
Using this lens, I argue how we produce, mobilise, and use evidence
has been shaped by the rationalist model, and how this model has shaped
policy appetites and continues to inﬂuence how we all do our work—
even though its failings are so widely understood. I argue that this
simple narrative has shaped the evidence-policy environment in three
main ways. Firstly, policymakers and researchers shape the evidence base
through supporting and creating particular forms of evidence. Secondly,
it shapes how evidence is mobilised , through offering roles and activities for
researchers and others to follow. Thirdly, it shapes how evidence is used by
policymakers, including selective evidence use.
3.1 Shaping the Evidence Base
The rationalist model of evidence-informed policymaking tells us that the
main priority for most research funders and researchers is on how to
improve the quality of the evidence base. As recounted above, for many
this has meant more investment in RCTs. The Education Endowment
Foundation (EEF) has, for example, “conducted over 80 randomised
controlled trials - often held up as the gold standard in evaluation – and
have around 80 more in the ﬁeld”. This is described as “hugely impres-
sive” (Sanders, 2019). The UK government recognised the importance of
RCTs in a report written on behalf of the Cabinet Ofﬁce (Haynes et al.,
2012) which argued for more and more RCTs. Alongside, this growth
has been a push for more systematic reviews—explicitly, in the case of
institutions such as the National Institute for Health and Clinical Excel-
lence (NICE) and the rest of the What Works organisations, which tend
to produce systematic reviews to inform policy and practice decisions. As
White describes, “more reviews, and more use of reviews” (White, 2019,
p. 4) are the explicit aims of these institutions.
In reality, of course, policymakers consume a far more heterogeneous
evidence diet than simply RCTs and SRs (Oliver & de Vocht, 2017). Yet
for a variety of reasons (including peer pressure, professional standards,
and research governance), researchers tend to focus on creating new inter-
ventions and evaluations, rather than—for example—analysing local data
on behalf of decision-makers. The relative under-investment in social and
political research, compared with the vast amounts spent on physical and
health sciences, is, I argue, a reﬂection of the way in which the rationalist
<<<PAGE=10>>>
86 K. OLIVER
model has shaped policymakers’ appetites for a particular diet of evidence
(Bastow et al., 2015).
Incentive structures within research organisations tend to encourage
researchers to do more research (Sarewitz, 2018), of particular design
(Oakley, 1990); not to focus on other activities such as ‘getting to know
policymakers’ or mobilising research effectively (Bammer, 2005; Ferlie
et al., 2012;P o w e l le ta l . , 2018). Researchers thus conduct what they
consider to be policy-relevant research, which is considered attractive
by policymakers, who then ask for more of the same. Public health
policy researchers have sometimes referred to this as ‘lifestyle drift’, where
despite understanding the critical role of wider determinants of health
like poverty and employment, both researchers and governments focus
on policies and programmes which operate at the individual level (Powell
et al., 2017; Rutter & Glonti, 2016). RCTs and experimental studies
are well-suited to assessing individual-level interventions, such as the
‘nudge’ techniques which ‘encourage’ people to make ‘better choices’
(Thaler & Sunstein, 2008). Today, governments around the world spend
billions on R&D, investing in systematic review facilities with explicit
remits to inform policy and practice. The What Works model—that is,
framing research questions around a solutions-oriented action, answer-
able by RCT and systematic review,digested with implications for policy
and practice—has been exported around the globe (Boaz et al., 2019;
Parkhurst, 2017).
One way of disrupting this feedback loop is to make evidence
production more democratic and participatory, through co-designing
policy-relevant research or interventions, for example, with end-users
(see Chapter 13, Cassola et al., 2022). But these are not without their
own challenges. How, for instance, could one easily involve a representa-
tive sample of all practitioners, ofﬁcials, politicians, parents, community
members, children, professionals who may be affected by a particular
policy? How would one identify and reach out to these groups? What
about the groups who would be affected by resources being withdrawn
from elsewhere to support a new policy? These are extremely challenging
steps to take within the conﬁnes of a (normal) research project, which
risks codesign and coproduction becoming an overly simpliﬁed, even
tokenistic process which does little to challenge existing social processes
and structures through research (Oliver et al., 2019).
How can we reset this conversation between producers and users of
evidence? One approach would be to imagine and critically assess the
<<<PAGE=11>>>
5 HOW POLICY APPETITES SHAPE … 87
knowledge-policy system in its entirety. One would wish to examine
how research funders (including governments) decide on priorities, how
these priorities are enacted via funding instruments, committees, and peer
review processes, and how researchers respond to these interventions.
One could then ask questions about who was involved in these insti-
tutions and processes at different levels, and how representative these
populations of actors were, compared with those on whom the research
may ultimately seek to have impact. There are signiﬁcant bodies of work
which examine research funding allocation (Jones & Wilsdon, 2018;
Shepherd et al., 2018), the reliability and replicability of research (Bishop,
2019; Ioannidis, 2005), and the need for transparency and ‘openness’ in
scientiﬁc practices (Nosek, 2017). While important, these efforts engage
primarily with research practices, not with the political dynamics shaping
the evidence base, which lead to what Fricker calls “epistemic injustice”
(Fricker, 2007). In essence, if some groups are prevented from having
a voice—through lack of participation or representation in research, for
example—then they become further disempowered, and policy continues
to reinforce existing power imbalances (Holliman, no date). We look
towards inclusive research practices (Duncan & Oliver, 2017; Stewart,
2017) to redress this balance, but we need clear, critical perspectives
on the roles of sexism, racism, and other biases to explore how our
knowledge production systems and outputs could be more representative.
Rather than simply assessing which types of research design were
preferred or arguing over which types of research were ‘better’ for policy,
one would wish to assess the broad and catholic appetite for data and
evidence of all kinds within policy, and seek to meet and expand this
appetite with robust evidence of many types. Perhaps most importantly,
one would wish to expand the common understanding of ‘evidence
production’ to include all these social and political processes, rather than
to focus merely on what research exists, what is ‘best,’ and what to do
next.
3.2 Shaping Evidence Mobilisation
The simple rationalist, technocratic model of RCT to policy also shapes
how researchers and policymakers look for, promote, and engage with
evidence. As has been already described, they fund particular organi-
sations and structures which make evidence digestible and accessible,
such as systematic reviews or policy briefs. The translation of research
<<<PAGE=12>>>
88 K. OLIVER
into actionable professional/practice guidelines is a key mode of knowl-
edge mobilisation, and the What Works Centres (among others) and
the multiple forms of policy lab, unit, or institute attached to univer-
sities serve a similar purpose. Yet these activities inevitably focus on the
evidence which is there, not the gaps. Many people call for more syntheses
(Donnelly et al., 2018), while acknowledging the problems with biased
indexing, publication, and dissemination, but producing more academic
papers is not going to address the systemic barriers to evidence use. For
example, how do researchers and knowledge mobilisers use messaging
and communication tools to persuade and engage with decision-makers?
If we reject the hierarchy of evidence, how can different forms of evidence
be assessed and compared? Movements such as Democratising Evidence
2
and Research for All 3 have begun these conversations, by publishing
non-academic outputs and committing to diverse and representative
writing teams—but this is still within the context of research production.
More thought is required on how this could be operationalised within
decision-making contexts.
In addition to the institutional context, the roles of individuals within
the rationalist model are clear. Academics and researchers are there not
to learn or discuss, but rather as experts there to inform and advise on
policy issues:
I think they’d like to think that their decision-making processes are at
least informed by in-house analysis, and then the evidence base” (Health
scientist)
You are advising the chief scientists and they are advising the government
on speciﬁc policies. (Social scientist)
2 Democratising Evidence is a movement within several disciplines which involves recog-
nising the potential of research as a vehicle for public engagement and equity. See, e.g.,
Nowotny, H. (2003). Democratising expertise and socially robust knowledge. Science and
Public Policy, 30 (3), 151–156.
3 Research for All is an academic journal focusing on research that involves universi-
ties and communities, services or industries working together. Contributors and readers
include researchers, policymakers, managers, practitioners, community-based organisa-
tions, schools, businesses, and intermediaries. It showcases research done collaboratively
and builds a community across academic disciplines, professional sectors, and types of
engagement.
<<<PAGE=13>>>
5 HOW POLICY APPETITES SHAPE … 89
Policymakers are there to receive wisdom, by selecting the best evidence
available to them. Researchers are encouraged to make their evidence
competitive by attending training courses on ‘how to increase your
impact’ to use rhetorical techniques as their opponents, such as appealing
to human values and experience, using stories to frame policy debates,
and being able to charm and dazzle when networking with policymakers
(Oliver et al., 2022; Zardo et al., 2014). Researchers acknowledge that
You’ve got to be very careful because the point is, we’re not supposed to
be marketing our own research and arguing for our own funding. (Social
Scientist)
but nevertheless, feel they should advocate for policy positions. Here,
a public health clinician/researcher describes how they advised a local
commissioner that stroke services should be reorganised towards early
intervention in specialist services; despite this not being his specialism:
One of the most important things I ever did in my medical career was
advise a fellow councillor about a new paper that - he’s an engineer and
couldn’t understand it, and I read it to him and said this is good and he
went on and he almost single-handedly rearranged stroke management in
this country … I read this thing on stroke for him, and I said, “Yes, the
science is ﬁne... What they’re saying is what happens, I’m happy with that,
no major issues. (Public health clinician)
Many may feel that this is unproblematic, as to compete with other
interests within the policy domain, one has to overstate to win any
ground. Yet, this overreaching beyond one’s expertise, or even beyond
the research data can call into question the moral compass of universities
and researchers in general:
the Universities will do anything if you turn to the universities and say well
you know I’m really interested in dancing frogs, off they’ll nip. They’ll be
like, where’s the grant, where’s the money, where’s the publication. (Public
health researcher)
By extension the credibility of research in general can be questioned,
where “scientists as ‘strategists’ engaged in a struggle for credibil-
ity” (Brown, 2015). As described above, by instating that RCT and
systematic review evidence forms the only credible basis for action,
<<<PAGE=14>>>
90 K. OLIVER
researchers have opened the door to a relatively easy way for other actors
to establish themselves as credible participants in policy debates (see
Chapter 9, Hawkins & Oliver, 2022). Much has been written about the
ways in which corporations create and curate evidence bases in order to
generate lack of certainty, or to attack policy positions which might be
detrimental to their growth. One of the starkest examples regards the
use of albumin in post-operative patients multiple RCTs were undertaken
between 1987 and 2005. A meta-analysis from 2005 showed that albumin
killed more critical care patients than saline, and crucially, that this would
have been was established by 1989, but for a further 30 years, more RCTs
were done mostly funded by albumin producers. The existence of on-
going research allowed them to say that it was not yet a settled question,
but they were doing their best to establish what was the optimal treatment
for patients (Chalmers, 2006). Elsewhere, colleagues have documented
the ways in which commercial companies (predominantly pharma, food
and alcohol, and tobacco companies) have used this tactic to create
uncertainty, establish themselves as credible actors in the policy space,
and undermine detrimental policies (Hawkins & Ettelt, 2018; Kickbusch
et al., 2016; Knai et al., 2018;M c K e e&S t u c k l e r ,2018). The existence of
a robust evidence base—whether attached to a university, policy, or other
entity—thus begins to offer the impression of credibility and security; and
its absence, cause for concern (Oreskes & Conway, 2010). If an evidence
base can be pointed to or created, the policy or actor is able to present
themselves as a disinterested participant in policy debates.
We must acknowledge that appeals to evidence cannot always, and in
fact rarely offer clear ways to navigate the political and social challenges
of our times (Deeming, 2013). A clearer picture of our values and prin-
ciples can clarify our aims, what we ask of the evidence base, and the
various roles for researchers in this knowledge economy. On the surface,
pressures on researchers and funders to increase their ‘impact’ are, no
doubt, well-meaning in their intention to improve outcomes for society
through ‘improved’ decision-making. However, the continued insistence
that there is a ‘right’ form of evidence which should inform decision-
making in the ‘right’ way has several unfortunate effects. As shown, it
effectively operates as a counter-argument to those who call for more
inclusive and participatory approaches to making and using knowledge.
It creates a hierarchy of knowledge which allows research users to assign
basically arbitrary values to different pieces of evidence such that some is
more ‘worthy’ than others. This can be taken as a proxy for credibility,
<<<PAGE=15>>>
5 HOW POLICY APPETITES SHAPE … 91
which in turn allows policy proposals to be attacked on this basis. And
ﬁnally, it shapes the behaviours of both research users and producers, in
that it creates perverse incentives for researchers to present themselves
and their work in particular ways which may undermine their credibility.
3.3 Shaping Evidence Use
Finally, how policymakers use evidence itself is also shaped by the
simplistic narrative. By focusing on quality and robustness of research,
researchers were naturally enough focusing on those elements in the
research-policy environment within their reach; but neglecting to think
through either the consequences of these debates, or the broader context
within which they were working. For instance, insisting that RCTs are the
most, even the only valid form of evidence enables policies to be attacked
for not being based on RCTs, even where this might be impossible
to achieve or inappropriate. RCT evidence is well-suited to establishing
effectiveness of individual-level interventions where outcomes are easily
quantiﬁable and measured. Many aspects of public health and social policy
and the associated interventions do not ﬁt these criteria. This reality was
underscored during the COVID-19 pandemic when debates arose about
whether governments should adopt policies with respect to making, phys-
ical distancing, or vaccination in the absence of completed RCTs. It is
possible that by sidestepping the broader questions of validity to focus
on methodological robustness, researchers have enabled policymakers to
seize on the RCT as a talisman of quality, making policies harder to chal-
lenge and depoliticising, or rather defusing debates about which policies
ought to be implemented.
A strong focus on experimental evidence all ows policymakers to sidestep
important questions about systemic problems . Recent sociological work has
pointed to similarities between “randomistas”, that is, proponents of
RCTs, and philanthro-capitalists in their belief in measurement, mistrust
in experts, belief in experimentation as a means to achieve ‘leverage, and
unstated but present liberal paternalism (de Souza Leão & Eyal, 2019;
Deeming, 2013). For example, De Souza Leão et al. describe the case of
a deworming RCT in Africa which showed increased educational attain-
ment for both treated and untreated children, although this later proved
to be unreplicable. Millions of dollars were invested in deworming, rather
than in improving school access, teacher training, or the many other
elements which combine to inﬂuence educational attainment. De Souza
<<<PAGE=16>>>
92 K. OLIVER
Leão et al. show that the presence of the RCT allowed donors and
decision-makers to sidestep complex, moral questions about resource
allocation and systems change, focus on what could be measured, and
evaluate only what was easily available. This is important, as these evidence
bases then become the justiﬁcation for further political action.
Another example concerns the UK Department of Health refusing to
fund an RCT of the Sure Start programme (thus ensuring no negative
results could be found (Melhuish et al., 2008, 2010). The preference for
certain methods and epistemologies allows policymakers to use legitimate
concerns about methods or generalisability to undermine and dismiss
evidence which may be inconvenient. This could enable the politicisa-
tion of research activities, where researchers are unable to test hypotheses
effectively, nor able to discuss their ﬁndings openly or honestly (Hartley
et al., 2017).
Thus, by insisting on the primacy of certain forms of knowledge,
researchers may be opening the door to a policy focus on interventions
for which experimental data is available and proliferating. Researchers and
funders thus interpreted policy as being rationalist and technocratic, and
responded with an increased focus on individual-level interventions, in
a feedback loop, leading to an overall lack of attention to gaps in the
evidence base, possible alternative policies, systems-level thinking, and
non-incremental change (Baum & Fisher, 2014).
The rationalist model also enables poor evidence use behaviours among
policymakers. For example, cherry-picking data. The classic example is the
youth recidivism intervention, Scared Straight, in which ‘at-risk’ youths
were exposed to the prison environment, in an attempt to demonstrate
its awfulness and prevent further crime. Proponents of Scared Straight
prefer an evaluation which demonstrates raised awareness of prison imme-
diately following the visit (Finckenauer & Finckenauer, 1999;P e t r o s i n o
et al., 2003), which they argue demonstrated effectiveness, but a system-
atic review of long-term evaluations shows increased offending in the
intervention arm (Petrosino et al., 2013). Similarly, the UK govern-
ment’s ﬂagship Troubled Families policy has shown no effect on its
target outcomes “despite persistent claims by politicians that it had
‘turned around’ the lives of tens of thousands of families and saved
over a billion pounds” (Butler, 2016). Sure Start is talked about both
as a success (Glass, 1999; Melhuish et al., 2010) and a failure (Clarke,
2006; Melhuish et al., 2008), according to whether one measures social
exclusion/participation, or educational attainment.
<<<PAGE=17>>>
5 HOW POLICY APPETITES SHAPE … 93
Evidence is ‘used’ when it enables a decision to be made. Yet we know
that decisions are rarely clear-cut, and where they are, evidence rarely
allows decision-makers to choose between options (Cairney et al., 2016).
To better understand how evidence is used in decision-making, we must
move past diagnosing ‘correct’ types of evidence or ‘correct’ types of use,
towards understanding the knowledge ecosystem within which the policy
problems are framed and discussed.
4 Conclusions
Evidence is shaped by those who create it (as funders, as participants,
or as researchers), those who curate and promote it (as writers, dissem-
inators, or synthesisers), and by those who use it. As Weiss argued
in 1977, improving evidence ‘use’ means, fundamentally, improving
decision-making. Her primary concern, as an evaluator and scholar of
public policy, was on how to improve the quality of public decision-
making; and for her, as for many of us, improving use of research evidence
played an important role in that process. Yet almost from its inception,
the evidence-based policy and practice movement has somehow conﬂated
these goals. Parsing them out allows us to ask what ‘quality’ evidence or
decisions look like, and who should participate in them. But too often,
the assumption has been made that good evidence will automatically lead
to better decisions.
Despite a good understanding of how evidence and policy interact
developed in the social and political sciences, many researchers continue
to misunderstand policy. Even when acknowledging its complexity and
arbitrariness, they offer rationalist conceptualisations, and technocratic
preferences regarding decision-making. In this world, policymakers seek
(and are offered) clear answers to deﬁned problems, or arbitration
between clear policy options. The researcher becomes an individualist
entrepreneur, attempting to maximise their own inﬂuence, often without
considering the moral, ethical, or political dimensions of their claims or
actions.
The technocratic vision has real dangers for democratic decision-
making, and for the credibility of evidence more generally. Either the
technocrats are simply unaware of the strength of the arguments made by
the democratisers, or they disagree with their stance. Is any reconciliation
possible?
<<<PAGE=18>>>
94 K. OLIVER
Framing the relationships between research and evidence production,
mobilisation, and use as a social construction shaped by power dynamics
and social interactions allows us to interrogate how these forces deter-
mine behaviours and outcomes, and we can start to see the knowledge
economy as an interrelated, mutually shaping dynamic system. Bringing
critical perspectives into this systemic approach to the study of knowledge
production, mobilisation, and use, we can illuminate the social pressures
which inﬂuence these processes.
This offers a new way to begin thinking about how to creatively shape
a more helpful environment for both policy and evidence. To return to
Nick Carr’s quote at the start of this article, we all know that New York
is not a city of alleys—that there is an evidence base beyond the hierarchy
of evidence, and that use is not always instrumental—but somehow this
realisation does not translate into more complete depictions. We can ask
why not. We can also ask what to do about it.
While useful for illuminating social dynamics which reinforce power
imbalances, this may not be a perfect lens for exploring evidence-based
policymaking. We do need to ask where responsibility and power lie;
how consensus about policy preferences is generated, and what are the
various roles of researchers and policymakers, and the importance of
agency within this system.
Finally, merely describing a problem is not a means to deal with it.
Yet by arming researchers and policymakers with critical perspectives on
how evidence and policy shape one another, we can start to have more
informed conversations about what a healthy system might look like. At
the very least, we need to start asking serious questions about the roles of
researchers and policymakers in sustaining the current system. Is it our job
as researchers to monitor and assess how well policymakers used evidence?
Is transparency enough? And how do the broader societal and cultural
aspects of the knowledge production system inﬂuence our practice as
researchers?
References
Bammer, G. (2005). Integration and implementation sciences: Building a new
specialization. Ecology and Society, 10 (2), 6.
Bastow, S., Dunleavy, P ., & Tinkler, J. (2015). The impact of the social sciences:
How academics and their research make a difference . How Academics and
Their Research Make a Difference. Sage. https://doi.org/10.4135/978147
3921511
<<<PAGE=19>>>
5 HOW POLICY APPETITES SHAPE … 95
Baum, F., & Fisher, M. (2014). Why behavioural health promotion endures
despite its failure to reduce health inequities. Sociology of Health and Illness,
36(2), 213–225. https://doi.org/10.1111/1467-9566.12112
Bishop, D. (2019). Rein in the four horsemen of irreproducibility. Nature.
https://doi.org/10.1038/d41586-019-01307-2
Boaz, A. et al. (2019) What works now? Evidence-informed policy and prac-
tice revisited . Policy Press. Available at: https://policy.bristoluniversitypress.
co.uk/what-works-now. Accessed 17 July 2018.
Brown, M. B. (2015). ‘Politicizing science: Conceptions of politics in science
and technology studies. Social Studies of Science . SAGE PublicationsSage UK:
London, England, 45 (1), 3–30. https://doi.org/10.1177/030631271455
6694
Butler, P . (2016). More than £1bn for troubled families “has had little impact”.
The Guardian . Available at: https://www.theguardian.com/society/2016/
oct/17/governments-448m-troubled-families-scheme-has-had-little-impact-
thinktank. Accessed 4 June 2019.
Cairney, P . (2021). The UK government’s COVID-19 policy: What does “guided
by the science” mean in practice?, Frontiers in Political Science . Frontiers
Media SA, 3 . https://doi.org/10.3389/FPOS.2021.624068/FULL
Cairney, P ., & Oliver, K. (2017). Evidence-based policymaking is not like
evidence-based medicine, so how far should you go to bridge the
divide between evidence and policy? Health Research Policy and Systems,
15(1).https://doi.org/10.1186/s12961-017-0192-x.
Cairney, P ., Oliver, K., & Wellstead, A. (2016). To bridge the divide between
evidence and policy: Reduce ambiguity as much as uncertainty. Public Admin-
istration Review, 76 (3), 399–402. https://doi.org/10.1111/puar.12555
Cassola, A., Fafard, P ., Palkovits, M., & Hoffman, S J. (2022). Mechanisms
to bridge the gap between science and politics in evidence-Informed policy-
making: Mapping the landscape. In P . Fafard, A. Cassola, & E. De Leeuw
(Eds.), Integrating science and politics for public health . Palgrave Springer.
Chalmers, I. (2006). Meeting the research information needs of patients and
clinicians more effectively. In Equator Network, 1st Annual Lecture .
Clarke, K. (2006). Childhood, parenting and early intervention: A critical exam-
ination of the Sure Start national programme. Critical Social Policy . 26(4),
699–721. Sage. https://doi.org/10.1177/0261018306068470
Cochrane, A. L. (1972). Effectiveness and efﬁciency: Random reﬂections on
health services. BMJ . https://doi.org/10.1136/bmj.328.7438.529
Deaton, A., & Cartwright, N. (2018). Understanding and misunderstanding
randomised controlled trials. Social Science & Medicine . Pergamon, 210 , 2–21.
https://doi.org/10.1016/J.SOCSCIMED.2017.12.005
<<<PAGE=20>>>
96 K. OLIVER
Deeming, C. (2013). Trials and tribulations: The “use” (and “misuse”) of
evidence in public policy. Social Policy & Administration. Wiley-Blackwell,
47 (4), 359. https://doi.org/10.1111/SPOL.12024
Degeling, C., et al. (2017). Inﬂuencing health policy through public delibera-
tion: Lessons learned from two decades of Citizens’/community juries. Social
Science and Medicine, 179 , 166–171. https://doi.org/10.1016/j.socscimed.
2017.03.003
Donnelly, C. A., et al. (2018). Four principles to make evidence synthesis more
useful for policy. Nature. Nature Publishing Group, 558 (7710), 361–364.
https://doi.org/10.1038/d41586-018-05414-4
Douglas, H. (2009). Science, policy, and the value-free ideal . University of
Pittsburgh Press.
Duncan, S., & Oliver, S. (2017). Editorial. Research for All, 1 (1), 1–5. https://
doi.org/10.18546/RFA.01.1.01
Fafard, P . (2015). Beyond the usual suspects: Using political science to enhance
public health policy making. Journal of Epidemiology and Community Health,
1129,1 – 4 . https://doi.org/10.1136/jech-2014-204608
Fafard, P ., & Hoffman, S. J. (2020). Rethinking knowledge translation for public
health policy. Evidence and Policy, 16 (1), 165–175. Policy Press. https://doi.
org/10.1332/174426418X15212871808802
Ferlie, E. et al. (2012). Knowledge mobilisation in healthcare: A critical review of
health sector and generic management literature. Social Science & Medicine,
74(8), 1297–1304. The Boulevard Langford Lane Kidlington, Oxford OX5
1GB UK: Pergamon/Elsevier Science Ltd. https://doi.org/10.1016/j.socsci
med.2011.11.042.
Finckenauer, J. O., & Finckenauer, J. O. (1999) Scared straight!: The panacea
phenomenon revisited . Waveland Press. Available at: https://www.ncjrs.gov/
App/Publications/abstract.aspx?ID=178617. Accessed 31 January 2018.
Fricker, M. (2007). Epistemic injustice: Power and the ethics of knowing .
Oxford University Press. https://doi.org/10.1093/acprof:oso/978019823
7907.001.0001
Glass, N. (1999). Sure Start: The development of an early intervention
programme for young children in the United Kingdom. Children & Society,
13(4), 257–264. Blackwell. https://doi.org/10.1002/CHI569
Greenhalgh, T. (2020). Will COVID-19 be evidence-based medicine’s nemesis?
PLOS Medicine. Public Library of Science, 17 (6). https://doi.org/10.1371/
JOURNAL.PMED.1003266
Hammersley, M. (2005). Is the evidence-based practice movement doing more
good than harm? Reﬂections on Iain Chalmers’ Case for Research-Based Policy
Making and Practice’, Evidence & Policy: A Journal of Research, Debate and
Practice. https://doi.org/10.1332/1744264052703203
<<<PAGE=21>>>
5 HOW POLICY APPETITES SHAPE … 97
Hartley, S., Pearce, W., & Taylor, A. (2017). Against the tide of depoliticisa-
tion: The politics of research governance. Policy & Politics, 45 (3), 361–377.
https://doi.org/10.1332/030557316X14681503832036
Hawkins, B., & Ettelt, S. (2018). The strategic uses of evidence in UK e-
cigarettes policy debates. Evidence & Policy: A Journal of Research, Debate
and Practice . https://doi.org/10.1332/174426418X15212872451438
Hawkins, B., & Oliver, K. (2022). Select committee governance and the produc-
tion of evidence: The case of UK E-cigarettes policy. In P . Fafard, A.
Cassola, & E. De Leeuw (Eds.), Integrating science and politics for public
health. Palgrave Springer.
Haynes, L., et al. (2012). Test. Developing public policy with randomised controlled
trials, SSRN. https://doi.org/10.2139/ssrn.2131581
Head, B. W. (2008). Three lenses of evidence-based policy. Australian Journal of
Public Administration, 67 (1), 1–11. https://doi.org/10.1111/j.1467-8500.
2007.00564.x
Holliman, R. (n.d.). Fairness in knowing: How should we engage with the
sciences? Engaging Research . Available at: http://www.open.ac.uk/blogs/
per/?p=8197 (Accessed: 17 May 2019).
Hopkins, A. et al. (2021). Are research-policy engagement activities informed by
policy theory and evidence? 7 challenges to the UK impact agenda. Policy,
Design and Practice .
Ioannidis, J. P . A. (2005). Why most published research ﬁndings are false. PLoS
Medicine, 2 (8). https://doi.org/10.1371/journal.pmed.0020124
Jasanoff, S., & Polsby, N. W. (1991). The ﬁfth branch: Science advisers as poli-
cymakers. Contemporary Sociology, 20 (5), 727. https://doi.org/10.2307/207
2218.
Jones, R., & Wilsdon, J. (2018) The biomedical bubble . Available at: www.nesta.
org.uk. Accessed 17 May 2019.
Kickbusch, I., Allen, L., & Franz, C. (2016). The commercial determinants
of health. The Lancet Global Health . https://doi.org/10.1016/S2214-109
X(16)30217-0
Knai, C., et al. (2018). Systems thinking as a framework for analyzing commer-
cial determinants of health. Milbank Quarterly, 96 (3), 472–498. https://doi.
org/10.1111/1468-0009.12339
Kothari, A., & Smith, M. J. (2022). Public health policymaking, politics, and
evidence. In P . Fafard, A. Cassola, & E. De Leeuw (Eds.), Integrating science
and politics for public health . Palgrave Springer.
McKee, M., & Stuckler, D. (2018). Revisiting the corporate and commercial
determinants of health. American Journal of Public Health . https://doi.org/
10.2105/AJPH.2018.304510
<<<PAGE=22>>>
98 K. OLIVER
Melhuish, E., Belsky, J., & Barnes, J. (2010). Evaluation and value of sure start.
Archives of disease in childhood, 95 (3), 159–161. BMJ. https://doi.org/10.
1136/adc.2009.161018.
Melhuish, E., Belsky, J., & Leyland, A. (2008). The impact of sure start local
programmes on three-year-olds and their families . Available at: http://eprints.
bbk.ac.uk/7579/. Accessed 31 January 2018.
Movsisyan, A., et al. (2018). Rating the quality of a body of evidence on
the effectiveness of health and social interventions: A systematic review and
mapping of evidence domains. Research Synthesis Methods . https://doi.org/
10.1002/jrsm.1290
Nosek, B. (2017). Opening science. In Open: The philosophy and practices that
are revolutionizing education and science . https://doi.org/10.5334/bbc.g.
Oakley, A. (1990). Who’s afraid of the randomised controlled trial? Women &
Health. https://doi.org/10.1300/j013v15n04_02
Oliver, K. A., & de Vocht, F. (2017). Deﬁning ‘evidence’ in public health:
A survey of policymakers’ uses and preferences. European Journal of Public
Health, 27 (suppl_2), 112–117.
Oliver, K. A. et al. (2022.). What works in academic-policy engagement? Evidence
and Policy . https://doi.org/10.1332/174426421X16420918447616
Oliver, K., Kothari, A., & Mays, N. (2019). The dark side of coproduction: Do
the costs outweigh the beneﬁts for health research? Health Research Policy
and Systems, 17 (1). https://doi.org/10.1186/s12961-019-0432-3.
Oliver, K., & Pearce, W. (2017). Three lessons from evidence-based medicine and
policy: Increase transparency, balance inputs and understand power. Palgrave
Communications, 3 (1), 43. https://doi.org/10.1057/s41599-017-0045-9
Oreskes, N., & Conway, E. M. (2010). Merchants of doubt: How a handful of
scientists obscured the truth on issues from tobacco smoke to global warming
(p. 355). Bloomsbury Press.
Oxman, A. D. et al. (2009). SUPPORT tools for evidence-informed health poli-
cymaking (STP) 16: Using research evidence in balancing the pros and cons of
policies. Health Research Policy and Systems, 7 (1). CAMPUS, 4 CRINAN ST,
LONDON N1 9XW, ENGLAND: BMC. https://doi.org/10.1186/1478-
4505-7-S1-S16.
Parkhurst, J. (2017). The politics of evidence: From evidence-based policy to the good
governance of evidence . Routledge Studies in Governance and Public Policy.
https://doi.org/doi:10.4324/9781315675008
Parkhurst, J. O., & Abeysinghe, S. (2016). What constitutes “good” evidence for
public health and social policy-making? From hierarchies to appropriateness.
Social Epistemology, 30 (5–6), 665–679. https://doi.org/10.1080/02691728.
2016.1172365
Pearce, W., & Raman, S. (2014). The new randomised controlled trials
(RCT) movement in public policy: Challenges of epistemic governance.
<<<PAGE=23>>>
5 HOW POLICY APPETITES SHAPE … 99
Policy Sciences,47 (4), 387–402. Springe. https://doi.org/10.1007/s11077-
014-9208-3
Petrosino, A. et al. (2013). “Scared straight” and other juvenile awareness
programs for preventing juvenile delinquency. Cochrane Database of Systematic
Reviews (3). https://doi.org/10.1002/14651858.CD002796.pub2.
Petrosino, A., Turpin-Petrosino, C., & Buehler, J. (2003). Scared straight and
other Juvenile awareness programs for preventing Juvenile delinquency: A
systematic review of the randomised experimental evidence. The ANNALS
of the American Academy of Political and Social Science, 589 (1), 41–62. Sage.
https://doi.org/10.1177/0002716203254693
Petticrew, M., & Roberts, H. (2003). Evidence, hierarchies, and typologies:
Horses for courses. Journal of Epidemiology and Community Health . https://
doi.org/10.1136/jech.57.7.527
Powell, A., Davies, H. T. O., & Nutley, S. M. (2018). Facing the challenges
of research-informed knowledge mobilisation: ’Practising what we preach?’,
public Administration, 96 (1), 36–52. WIley. 111 RIVER ST, HOBOKEN
07030–5774. https://doi.org/10.1111/padm.12365.
Powell, K. et al. (2017). Theorising lifestyle drift in health promotion: Explaining
community and voluntary sector engagement practices in disadvantaged areas.
Taylor & Francis. Routledge, 27 (5), 554–565. https://doi.org/10.1080/095
81596.2017.1356909
Rutter, H., & Glonti, K. (2016). Towards a new model of evidence for public
health. The Lancet, 388 ,S 7 . https://doi.org/10.1016/S0140-6736(16)322
43-7
Sanders, M. (2019). We owe a debt to Kevan Collins . KCL News Centre.
Available at: https://www.kcl.ac.uk/news/we-owe-a-debt-to-kevan-collins.
Accessed 17 May 2019.
Sarewitz, D. (2018). Of cold mice and isotopes or should we do less science?
In Science and politics: Exploring relations between academic research, higher
education, and science policy summer school in higher education research and
science studies . Bonn. Available at: https://sﬁs.asu.edu/sites/default/ﬁles/sho
uld_we_do_less_science-revised_distrib.pdf.
Shenderovich, Y., Sutherland, A., & Grant, S. (2019) Assessing conﬁdence in
“what works” in social policy . RAND blog.
Shepherd, J. et al. (2018). Peer review of health research funding proposals:
A systematic map and systematic review of innovations for effectiveness and
efﬁciency. PloS One, 13 (5), e0196914 (Ed., G. E. Derrick). Public Library of
Science. https://doi.org/10.1371/journal.pone.0196914.
Souza Leão, D. L. & Eyal, G. (2019). The rise of randomised controlled
trials (RCTs) in international development in historical perspective. Theory
and Society (pp. 1–36). Springer. https://doi.org/10.1007/s11186-019-093
52-6.
<<<PAGE=24>>>
100 K. OLIVER
Stewart, R. (2017). Terminology and tensions within evidence-informed
decision-making in South Africa over a 15-year period. Research for All .
https://doi.org/10.18546/RFA.01.2.03
Thaler, R., & Sunstein, C. (2008). Nudge: Improving desicions abouth health,
wealth and happiness , Nudge: Improving decisions about health, wealth, and
happiness. https://doi.org/10.1007/s10602-008-9056-2.
Tilley, L. (2017). Resisting piratic method by doing research otherwise. Soci-
ology, 51 (1), 27–42. https://doi.org/10.1177/0038038516656992.S a g e .
https://doi.org/10.1177/0038038516656992.
Watts, C. (2019). Using RCTS to evaluate social interventions: Have we got it
right? | LSHTM . CEDIL and Centre for Evaluation Lecture Series. Avail-
able at: https://www.lshtm.ac.uk/newsevents/events/using-rcts-evaluate-soc
ial-interventions-have-we-got-it-right. Accessed 20 May 2019.
Webel, A. R. et al. (2010). A systematic review of the effectiveness of peer-based
interventions on health-related behaviors in adults. American journal of public
health, 100 (2), 247–253. American Public Health Association. https://doi.
org/10.2105/AJPH.2008.149419.
White, H. (2019). The twenty-ﬁrst century experimenting society: The four waves of
the evidence revolution, 5 (1). https://doi.org/10.1057/s41599-019-0253-6.
Wood, M. (2019). Hyper-active governance: How governments manage the politics
of expertise . How governments manage the politics of expertise. Cambridge
University Press. https://doi.org/10.1017/9781108592437
Wynne, B. (2013). Social identities and public uptake of science: Chernobyl,
Sellaﬁeld, and environmental radioactivity sciences. Radioactivity in the envi-
ronment, 19 , 283–309. https://doi.org/10.1016/B978-0-08-045015-5.000
16-2
Zardo, P ., Collie, A., & Livingstone, C. (2014). External factors affecting
decision-making and use of evidence in an Australian public health policy envi-
ronment. Social Science & Medicine, 108 (SI), 120–127. Elsevier Science Ltd.,
The Boulevard Langford Lane Kidlington Oxford OX5 1GB UK. https://
doi.org/10.1016/j.socscimed.2014.02.046.
<<<PAGE=25>>>
5 HOW POLICY APPETITES SHAPE … 101
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License ( http://creativecommons.org/licenses/
by/4.0/), which permits use, sharing, adaptation, distribution and reproduction
in any medium or format, as long as you give appropriate credit to the original
author(s) and the source, provide a link to the Creative Commons license and
indicate if changes were made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line
to the material. If material is not included in the chapter’s Creative Commons
license and your intended use is not permitted by statutory regulation or exceeds
the permitted use, you will need to obtain permission directly from the copyright
holder.