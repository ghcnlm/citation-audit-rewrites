<<<PAGE=1>>>
HOW CAN CAPACITY DEVELOPMENT 
PROMOTE EVIDENCE-INFORMED POLICY 
MAKING? 
 
Literature Review for the Building Capacity to Use 
Research Evidence (BCURE) Programme 
 
Melanie Punton 
 
Research assistant: Katharine Hagerman 
 
Other contributors: Cheryl Brown, Leandro Echt, Catherine Fisher, Rob Lloyd, 
Sarah Morton and Isabel Vogel 
 
 
April 2016
<<<PAGE=2>>>
www.itad.com/knowledge-and-resources/bcure  2 
 
Contents 
 
Contents ................................ ................................ ................................ ................................ ................................  2 
Acronyms ................................ ................................ ................................ ................................ ..............................  3 
Executive summary ................................ ................................ ................................ ................................ ............... 4 
Background ................................ ................................ ................................ ................................ ...........................  7 
Methodology ................................ ................................ ................................ ................................ .......................  10 
1. What is ‘building capacity for evidence-informed policy making’? ................................ ..........................  17 
1.1. What is ‘research evidence’, and what makes it ‘good quality’? 20 
1.2. What is ‘policy’, and how can evidence benefit policy making? 22 
1.2.1. Theories of policy processes and the role of evidence in decision making ................................ . 22 
1.2.2. Theories relating to researcher-policy relationships and networks ................................ ............ 25 
1.2.3. Theories relating to cognitive processes ................................ ................................ ..................... 27 
1.2.4. How does evidence use contribute to policy quality?................................ ................................ . 28 
1.3. What is ‘capacity’ for EIPM and how do we ‘build’ it? 30 
1.3.1. Theories of capacity development ................................ ................................ ..............................  30 
1.3.2. Theories on the characteristics of adult learners, and principles of adult learning .................... 32 
1.4. Conclusions and implications for the BCURE evaluation 35 
2. What factors promote and constrain evidence-informed policy making? ................................ ............... 37 
2.1. Individual-level factors affecting EIPM 41 
2.2. Interpersonal factors affecting EIPM 44 
2.3. Organisational factors affecting EIPM 45 
2.4. Institutional factors affecting EIPM 46 
2.5. Conclusions and implications for the BCURE evaluation 52 
3. What is the evidence on how to build capacity for evidence-informed policy making? ..........................  55 
3.1. Individual change 57 
3.2. Interpersonal change 64 
3.3. Organisational change 74 
3.4. Institutional change 79 
3.5. Policy change and policy quality 79 
3.6. Conclusions and implications for the BCURE evaluation 83 
References ................................ ................................ ................................ ................................ ..........................  85 
Annex 1. Full search strategy and search terms ................................ ................................ ................................ ... 90
<<<PAGE=3>>>
www.itad.com/knowledge-and-resources/bcure  3 
 
Acronyms 
AFREPERN African Energy Policy Research Network 
AIDS Acquired Immune Deficiency Syndrome 
BCURE Building Capacity To Use Research Evidence Programme 
BSE Bovine Spongiform Encephalopathy  
CoP Community of Practice 
CSO Civil Society Organisation 
DDM Data For Decision Making Programme 
DFID UK Department For International Development 
EIPM Evidence-Informed Policy Making 
FAO Food And Agricultural Organisation 
GRADE Grading of Recommendations Assessment, Development and Evaluation 
HIC High-Income Country 
HIV Human Immunodeficiency Virus 
ICAI Independent Commission For Aid Impact 
ICT Information And Communications Technology 
KB Knowledge Broker 
LIC Lower-Income Country 
NGO Non-Governmental Organisation 
NHS UK National Health Service  
OECD-DAC The Organisation For Economic Co-Operation And Development’s Development 
Assistance Committee 
OR Other Review 
PEPFAR President's Emergency Plan For AIDS Relief 
PHSI Partnerships For Health System Improvements Programme 
PRSP Poverty Reduction Strategy Papers  
RCT Randomised Controlled Trial 
SR Systematic Review 
UK United Kingdom 
UMIC Upper Middle Income Country 
US United States 
USAID United States Agency For International Development 
WHO World Health Organisation
<<<PAGE=4>>>
www.itad.com/knowledge-and-resources/bcure  4 
 
Executive summary 
How can capacity development promote evidence-
informed decision making? This review discusses the 
messy, complex nature of evidence use in policy 
processes; casts a spotlight on some of the individual, 
interpersonal, organisational and institutional factors 
that promote and constrain use of evidence; and 
examines the primary evidence base to investigate 
what works to build the capacity of decision makers to 
use evidence, for whom, in what circumstances, and 
why. 
Why this review? The Building Capacity to Use Research 
Evidence (BCURE) programme works with policy makers 
in low and middle-income countries, developing skills, 
knowledge and systems to improve the use of evidence 
in decision making. Funded by the UK Department for 
International Development (DFID) and launched in 2013, 
BCURE will invest £13 million over three years in a 
number of linked capacity development projects across 
Africa and Asia. This literature review was written as part 
of the evaluation of BCURE, which runs alongside the 
programme and aims to strengthen the evidence base 
on capacity development for evidence-informed policy 
making (EIPM). 
The BCURE evaluation team recognise that there is a 
large, growing and disparate evidence base of relevance 
to EIPM, spanning a wide range of disciplines. This 
review attempts to synthesise some of this evidence, in 
order to provide a practical resource summarising 
existing knowledge about EIPM and how to promote it. It 
also plays a crucial role in the evaluation design, in 
examining, testing and refining the draft Theory of 
Change for the programme, and contributing to 
answering the evaluation questions. 
Structure of the review: This paper is structured in three 
sections, each standing alone with its own conclusion: 
Section 1 discusses the theories and assumptions 
underpinning the BCURE programme and the concept of 
‘EIPM’, providing an overview of the diverse and rich 
theoretical literature on this topic. 
Section 2 outlines the most significant and well-
evidenced barriers to and enablers of evidence use by 
decision makers. It then examines some of the individual, 
interpersonal, organisational and institutional factors 
that promote or constrain evidence use in policy making, 
focusing particularly on political, psychological and 
cultural factors that are less frequently discussed in 
existing secondary reviews. 
Finally, Section 3 examines primary evidence from 
studies of interventions aiming to build capacity for 
EIPM. It adopts a realist synthesis approach to examine 
what works, for whom, in what circumstances, and why. 
Section 1: What is ‘building capacity for evidence-
informed policy making’? The concept of EIPM is 
underpinned by a growing and highly interdisciplinary 
literature, comprising a large number of overlapping 
theories and models. These range from evidence 
hierarchies to learning theories; models of policy 
processes to concepts of complexity. We believe it is 
important for policy makers and practitioners to get to 
grips with the theory underpinning EIPM, especially 
given criticism that EIPM research and interventions fail 
to account of the messy reality of policy processes. 
Building theoretical insights into interventions can help 
practitioners avoid common traps and design 
programmes that are more likely to lead to change. 
Section 1 asks three questions: 
What is ‘research evidence’, and what makes it ‘good 
quality’? There is widespread agreement that research 
evidence is just one type of evidence required for policy. 
Although there is some agreement over what counts as 
‘good quality’ evidence, ‘evidence hierarchies’ should be 
used with caution, as appropriateness of evidence may 
be more important than its position on a hierarchy. 
Several writers argue that research evidence is not 
neutral – first because it reflects pre-existing views and 
beliefs of researchers and commissioners, and secondly 
because it rarely points to an obviously optimal solution, 
implying that contestation over its interpretation is 
inevitable. 
What is ‘policy’, and how can evidence benefit policy 
making? The rational ‘policy cycle’ model is now largely 
discounted in theoretical work on EIPM, with more 
recent models and empirical evidence emphasising the 
non-linear nature of policy change, the importance of 
interactions between various networks of actors, and the 
role of power and politics in shaping evidence use. 
Insights from psychological literature also emphasise the 
importance of mental models, contextual cues and social 
norms, which affect how people understand and 
interpret evidence.
<<<PAGE=5>>>
www.itad.com/knowledge-and-resources/bcure  5 
 
However, definitions of policy quality considered in this 
review – such as those used by the UK Civil Service – 
retain close links to linear and rational models; or at 
least retain assumptions about the objectivity of 
knowledge and evidence interpretation which several 
theories discussed in this section challenge. The BCURE 
evaluation will need to navigate this tension, given its 
remit to measure improvements in policy quality as a 
result of BCURE interventions. 
What is ‘capacity’ for EIPM and how do we ‘build’ it? 
Recent thinking on capacity development suggests that 
capacity is complex and multi-dimensional. Capacity 
development therefore demands involves more than a 
focus on individual skills, requiring intervention at 
individual, interpersonal, organisational and institutional 
levels. Theories and models of adult learning provide 
insights into how individual learning takes place, which is 
important given the strong emphasis on training within 
the BCURE programme. Different theories of learning 
provide a diverse set of theoretical frames for 
understanding the mechanisms that link training to 
individual behaviour change. 
Section 2: What factors promote and constrain evidence-
informed policy making? There is a large amount of 
evidence on the barriers to and facilitators of EIPM, 
synthesised in a number of secondary reviews. However, 
this evidence has been criticised for focusing on single 
elements of the policy-making process and relying on the 
perceptions of research producers and users; rather 
than considering how evidence is actually used within 
policy processes as a whole. This section therefore 
considers some of the primary evidence on 
psychological, political, cultural and institutional factors 
affecting EIPM (and the interrelationships between 
them) – taking into account theories of power, politics, 
networks, cognitive processes and complexity 
discussed in Section 1. This evidence finds that: 
 Individual beliefs, attitudes and motivations to use 
evidence are connected to pre-existing beliefs, and to 
prevailing norms and values. Evidence may be 
ignored or side-lined if it counters past experience, 
beliefs about what counts as ‘good’ evidence may 
result in useful knowledge being discounted, and 
certain evidence findings may be viewed as 
‘unacceptable’ in particular contexts and so ignored. 
 Organisational factors can affect individual 
motivation or ability to use evidence. Individual 
motivation for EIPM may be increased if evidence is 
promoted or valued within an organisation, lack of 
time to access and appraise research partly reflects 
an organisation’s ‘culture’ of evidence use, and 
hierarchical management of information, 
organisational silos and poor organisational memory 
can limit access to research. 
 Non-government actors can influence the extent and 
nature of EIPM. International donors can both 
promote and constrain the effective use of evidence 
in decision making depending on their priorities, 
private sector actors can exert pressure which 
‘blocks’ evidence-informed decisions, and the media 
(and the general public) may act as a barrier to 
EIPM. Civil society can put pressure on government 
actors to use evidence, build momentum behind 
ideas, and bring together different forms of 
knowledge. 
 Institutional factors such as sudden change (e.g. 
crises or regime changes), levels of decentralisation 
and levels of democracy can all generate 
opportunities for or barriers to EIPM. 
Section 3: What is the evidence on how to build capacity 
for evidence-informed policy making?  A realist synthesis 
approach was used to examine primary evidence on 
interventions aiming to build capacity for EIPM. This 
involved examining the mechanisms through which 
interventions lead to particular outcomes, along with the 
features of interventions and the wider context that 
either enable or hinder these mechanisms. The 
interventions examined in this review largely aimed to 
develop capacity for evidence use or public sector 
decision making in health, and around half were based in 
lower and middle-income contexts. 
Overall, the intervention evidence on capacity 
development for EIPM is limited. Most evidence relates 
to training courses, with more limited evidence on the 
impact of networks, knowledge brokers, champions, 
organisational systems and tools. Most studies did not 
contain explicit information on mechanisms of change – 
identifying these involved reading between the lines, 
looking for common themes and making links to the 
theoretical literature. Few intervention studies refer to 
the more recent theories of EIPM discussed in Section 1, 
which emphasise the messy, contested and political 
nature of evidence use in policy making.
<<<PAGE=6>>>
www.itad.com/knowledge-and-resources/bcure  6 
 
Despite the small evidence base, useful lessons can be 
distilled from these studies on how and why different 
interventions may have resulted in (or not resulted in) 
change, and the contextual and intervention factors that 
helped or hinder success. Some of the most significant 
findings are as follows: 
 
 Studies examining individual-level interventions, 
particularly training, suggest that combining 
classroom learning with on-site projects and actively 
engaging participants’ organisations may be linked 
to training success; especially as supportive 
organisations appeared to be an important 
contextual factor influencing training impact. One 
helpful way of understanding the mechanism 
through which training can improve capacity is the 
theory of self-efficacy – training increases 
participants’ confidence in their capability to 
perform a certain task or handle a particular 
situation. 
 Studies relating to interpersonal-level interventions 
discussed the role of networks, knowledge brokers 
and champions in promoting EIPM. Individuals can 
lead to change through the mechanisms of 
‘cheerleading’, acting as ‘transformational leaders’ or 
‘network facilitators,’ or promoting ‘social learning’ 
through role-modelling EIPM behaviours. Effective 
champions and knowledge brokers appear to 
possess specific interpersonal skills, vision and 
commitment, and an appropriate level of seniority in 
an organisation. The evidence on networks suggests 
they may lead to change through the mechanism of 
‘social processing’ – in which beliefs within a group 
shift towards a consensus – and this may lead away 
from EIPM as well as towards it. 
 Organisational tools and systems appear to work by 
facilitating behaviour change (making a person’s job 
easier), or reinforcing it through for example 
rewards, audit or feedback. EIPM tools may also lead 
to change by increasing the value staff place on 
evidence, through convincing them of the benefits 
that data can bring to decision making. A virtuous 
circle may emerge, in which increased use of 
evidence leads to greater demand for it, and so on. 
 
What are the lessons for the BCURE evaluation, and for 
others interested in promoting evidence-informed policy? 
 Policy and practice can be strengthened by 
consideration of theories: around power and policy 
processes, networks, complex systems, cognitive 
features, capacity development, and individual 
learning. These may help policy makers and 
practitioners avoid unintentionally basing their ideas 
about evidence use on linear and rational models, 
which are now widely discounted. 
 It is important to consider the political, psychological, 
cultural and institutional factors promoting and 
constraining evidence use, drawing on the theories 
mentioned above. This implies the need for the 
BCURE evaluation team to collect data on individual 
beliefs, attitudes and motivations, and how they link 
to organisational factors and to social norms and 
values. The evaluation should also consider the 
influence of the wider institutional environment on 
evidence use in BCURE contexts – for example the 
role of international donors, private sector actors, 
the media and civil society, and the influence of 
historical events on the ways in which evidence is 
used and understood. 
 The evidence base on capacity development for EIPM 
is small, largely derived from the health field, and 
weighted towards studies examining the impact of 
training on individual capacity. There are significant 
evidence gaps around the role of interpersonal and 
organisational interventions in promoting change, 
and regarding the influence of EIPM capacity 
development on policy change and policy quality. 
However, useful evidence suggesting how and why 
EIPM capacity development can lead to change have 
emerged from this study. The BCURE evaluation will 
explicitly draw on the theory and evidence discussed 
in this review to test and further illuminate these 
mechanisms, and the features of context and 
intervention design that lead to (or inhibit) change.
<<<PAGE=7>>>
www.itad.com/knowledge-and-resources/bcure  7 
 
Background 
The Building Capacity to Use Research Evidence (BCURE) programme works with policy makers in low and 
middle-income countries, aiming to develop skills, knowledge and systems in order to improve the use of 
evidence in decision making. Funded by the UK Department for International Development (DFID) and 
launched in 2013, the BCURE programme will invest £13 million over three years in a number of linked 
capacity development projects across Africa and Asia.1 
Itad (in association with The School of Public Leadership of Stellenbosch University and CommsConsult) are 
conducting an independent impact evaluation of BCURE, which is running alongside the programme. The aim 
of the evaluation is to strengthen the evidence base to support evidence-informed policy making (EIPM) in 
developing countries – by drawing on both primary data from BCURE programmes (collected between 2015 
and 2017) and the existing evidence base (as summarised in this review). 
There is a large and disparate literature of relevance to EIPM, spanning a wide range of disciplines. The review 
attempts to synthesise some of this evidence in order to provide a practical resource summarising existing 
knowledge about EIPM and how to promote it. It also plays a crucial role in the evaluation design; through 
examining, testing and refining the draft Theory of Change for the programme, and contributing to answering 
the evaluation questions. 
This review is structured in three parts. Each part stands alone, so readers can navigate directly to the section 
of most interest. Section 1 discusses the theories and assumptions underpinning the BCURE programme and 
the concept of ‘EIPM’, providing an overview of the diverse and rich theoretical literature on this topic. 
Section 2 outlines the most significant and well-evidenced barriers to and enablers of evidence use by 
decision makers, and examines some of the individual, interpersonal, organisational and institutional factors 
that promote or constrain evidence use in policy making. Finally, Section 3 examines primary evidence from 
studies of interventions aiming to build capacity for EIPM. It adopts a realist synthesis approach to examine 
what works, for whom, in what circumstances, and why. 
The BCURE Theory of Change 
This review was driven by and structured around the BCURE evaluation Theory of Change. This represents the 
evaluation team’s theory about how the different BCURE projects will result in change across their varied 
contexts. 
The Theory of Change depicts the activities (interventions and outputs) of BCURE providers. These involve 
individual-level interventions (such as training); interpersonal-level interventions (such the use of ‘evidence 
champions’ in organisations, and the development of policy and evidence networks); and organisational 
interventions (including the development of policies, systems and procedures for evidence use). These 
activities predominantly target high-level government policy makers (such as ministerial staff) and mid-level 
government policy makers (such as mid-level civil servants). 
These interventions are anticipated to lead to change at individual, interpersonal, organisational and 
institutional levels. Change at each of these four levels is expected to influence changes in others, in non-
linear ways. 
 
1 Further information on the BCURE programme and the evaluation is available here: http://www.itad.com/knowledge-and-
resources/bcure.
<<<PAGE=8>>>
www.itad.com/knowledge-and-resources/bcure  8 
 
At individual level, BCURE activities will improve the skills and knowledge of targeted stakeholders, increasing 
their capacity for EIPM. Activities will also result in increased positive intention among and commitment of 
individuals to use evidence, and in individuals placing greater value on evidence in their work. At interpersonal 
level, organisational ‘champions’ will endorse EIPM and help move the agenda forward in their institutions, 
and networks will be developed and strengthened between national and international institutions – providing 
an environment for learning and engagement. 
Individual and interpersonal-level changes, together with direct interventions targeting organisational 
processes, are expected to contribute to organisational-level change. This includes the development of 
systems and procedures, policies and guidelines, and professional development opportunities; which together 
will support and incentivise EIPM. Individual, interpersonal and organisational-level change will also contribute 
to change at institutional level, including increased interest in EIPM within civil society, the media and the 
public; facilitating these actors to more effectively engage with EIPM. 
Finally, the combination of individual, organisational, network and institutional change will increase demand 
for and use of evidence among targeted stakeholders, which will result in policy and practice being increasingly 
informed by evidence. This in turn will lead to improved quality of policies and programmes. These long-term 
changes will lead to the programme impact: poverty reduction and improved quality of life. 
The Theory of Change can be summarised in two sentences as follows: 
Developing the capacity of decision makers to use research evidence (through building knowledge, skills, 
commitment, relationships and systems at individual, interpersonal, organisational and institutional levels) will 
allow them to access, appraise and apply good quality evidence more effectively when forming policy. This will 
improve the quality of policies, ultimately benefitting more poor people.
<<<PAGE=9>>>
www.itad.com/knowledge-and-resources/bcure  9 
 
 The BCURE Theory of Change2 
 
 
2 Notes:  
- The Theory of Change is intended to be non-linear, but the limits of the schematic mean that it is represented as a progression from left to right.  
- Although we have presented the programme interventions at the left-hand side of the diagram for ease of reading, BCURE partners are planning interventions at different entry points across the 
Theory of Change. 
6. Policy and practice is  informed by 
research evidence
Individual interventions:
Training
Mentoring
Secondments
Development of 
evidence leaders / 
'champions'
Organisational 
interventions:
Facilitating and developing 
institutional processes, 
procedures, and systems
High-level government 
policy makers:
Ministerial staff
Cabinet Secretaries
Parliamentarians
Senior civil servants
Mid-level government  
policy makers:
Technical and research 
staff in government 
Departments
Mid-level civil servants
Civil society, the media, 
researchers, and the 
public
Network interventions:
Policy development pilots 
and demonstration cases
Policy networks and 
relationships 
strengthening
Policy dialogue with civil 
society / media  
1.1 Improved skills, knowledge and confidence 
of individuals around accessing,appraising and 
using evidence  in policy process
1.2. Improved motivation and commitment of 
individuals  to use evidence:  eg Ministerial staff 
seek out  expert advice
2.1. Targeted leaders champion and endorse 
EIPM
3.1. Organisational systems and procedures are 
established that support and incentivise EIPM 
eg. Budgetary and approval incentives around 
EIPM for policy approval processes
2.2 Strengthened interaction between national 
and international individuals and institutions 
around the production and use of evidence 
3.2. Policies and guidelines on EIPM are 
established and being used e.g. standards, 
quality assurance of policy proposals 
3.3. EIPM is integrated into civil service 
competency frameworks, professional 
development and training
4.2. Civil society and the mediaregularly and 
effectively engage in / report EIPM
1.3. Individuals value the use of evidence to 
deliver mandates and political goals 
4.1. Increased interest in and debates onthe 
use of evidence in policy making by civil  society 
, the media  and the public
5. Increase in the demand for and 
use of evidence 
Individual level change
Interpersonal level change 
Organisationallevel change
Changes in the institutional context 
Impact
Poverty reduction
and improved quality 
of life
7. The quality of policies and 
programmes will  improve
<<<PAGE=10>>>
www.itad.com/knowledge-and-resources/bcure 10 
 
Methodology 
1. The aim of this paper: This study aimed to identify evidence to test and further articulate the theory of how 
the BCURE programme is expected to lead to change. The review is therefore structured around the draft 
BCURE Theory of Change described above – drawing on evidence to support, challenge and further articulate 
the hypothesised causal links, planned outcomes and assumptions underpinning the BCURE programme. In 
building the capacity of decision makers to access, appraise and apply research evidence, BCURE focuses on 
the ‘demand side’ of EIPM (Newman et al. 2012). As a result, this review focuses on evidence relating to 
‘demand side’ strategies for promoting evidence use in policy making, and deliberately excludes the growing 
evidence base on ‘supply side’ interventions (for example, on how to effectively disseminate research and 
encourage research uptake). 
2. Approach and research questions: This review adopts a ‘realist synthesis’ approach. Rather than asking ‘what 
works?’, a realist synthesis aims to identify what exactly it is about interventions that results in change, in 
which contexts, for whom, and why (Pawson & Tilley 1997). This approach complements an overarching 
realist design for the BCURE evaluation – which aims to develop and test theory about how and why different 
BCURE projects lead to (or do not lead to) change. 
Unlike a systematic review, a realist review does not aim to identify all potentially relevant papers, 
systematically extract information from them and then aggregate it. It also does not attempt to locate all 
relevant evidence – recognising that there is no finite set of ‘relevant papers’, particularly on a topic as broad 
as the use of evidence in decision making. Instead, a realist review aims to locate the most relevant evidence, 
in order to draw out lessons on how past interventions have worked (or not worked), in what ways, for whom, 
in what circumstances, in what respects, and why (Pawson 2006a). 
The review therefore adopted an iterative search strategy, with new searches conducted as understanding 
grew about particular theories (and as new theories were uncovered in the literature), using additional and 
revised search terms. Searches for relevant evidence continued throughout the synthesis and write-up stages 
of the report. A paper was deemed relevant if it was judged to contribute to our understanding of the BCURE 
Theory of Change: did it provide evidence to support, challenge or further articulate the outcomes, the 
hypothesised causal links, or the assumptions? 
Research questions 
Six research questions (RQs) guided the searches; aligning closely with the Theory of Change. The review has 
not been structured using these questions, as the scope and emphasis shifted throughout the search process 
(discussed further below). However, broadly speaking RQ 1 is largely addressed in Sections 1 and 2, and RQs 
2-6 in Section 3. 
1. What factors can promote and constrain EIPM in public sector environments? 
2. What factors lead to professional skills and/or knowledge being acquired by public sector workers through 
teaching and training? 
3. How and in what circumstances can capacity development interventions promote individual behaviour 
change within organisations? 
4. How and in what circumstances can capacity development interventions promote organisational, network 
and institutional change? 
5. How and in what circumstances can capacity development interventions increase the demand for and use of 
evidence in policy making? 
6. How and in what circumstances can interventions that increase demand for and use of research evidence 
lead to improved policy quality?
<<<PAGE=11>>>
www.itad.com/knowledge-and-resources/bcure 11 
 
3. Search strategy and inclusion criteria: Structured database and snowball searches were conducted using 
search strings to identify relevant literature. We focused our search on academic fields above and beyond the 
international development literature, as initial scoping suggested these may contain useful insights. Academic 
fields included: political science, health, public administration, health, psychology, and adult education and 
training (described in more detail in Annex 1). 
Inclusion criteria: 
In line with the realist synthesis approach described above, the primary consideration guiding the literature 
searches was relevance to the BCURE Theory of Change. We also adopted the below set of inclusion criteria to 
guide the literature search and selection. 
 We only considered papers available in English. 
 We generally only considered papers that were published since 2000. However, if an earlier paper 
was considered highly relevant it was included. 
 We focused our search primarily on published and unpublished papers and reports. However, we 
also considered project documentation if it was made available and deemed relevant (e.g. 
Theories of Change, project reports). 
 We prioritised primary empirical studies, given our aim of providing a practical summary of 
evidence on what works to promote EIPM. However, in practice much of the relevant literature 
was theoretical or conceptual in nature. We excluded models, theories and opinion pieces not 
based on empirical evidence. 
 We prioritised papers examining low and middle-income country contexts. However, in practice 
much of the relevant primary evidence (particularly evidence discussing intervention outcomes) 
related to high-income contexts. 
 We generally only considered papers available in electronic formats. Books that were found in the 
search were only included if a) one of the evaluation team had a copy; or b) the book was 
available on Google Books. 
 Papers behind paywalls were included if they could be accessed via open-source formats, or via 
the institutional access of a member of the evaluation team. 
Our search approach involved structured database searches and snowball searches. 
Structured database searches: 
We used search strings to retrieve literature from websites and online databases. Our approach to structured 
searching was as follows: 
 Hand searches were conducted of selected databases, by systematically reviewing all listed titles 
to locate literature relevant to the RQs. The following databases were hand searched: 
o All outputs listed related to the Research and Policy programme on the Overseas 
Development Institute website [405] = (17 relevant results). 
o All articles from 2012, 2013 and 2014 of the journal Evidence & Policy: A Journal of 
Research, Debate and Practice [89] = (15 relevant results). 
o All World Development Reports since 2000 [15] = (1 relevant result). 
o All systematic reviews listed in the 3ie Systematic Review Database [280] = (0 relevant 
results). 
o All case studies categorised as ‘research to policy’ in the Eldis database [44] = (0 relevant 
results).
<<<PAGE=12>>>
www.itad.com/knowledge-and-resources/bcure 12 
 
 Selected websites were then searched using Boolean searches and smart Google search 
functions, using the search terms listed in Annex 1. The websites searched included the databases 
listed above, and also the websites/databases of: the Institute of Development Studies, the British 
Library of Development Studies, the Overseas Development Institute, the International 
Development Research Centre, the United Nations Development Programme, and AusAID. 
 General Boolean searches were then conducted on Google and Google Scholar. 
 We intended to conduct additional searches on scholarly and international development 
databases (including Research for Development (R4D), Emerald, and Southern Open Access 
repositories such as African Journals Online). However, this was not possible due to time 
constraints. 
Snowball searches: 
Bibliography search: We searched the bibliographies of relevant literature to identify additional sources. Given 
time constraints, this searching was limited to the bibliographies of BCURE provider proposals and the most 
relevant secondary review papers. 
Expert recommendations: We reached out to the evaluation team’s wide network of contacts with expertise in 
various fields, to ask for suggestions on potentially relevant sources as well as further contact details for 
additional experts who may be able to assist. We also communicated our work on the literature review to 
wider EIPM networks, including the Research to Action (R2A) website and the Evidence Based Policy in 
Development Network (EBPDN) mailing list. This strategy resulted in 51 recommendations of sources for this 
review, which were all screened for relevance. 
4. The evolution of the search strategy: In line with the RQs, the searches originally focused on primary studies 
relevant to the evaluation Theory of Change. As anticipated, most relevant published literature was secondary 
and/or conceptual in nature. This rich conceptual literature reveals that the concept of ‘evidence-informed 
policy’ is underpinned by a large number of diverse and overlapping theories and assumptions, including those 
around the nature of evidence and policy processes, the links between knowledge, power and politics, and 
the ways in which people understand evidence in light of existing mental models. 
Although we originally intended to draw on this theoretical literature sparingly and focus predominantly on 
synthesising primary evidence (in line with the inclusion criteria above), during the search process the 
importance of the theoretical literature became apparent. First, a realist review is fundamentally about 
articulating and testing theories about how and why change happens (Pawson et al. 2004; Pawson 2006a). 
The theoretical literature provides a large number of rich and useful insights into how EIPM might (or might 
not) work, which will help the evaluation team articulate and measure the various ways in which BCURE 
projects contribute to (or fail to result in) change. Second, some recent literature on EIPM is critical of other 
evidence in this field for failing to recognise the complexity of policy change, utilise theoretical learning from 
policy studies, or make explicit underlying assumptions about policy processes (Oliver, Lorenc, et al. 2014; du 
Toit 2012; Wesselink et al. 2014) – as discussed in Section 1 below. The scope of this review was therefore 
expanded – both to avoid this criticism, and to provide a detailed summary of relevant theoretical literature 
for the benefit of other policy makers and practitioners working on promoting EIPM.
<<<PAGE=13>>>
www.itad.com/knowledge-and-resources/bcure 13 
 
5. Classifying the literature: Throughout the search process, relevant literature that met the inclusion criteria 
(retrieved from both snowball and structured searches) was classified in an Excel matrix; available alongside 
this review.3 The classification process documented: 
 Basic details: Full citation, year of publication and URL. 
 Source: whether evidence was located through database or snowball searches. This helped the 
researchers keep track of whether snowball searches were steering the evidence in a particular 
direction, in order to help counter bias. 
 Domain: the academic field the literature was drawn from; although this is a relatively crude 
classification, as many sources relevant to EIPM are interdisciplinary and span different domains. 
 Publication status: peer reviewed publication, organisational report, unpublished/internal report, 
thesis, book, or webpage. 
 Type of evidence: primary empirical study, secondary review study, or theoretical/conceptual 
paper. This classification follows the DFID How To Note on Assessing the Strength of Evidence, 
(DFID 2014a). Primary studies were also classified according to whether they were ‘intervention 
studies’ (reporting evidence relating to a specific intervention – for example a capacity 
development programme) or not. 
 Research design: experimental, quasi-experimental or observational for primary studies; 
systematic review or other review for secondary studies (also following guidance in the DFID How 
To Note, 2014a). 
 Research methods: e.g. RCT, case study, qualitative methods such as interviews and focus groups, 
large-n survey, participatory action research. 
 Geographical context of primary research: the country in which research was conducted. Research 
was also classified according to whether it was conducted in lower-income or higher-income 
contexts.4 
 Quality: All primary intervention studies that were deemed relevant to the RQs were assessed for 
quality, as detailed below. 
6. Appraising and quality assessing the literature. Given the wide scope of the RQs and relevant literature, a 
decision was made to prioritise depth and breadth of discussion over rigorous quality assessment of sources 
within the resources available for this review. This decision was guided by the realist principles underpinning 
the BCURE evaluation. Realist syntheses, unlike systematic reviews, do not aim to synthesise the conclusions 
of studies, in order to accumulate evidence about ‘what works’. Rather, they aim to draw on the relevant 
aspects of studies that can contribute to a better understanding of how and why interventions work or do not 
work – and this may include theories, snippets of primary data, or contextual information as well as overall 
conclusions (Pawson 2006b; Pawson 2005). There is therefore limited utility in systematically applying 
rigorous quality criteria to sources ‘as a whole’, in order to indicate how confident a reader can be in their 
conclusions. Instead, Pawson recommends that ‘judgement about rigour is made not on the basis of pre-
formulated checklists, but in relation to the precise usage of each fragment of evidence within the review’ 
(Pawson 2006c). 
Based on these considerations, six simple quality criteria were adopted for assessing primary intervention 
evidence. The intention was not to provide a comprehensive quality assessment; but rather to act as a rough 
 
3 All studies incorporated in the literature review were fully classified. However, not all classification criteria were recorded for studies 
not included in the review, due to time constraints. 
4 Countries were classified using World Bank 2015 data, into lower-income countries (LICs: including low-income and lower-middle 
income countries in the World Bank classification); upper-middle income countries (UMICs) and higher-income countries (HICs) (World 
Bank 2015a).
<<<PAGE=14>>>
www.itad.com/knowledge-and-resources/bcure 14 
 
proxy of lower and higher quality studies using criteria that were relatively quick and easy to apply. The six 
criteria were derived from the DFID How To Note on Assessing the Strength of Evidence (DFID 2014a): 
1. Transparency of study design. 
2. Transparency of data sources and analysis methods. 
3. Transparency about the context and location of the study. 
4. Transparency of sampling approach and sampling frame. 
5. Triangulation of sources and methods. 
6. Discussion of limitations. 
Studies were given scores for each criterion: ‘yes’, (2 points) ‘no’ (0 points) or ‘partly’ (1 point) (e.g. if a study 
was transparent about its data collection methods but not its analysis methods). Studies were deemed ‘low 
quality’ if they scored 4 points or fewer; ‘medium quality’ with 5–8 points, and ‘high’ quality with 9–12 points. 
Non-intervention primary studies were not quality assessed in this way, given time constraints. 
In addition to these basic quality scores, the lead researcher conducted ad hoc additional quality assessments 
of a small number of intervention and non-intervention studies considered for Section 3 during the synthesis 
stage. This aimed to ensure confidence in the quality of the precise fragment of the study that was relevant to 
the discussion in Section 3 (Pawson 2006c). These secondary assessments drew on a wider range of quality 
criteria outlined in the DFID How-To Note than was possible to incorporate in the quantitative quality scores. 
Findings from studies which were deemed problematic on quality grounds were excluded (for example 
conclusions were not included in the synthesis if they did not clearly follow from the study’s findings). 
In total, 344 studies were retrieved from searches and added to the literature review database, of which 270 
were deemed relevant based on a preliminary abstract scan. The sources of data are summarised in Table 1 
below. 
Table 1. Summary of studies retrieved through searches 
Source of data Total papers retrieved Full text reviewed Cited in final paper 
Database search 217 120 93 
Snowball: bibliography search 41 19 9 
Snowball: external or team recommendation 51 51 21 
Snowball: provider proposal 35 15 9 
Total 344 205 132 
 
 
 
 
 
It was not possible to review the full texts of all relevant papers due to time constraints, so additional criteria 
were developed to prioritise sources. Papers meeting the following criteria were read in full, and relevant 
papers incorporated into the review: 
139 papers not reviewed: 
 Insufficient time: 82 
 Abstract not deemed relevant: 
47 
 Book: 7 
 Could not gain access: 3 
73 papers excluded after full text 
review: 
 Not deemed relevant: 69 
 Insufficient quality: 4
<<<PAGE=15>>>
www.itad.com/knowledge-and-resources/bcure 15 
 
 All primary studies (intervention and non-intervention studies) relating to low- and middle-income 
country contexts published since 2010. 
 All primary intervention studies relating to high-income contexts published since 2010. 
 All systematic reviews, and all other secondary reviews published since 2010. 
 All studies recommended by experts through the snowball search. 
 
2010 was a relatively arbitrary cut-off point designed to narrow the number of papers in particular categories 
to a manageable level. Papers published earlier were considered if they were particularly relevant; but given 
the large amount of literature in this field we decided to prioritise more recent evidence which may not yet 
have been synthesised elsewhere. The evaluation team will revisit the literature to update this review at a 
later point in the evaluation, and at that point will aim to review some of the sources that were excluded due 
to time constraints at this stage. 
7. The nature of the evidence base. Table 2 provides a summary of the evidence discussed in this review 
Table 2. Summary of literature included in this report 
Field Primary study Secondary review Theoretical/conceptual Total 
Health 26 8 7 41 
Development studies 13 5 14 32 
Adult education and training     23 23 
Public administration 6   6 12 
Political science   1 8 9 
EIPM 3 1 4 8 
Psychology     4 4 
Sociology   1   1 
Anthropology 1     1 
Management   1   1 
Total 49 17 66 132 
 
 
 
 
 
 
 
 
8. Limitations: Due to the wide remit of this literature review and the inevitable constraints of time and 
resources, this paper does not provide a fully comprehensive overview of the evidence on capacity building 
for EIPM. A broad and diverse range of literature from a number of different fields is relevant to this topic, 
and decisions regarding which papers to include were made based on pragmatic reasons of time and 
Secondary reviews: 
 
 Systematic review: 9 
 Other review: 8 
Primary studies: 
 
 Intervention studies: 18 
 Non-intervention studies: 31
<<<PAGE=16>>>
www.itad.com/knowledge-and-resources/bcure 16 
 
resources as well as the inclusion criteria discussed above. As a result, the evidence in this review represents a 
systematic but partial synthesis of the available literature; and it is likely that relevant papers have been 
omitted. We intend to update this review at a later stage of the evaluation, presenting an opportunity to 
include additional studies that are not incorporated in this version. Readers are encouraged to forward any 
additional relevant literature they are aware of to the evaluation team.5 
Non-intervention primary studies were not formally quality assessed due to time constraints, which may 
mean that some low quality evidence has been incorporated in the review. However, these studies were 
informally assessed by the lead researcher during the synthesis stage as described above, somewhat 
mitigating this limitation.  
 
5 See contact details at http://itad.com/projects/evaluation-of-approaches-to-build-capacity-for-use-of-research-evidence-bcure/
<<<PAGE=17>>>
www.itad.com/knowledge-and-resources/bcure 17 
 
1. What is ‘building capacity for evidence-informed policy making’? 
Overview 
The literature on EIPM is growing and interdisciplinary. It contains a wide range of overlapping theories and 
models concerning the ways in which evidence is used in policy making. This theory is important to policy and 
practice, especially given that several writers criticise EIPM research and interventions for a failure to make 
use of theoretical insights or to acknowledge non-rational theories of policy processes. 
The BCURE programme is underpinned by a number of assumptions about the nature of evidence, policy 
making and capacity development. These are examined through a study of the evidence in relation to three 
questions: 
 
1. What is ‘research evidence’, and what makes it ‘good quality’? Several theoretical and empirical studies 
suggest that research evidence is just one type of evidence required in policy making, and cannot be 
easily isolated from other forms of knowledge in policy debates. There is growing agreement that 
appropriateness of evidence may be more important than its position on a generic evidence quality 
hierarchy, and several writers argue that contestation over evidence interpretation is inevitable given that 
evidence is never ‘neutral’. 
 
2. What is ‘policy’, and how can evidence benefit policy making? Many frameworks for understanding the 
role of evidence in policy link to rational and linear models of policy processes such as the ‘policy cycle.’ 
These have been widely criticised in both theoretical and empirical studies for ignoring the messy realities 
of policy making. More recent models emphasise the non-linear nature of policy change, the importance 
of interactions between various networks of actors, and the role of power and politics in shaping evidence 
use. Psychological theories also stress the cognitive limits of rationality and the importance of mental 
models in shaping how we interpret evidence. However, definitions of policy quality considered in this 
review – such as those used by the UK Civil Service – retain close links to linear and rational models; or at 
least retain assumptions about the objectivity of knowledge and evidence interpretation which several 
theories discussed in this section challenge. The BCURE evaluation will need to navigate this tension, given 
its remit to measure improvements in policy quality as a result of BCURE interventions. 
 
3. What is ‘capacity to access, appraise and apply evidence’, and how do we ‘build’ it? Recent empirically 
based definitions of capacity suggest that it is complex and multi-dimensional. Theories of complexity 
suggest the importance of considering whole systems and expecting non-linear change and feedback 
loops within EIPM capacity development interventions. Recent models of capacity suggest that building 
capacity for EIPM should involve much more than individual skill development, as it requires change at 
individual, interpersonal, organisational and institutional levels in relation to evidence access, appraisal, 
interpretation and use. Theories of adult learning provide insights into how individuals learn, which is 
important given the strong emphasis on training within the BCURE programme. For example, theories of 
andragogy and self-directed learning suggest several ‘key principles’ that may help inform EIPM training 
courses, and different schools of learning provide a diverse set of models for understanding the 
mechanisms that link training to individual behaviour change – including theories of self-efficacy and 
social learning.
<<<PAGE=18>>>
www.itad.com/knowledge-and-resources/bcure 18 
 
This section examines the theoretical and conceptual literature around EIPM, drawing links between diverse 
ideas, models and frameworks in order to illuminate the underlying theory behind the BCURE programme. 
Why consider theoretical and conceptual literature? 
Readers wishing to dip straight into the primary evidence on EIPM should skip to Section 2 or Section 3. 
However, we believe that there is a strong argument for understanding and learning from the diverse theories 
around EIPM, in order to strengthen current practice. 
First, recent literature is increasingly critical of evidence on EIPM for its failure to 
take account of theory. For example, the authors of a recent systematic review 
on enablers of and barriers to EIPM criticise researchers for failing to make use 
of rich theoretical insights from policy studies, or make explicit the models of 
research and policy change assumed in research (Oliver, Lorenc, et al. 2014). 
Similarly, EIPM has been criticised for viewing policy as purely rational and 
instrumental, failing to consider ‘the universe of policy discourse and practice’ 
or to acknowledge the role of normative beliefs about how evidence should be 
used in policy making (du Toit 2012; Wesselink et al. 2014). Failing to attend to 
the theoretical literature is likely to have an impact on programme 
effectiveness. For example, many EIPM interventions (including those discussed 
in Section 3) implicitly invoke linear models of policy change, which the 
evidence discussed below suggests do not always reflect the reality of policy 
processes. Building theoretical insights into interventions can help practitioners 
avoid common traps and design programmes that are more likely to lead to 
change. 
Second, recognising the theoretical and conceptual underpinnings to EIPM is particularly important for a realist 
evaluation. The BCURE realist evaluation aims to open up the ‘black box’ between interventions and 
outcomes, in order to understand what it is about the intervention that leads to (or does not lead to) change 
in different contexts (Pawson & Tilley 1997). Theoretical models help to signpost and predict outcomes that 
may be expected from EIPM capacity development interventions, as well as the mechanisms through which 
they lead to change. Therefore, drawing on the rich conceptual literature around EIPM will allow the BCURE 
evaluation team to develop and test theories about how BCURE projects contribute to (or fail to result in) the 
outcomes in the Theory of Change. 
Finally, EIPM is full of abstract theoretical concepts – including ‘evidence’, ‘policy’ and ‘capacity’. These terms 
can be, and are, interpreted in very different ways, which hinders attempts to synthesise knowledge and distil 
evidence about what works (McCormack et al. 2013). This section is used to help the BCURE evaluation team 
develop transparent definitions of these concepts, which are grounded in existing thinking and will help make 
sense of the primary data collected through the evaluation. 
The theory behind the BCURE programme 
The overarching theory of the BCURE programme is depicted in the programme Theory of Change described 
in the Background section above. It can be summarised as follows: 
Developing the capacity of decision makers to use research evidence (through building knowledge, skills, 
commitment, relationships and systems at individual, interpersonal, organisational and institutional levels) will 
“Failing to attend 
to the theoretical 
literature is likely 
to have an impact 
on programme 
effectiveness.”
<<<PAGE=19>>>
www.itad.com/knowledge-and-resources/bcure 19 
 
allow them to access, appraise and apply good quality evidence more effectively when forming policy. This will 
improve the quality of policies, ultimately benefitting more poor people. 
Many of the EIPM capacity development interventions considered in Section 3 implicitly share this theory, or 
something similar. Underpinning it are a number of assumptions about the nature of evidence, policy making 
and capacity development, summarised in the following three questions: 
1. What is ‘research evidence’, and what makes it ‘good quality’? 
2. What is ‘policy’, and how can evidence improve its ‘quality’? 
3. What is ‘capacity to access, appraise and apply evidence’, and how do we ‘build’ it? 
This section attempts to shed light on these questions through drawing on a range of theories from a variety 
of thematic fields – including international development, health, political science, and psychology. 
A brief background to ‘evidence-informed policy’ and the nature of the evidence base 
Calls for ‘evidence-based policy’ go back at least 50 years (Wesselink et al. 2014). Demands for policy to be 
‘informed by evidence’ are often driven by a growing focus on the need for robust decision making, 
accountability to funders, and pressures to ensure taxpayers’ money is spent on policies that ‘work’. The rise 
of EIPM in UK government discourse is associated by several writers with the modernising agenda of the New 
Labour government when it came to power in 1997 (Smith & Joyce 2012; du Toit 2012; Broadbent 2012). 
Some writers argue that this helped spread the concept of EIPM to the international development field and, 
consequently, the agendas of several low and middle-income countries through the influence of DFID (du Toit 
2012; Broadbent 2012). There is also an established movement for evidence-based medicine in public health, 
including calls by the World Health Organization (WHO) for an ‘evidence-based approach to health promotion 
and practice’ (Smith & Joyce 2012). 
These influences among others have inspired a growing and interdisciplinary literature on EIPM, spanning a 
number of fields including international development, health, public administration, education and 
management. A systematic review from the health field recently concluded that ‘evidence-based policy and 
practice, knowledge translation, and related concepts have become touchstones across a vast range of 
disciplines – almost sub-disciplines in their own right, with canons and conceptual toolkits of their own’ 
(Oliver, Lorenc, et al. 2014). 
There are a number of variants of the term ‘evidence-informed policy’, in the literature, including ‘evidence- 
based policy’ (Oliver et al. 2014), ‘evidence-based medicine’ (Yost et al. 2014), and ‘evidence-based decision 
making’ (Tang et al. 2005). Although these terms are not entirely synonymous, for the sake of simplicity this 
section groups them under the common label ‘EIPM’. 
Limitations of this section 
Given the large and interdisciplinary nature of the theoretical literature on EIPM, this section cannot claim to 
provide a full and systematic summary of all relevant evidence. Choices about literature to include were 
guided by the BCURE Theory of Change (see the Methodology section above). The Theory of Change was 
developed based on the experience and knowledge of the BCURE evaluation team and therefore (explicitly 
and implicitly) drew on a number of the EIPM frameworks and models discussed below. As such, this section 
does not claim to provide a neutral overview of the theoretical literature. Rather, it aims to further articulate 
the assumptions underpinning the programme Theory of Change, and highlight some of the theories that can 
potentially add value to the BCURE evaluation – as well as providing an accessible overview of some of the 
main ideas and debates in the field.
<<<PAGE=20>>>
www.itad.com/knowledge-and-resources/bcure 20 
 
1.1. What is ‘research evidence’, and what makes it ‘good quality’? 
The BCURE programme understands research evidence in a broad sense, to include published academic 
research papers, statistical databases, ‘established’ (i.e. widely debated and accepted) policy papers and 
positions, and evaluation findings (of sufficient quality and rigour) (DFID 2013a). Implicit in this definition is 
the idea that research evidence is based on particular methods, which are ‘scientific, independent, academic, 
rigorous, subject to validation and open to critique’ (Broadbent 2012). 
 
‘Research evidence’ is just one type of evidence required for policy making. There is widespread agreement in 
EIPM literature that there are many types of non-research evidence important to policy making processes 
(see e.g. Broadbent 2012; Jones 2009; Sutcliffe and Court 2005), including: 
 
 Process and practice knowledge concerning how to implement programmes or policies, for example 
based on organisational and systems data. 
 ‘Tacit’ knowledge – the unwritten, unspoken knowledge held by individuals based on their 
experiences. 
 Critical and reflective knowledge, relating to values and ethical commitments within a society. 
 Communal knowledge, for example about what counts as ‘common sense’ or ‘tradition’ in a 
community or culture. 
 Public opinion and other types of citizen knowledge. 
 
Several sources suggest that policy makers view ‘evidence’ for decision making as incorporating some or all of 
these categories, as well as research evidence. For example, the UK’s Department for Environment and Rural 
Affairs (Defra) has adopted a wide definition of evidence, incorporating research, statistical data and evidence 
from citizen knowledge (Shaxson 2014). Similarly, a study of the use of research evidence in four African 
policy debates found that ‘narrow “Western” understandings of research-based evidence fail to account for 
much of the evidence actually used in the policy debates studied, with practical and communal evidence often 
taking centre stage’ (Broadbent 2012). 
 
There is some agreement over what counts as ‘good quality’ evidence, but ‘evidence hierarchies’ are 
controversial. Evidence hierarchies explicitly rank different research approaches and methods according to 
their relative authority (Evans 2003). They often place randomised experiments (and systematic reviews of 
them) at the top, with observational studies accorded much lower credibility (Nutley et al. 2002). Evidence 
quality frameworks or criteria often do not explicitly rank research methods in this way, but instead provide 
criteria to assess the methodology and design of primary studies – through examining factors such as 
transparency, reliability, validity, appropriateness and cogency. Quality criteria also often refer to the ‘size of 
the evidence base’, underpinned by the assumption that studies can be ‘added up’ to generate more reliable 
findings. This implies the importance of systematic reviews and other types of evidence synthesis (Davies 
2013). 
 
Hierarchies and quality frameworks are particularly well developed and widely used in the health field; for 
example the GRADE approach used by the UK National Institute for Health and Care Excellence (NICE 2014). 
Several sets of evidence quality criteria also exist within the international development sector (e.g. DFID 
2014a; IMF 2003; USAID 2012).
<<<PAGE=21>>>
www.itad.com/knowledge-and-resources/bcure 21 
 
However, several commentators in the EIPM literature express concern that evidence hierarchies and quality 
criteria unfairly downgrade qualitative data, particularly data collected through methods such as ethnography. 
This tends to affect social science evidence, as well as privilege research evidence over the other types of 
knowledge discussed above (Sutcliffe & Court 2005; Nutley et al. 2002; Boswell 2014). The role of evidence 
hierarchies is also beginning to be questioned in the health field – the UK’s National Institute of Health and 
Care Excellence (NICE) holds a ‘sceptical view of the untargeted use of formal hierarchies’, arguing that ‘the 
appropriateness of the evidence to the question is more important, not its place in any such hierarchy’ (Ruiz & 
Breckon 2014). 
Research evidence is not neutral. Several writers from the public administration 
and international development fields suggest that ‘evidence’ (including 
‘research evidence’) is not a neutral category. First, Broadbent (2012) points out 
that research does not take place in a vacuum – it is commissioned, designed, 
framed, conducted and communicated by people with their own ideas about 
what is important and what is not, and their own beliefs and assumptions about 
the world and the topic they are researching. The evidence that is available on 
an issue will therefore always at least partly reflect the existing views and 
beliefs of researchers and research commissioners. 
Second, several sources suggest that evidence rarely (if ever) points clearly to an optimal decision, implying 
that debate and contestation over what evidence means and how it should be used is inevitable. Evidence on 
a given topic often exists in huge quantities, spanning multiple academic fields, and providing a huge array of 
(often contradictory) insights (du Toit 2012). This is particularly true in many areas of international 
development, where issues are complex and contested and the evidence base is often very small – for 
example, evidence on conflict and fragile states (Waldman 2014). Even ‘gold standard’ randomised control 
trials only provide an insight into whether a particular intervention worked in a specific context, requiring 
careful assessment in order to decide whether findings can be applied elsewhere (Pritchett & Sandefur 2013). 
Systematic reviews synthesise findings from primary sources in a rigorous way, but often gloss over important 
features of context – a crucial consideration in international development decision making (Mallett et al. 
2012). Evidence therefore cannot ‘speak for itself’ – all evidence requires interpretation in order to assess its 
relevance to a particular policy process or decision (Parkhurst 2014; Davies 2013; Wesselink et al. 2014). 
Summary and implications: It is widely agreed that research evidence is just one type of evidence required for 
policy. ‘Evidence hierarchies’ can be helpful guides to commonly agreed standards of ‘evidence quality’ (such 
as validity and reliability); but they should be used with caution, as appropriateness of evidence may be more 
important than position on a generic evidence hierarchy. Finally, several sources suggest that evidence is not 
neutral – firstly because it reflects pre-existing views and beliefs of researchers and commissioners, and 
secondly because it rarely points to an obviously optimal solution, implying that contestation over its meaning 
is inevitable. 
Based on this evidence, we recommend that: the BCURE evaluation should be conscious of the interplay 
between research evidence and other forms of knowledge, the appropriateness of evidence used by decision 
makers as well as its quality, the question of how far available evidence reflects the existing views and beliefs 
of researchers and commissioners, and finally the ways in which evidence is interpreted, debated and 
contested in decision making processes. 
  
“Research does 
not take place 
in a vacuum…”
<<<PAGE=22>>>
www.itad.com/knowledge-and-resources/bcure 22 
 
1.2. What is ‘policy’, and how can evidence benefit policy making? 
This section examines a diverse range of theories and models within EIPM literature, which help to articulate 
the ways in which evidence is used in (and can benefit) policy making. These theories are largely not mutually 
exclusive – Jones et al. (2009) argue that ‘the knowledge-policy interface is too complex to encapsulate in any 
single framework’. However, this section demonstrates that recent thinking on EIPM has moved away from 
some theories (such as rational and linear models of how evidence is used in policy) and towards others (such 
as theories acknowledging the central role of power and politics in policy making). 
This review focusses on ‘public policy,’ understood as as ‘a deliberate plan of action to guide decisions and 
achieve desired outcomes…adopted and implemented by government actors, which affects or is visible to the 
public’ (Jones 2009). The review adopts a broad definition of public policy: 
 Incorporating a wide range of activities; including the processes of decision making, the decisions and 
actions (written, spoken and implied) taken during and as a result of these processes, and the 
implementation of decisions and what happens as a result (Hallsworth et al. 2011; Jones 2009; Cloete 
& De Coning 2011; Dunn 2012). 
 Involving a broad range of actors, including local and national bodies (e.g. government ministries, 
local government departments); parastatal and semi-autonomous bodies; the legislature; and non-
state actors including the media, civil society, the general public, the private sector and international 
donors (Newman et al. 2012). 
1.2.1. Theories of policy processes and the role of evidence in decision making 
Historically, EIPM has been associated with rational models of policy processes and knowledge transfer. There 
are many different ways of conceptualising the role of evidence in policy processes. Several writers within the 
development, public administration and health fields argue that EIPM has historically been associated with 
‘rational’ models of policy change such as the policy cycle (Jones 2009; Hallsworth et al. 2011; du Toit 2012). 
The policy cycle model was first articulated by Lasswell (1977), and portrays a policy process as moving from 
defining a problem (agenda setting) through to policy formulation, selecting a preferred solution, designing 
the policy, implementing and monitoring it, and finally evaluating it, with the results fed back into the next 
round of the policy cycle. 
Models of EIPM that are based on the policy cycle depict evidence as providing neutral inputs at each point in 
the cycle, which improve policy incrementally and according to logic and reason (Jones 2009). An example 
includes Greenhalgh’s (2003) model of the six-stage evidence-based approach to healthcare. There are close 
links between these models and instrumental frameworks of evidence use in policy processes; which depict 
research findings as being consciously and directly applied by actors to shape policies, processes, or further 
research (Weiss 1982) (see Box 1). 
Guidelines from the UK Civil Service on the role evidence should play in UK policy processes are clearly aligned 
with these rational and instrumental theories. Given the potential influence of these frameworks on DFID 
policy processes (and subsequently the thinking behind the BCURE programme), they are worth considering 
here. The Modernising Government White Paper of 1999 and the 2010 Policy Skills Framework articulate the 
ways in which evidence is expected to influence UK policy processes, resonating clearly with the policy cycle: 
 Evidence helps define and frame issues, ensuring ‘the problem’ is accurately articulated. 
 Evidence helps articulate options and develop solutions to problems, enabling questioning of 
established ways of doing things, and learning about what has worked and not worked elsewhere.
<<<PAGE=23>>>
www.itad.com/knowledge-and-resources/bcure 23 
 
 Evidence allows different policy options to be assessed for issues such as cost-effectiveness, risk and 
benefits, and potential impacts – helping work out which option is the most appropriate in a 
particular situation. 
 Evidence helps demonstrate whether a policy has been effective or not, and understand how a policy 
has affected different groups of people – helping to inform decisions about what to do differently in 
future. (Cabinet Office 1999; Hallsworth & Rutter 2011; UK Civil Service 2010). 
Shaxson (2014) provides an example of how these 
guidelines play out in practice within the UK Civil 
Service. The UK’s Department for Environment, Food 
and Rural Affairs used the policy cycle model to 
frame five ‘big questions’ for policy teams to 
consider as part of the department’s Evidence 
Investment Strategy – including ‘Where are we 
now?’ (how evidence is used to understand the 
context); ‘Where are we going?’ and ‘How do we get 
there?’ (how evidence is used to understand drivers 
and trends, and identify solutions to problems), and 
‘How well did we do?’ (how evidence is used to 
monitor and evaluate progress and impact). 
However, the utility of rational and policy cycle 
models has been widely questioned. The policy cycle 
is widely criticised in recent literature on EIPM for its 
assumptions regarding the rational and problem-
solving nature of policy processes, and for ignoring 
the messy reality of policy making (Jones 2009; Morton 2012; Hallsworth et al. 2011). For example, Hallsworth 
et al. (2011) examined the perceptions of UK civil servants and ministers, finding that ‘virtually every 
interviewee dismissed policy cycles…as being divorced from reality.’ Evidence from lower-income contexts 
similarly questions the utility of the policy cycle; with several empirical studies (discussed further below and in 
Section 2) demonstrating that the role of evidence in policy processes bears limited resemblance to the stages 
in linear and rational models (e.g. Broadbent 2012; du Toit 2012; Hunsmann 2012). Weiss also presented 
empirical evidence to suggest that instrumental use of evidence (see Box 1) is ‘rare, particularly when the 
issues are complex, the consequences are uncertain, and a multitude of actors are engaged in the decision-
making process’ (Weiss 1980). 
The majority of theoretical papers on EIPM reviewed for this 
paper accept the premise that evidence is just one part of a 
patchwork of factors influencing policy decisions; alongside 
political and strategic considerations, expert opinion, 
stakeholder and public pressure, and resource constraints (e.g. 
Davies et al. 2012; Newman et al. 2012; Sutcliffe and Court 2005; 
Jones et al. 2013; Crewe & Young 2002). In recent years, the 
literature on EIPM has moved away from linear and rational 
models, and towards models which emphasise the role of 
politics and power (as opposed to deliberative reason and logic) 
in determining how evidence influences policy processes.  
Box 1.  Models depicting  the role of evidence in 
policy processes (see Weiss 1979, 1982) 
‘Instrumental’ model:  specific research findings are 
consciously applied to influence something concrete, 
such as a policy, programme, or other piece of research.   
‘Enlightenment’ model:  concepts and theories from 
research gradually ‘percolate’ through society, ‘coming 
to sha pe the way in which people think about social 
issues’. 
‘Interactive’ model: policy processes involve ‘a 
disorderly set of interconnections and back-and-
forthness’ between different groups. 
‘Political’ model: research is used to lobby for particular 
interests. 
‘Tactical’ model: research is used to delay decisions, 
deflect criticism, or enhance prestige. 
 
“Evidence is just one part 
of a patchwork of factors 
influencing policy 
decisions...”
<<<PAGE=24>>>
www.itad.com/knowledge-and-resources/bcure 24 
 
The ‘pluralism and opportunism’ model of evidence-use challenges the rationality of the policy making process, 
emphasising that policy making is often messy and opportunistic and requires pragmatic decisions by a range 
of actors in the face of uncertainty (Jones 2009). The pluralism and opportunism model relates to a number of 
political science theories that emphasise the non-linear nature of policy processes. For example, 
incrementalism – or the ‘science of muddling through’ – suggests that policy making evolves over time, 
through small, incremental steps in which values and empirical analysis are closely intertwined. Analysis of 
evidence is ‘drastically limited’ by time, resources and the limits of human rationality, meaning that possible 
outcomes and alternatives to decisions are inevitably neglected (Lindblom 1959). ‘Streams’ frameworks focus 
on the policy windows of opportunity that can open up around major events, providing opportunities for 
evidence to feed into the ‘problem stream’ (which specifies which issues are significant) or the ‘policy stream’ 
(the ideas on the table that are being considered to solve identified problems). ‘Spaces’ models emphasise 
particular places or moments where policies can be influenced – including ‘closed spaces’ (where policy is 
made by a small set of actors behind closed doors), ‘invited spaces’ (where civil society or other actors are 
given a platform to introduce new ideas) or ‘claimed spaces’ (where less powerful groups create spaces, or 
claim them for themselves) (Jones 2009). 
More radical models suggest that power is infused throughout the process of evidence production and use. 
Jones argues that the pluralism and opportunism model of EIPM has recently begun to give way to a ‘politics 
and legitimisation’ model, in which power and politics are held to be ‘infused through the knowledge process, 
from generation to uptake.’ In this model, evidence is understood to ‘reflect and sustain existing power 
structures’, actively used by policy actors ‘in processes of contest, negotiation, legitimisation and 
marginalisation’ (Jones 2009). Theories on the role of power in evidence use can be summarised in terms of 
three ‘interlocking types of relations’ (Jones 2009; Sumner et al. 2011): 
1. Power can be understood in terms of actors and networks – competing interest groups working 
together or against one another to advance their interests. Knowledge and evidence is used 
consciously by these groups to win political battles; as ammunition, or tactically to support decisions 
or stall action. This links to Weiss’s political model of evidence use (see Box 1 above), in which 
research is used to lobby for particular interests; while in a tactical model research is used to delay 
decisions, deflect criticism, or enhance prestige (Weiss 1979). Both of these fall under the umbrella of 
‘symbolic’ use of evidence, in which evidence is used to legitimise a decision that has already been 
made; for example a politician using research to justify a policy they would have created anyway 
(Weiss 1982). 
 
2. Power can also be understood in terms of formal and informal ‘institutions’ – which include 
organisations, socioeconomic environments, and patterns of behaviour, which shape the ‘rules of the 
game’. They define who is able to participate in decision making, and they shape the strategies, 
beliefs and actions of individuals within it. Issues such as the extent of democracy and media freedom 
and the level of government centralisation all generate opportunities and constraints, and affect 
future decisions. Knowledge is ‘translated’ in ways that fit with prevailing institutions, which may keep 
particular ideas off the agenda or embed others in law. 
 
3. Finally, theories of discourse hold that power and knowledge are inextricably intertwined. 
‘Knowledge’ in the form of concepts, metaphors, rules of logic and ideas which may be taken for 
granted or seen as ‘common sense’ in a particular society determine what policy makers can 
understand and articulate, and therefore the policy ideas they are likely to adopt. Theories of 
discourse can be linked to Weiss’s enlightenment model of research use (see Box 1 above): in which 
concepts and theories from research gradually ‘percolate’ through society, ‘coming to shape the way
<<<PAGE=25>>>
www.itad.com/knowledge-and-resources/bcure 25 
 
in which people think about social issues’. Policy makers may not be able to point to a specific study 
that influenced a decision, but research can sensitise them on new issues, turn non-problems into 
problems (and vice-versa), and redefine the policy agenda (Weiss 1979). 
Although the theoretical and conceptual literature on EIPM considered for this review generally discounts 
rational models of policy processes in favour of theories recognising the centrality of power and politics, this is 
not always reflected in EIPM programmes and practice. As discussed above, UK Civil Service guidelines and 
frameworks of evidence use still link strongly to the policy cycle model. Authors of one recent systematic view 
similarly found that policy cycle models are still ‘common currency’ within health policy and other fields 
(Oliver, Lorenc, et al. 2014). The primary empirical evidence base discussed in Section 3 also makes limited 
reference to the political models and theories discussed above, and much of it appears implicitly based on the 
assumption that evidence use is to some extent rational and linear. 
Some authors see the policy cycle as a useful starting point or heuristic device, while acknowledging that it 
does not reflect the realities of policy making (Sabatier & Jenkins-Smith 1993). Others point out that there are 
specific situations in which research can contribute to change in rational and linear ways. For example, Nutley 
et al. (2007) conceptualise research impact on a spectrum, ranging from more instrumental applications (in 
which research is used directly to inform practice and policy change) to more conceptual uses (in which 
research shifts knowledge, understanding and awareness of an issue). However, Hallsworth et al. (2011) 
question whether it is really acceptable to continue using the policy cycle as a model for understanding 
political processes, claiming it represents an ‘unrealistic ideal’ and a ‘policy myth’. Overall, the literature 
discussed in this section suggests the need for the BCURE evaluation to look beyond rational and linear 
models of policy processes and evidence use, while recognising their continued application within EIPM 
practices and processes. 
Summary: Many frameworks for understanding the role of evidence in policy and in improving policy quality 
link to the ‘policy cycle’ model. This portrays policy making as a rational and linear process, in which 
knowledge provides instrumental and neutral inputs at defined stages. The utility of this model has been 
widely criticised for ignoring the messy realities of policy making, but is still an important influence on EIPM 
thinking. 
More recent models relating to evidence use in policy processes place a greater emphasis on the role of power 
and politics in shaping the ways in which evidence is used in policy making; for example the theories of 
incrementalism, policy ‘spaces’ and policy ‘streams’. More radical models suggest that power is infused 
throughout the process of evidence production and use. In these theories, power – in terms of actors and 
networks, institutions, and discourse – is understood to not only influence how evidence is used but how it is 
understood and articulated in different contexts. 
Based on this evidence, we recommend that: the BCURE evaluation team make limited use of rational and 
linear models of policy change and knowledge translation, and look beyond instrumental theories of evidence 
use. The evaluation should explicitly consider the role of power and politics (in terms of actors and networks, 
institutions and discourse) when studying how evidence is used in BCURE contexts. 
 
1.2.2. Theories relating to researcher-policy relationships and networks 
Linked to but distinct from the models discussed above are a further set of theories regarding the 
relationships between researchers and policy makers, and the impact of these relationships on evidence use 
in policy processes.
<<<PAGE=26>>>
www.itad.com/knowledge-and-resources/bcure 26 
 
 In the ‘two communities’ model, policy actors are viewed as having different priorities, languages and practices 
from those of researchers (Brown 2012). This assumes that there is a ‘gap’ between researchers and policy 
makers that needs to be ‘bridged’ in order to get policy makers to use evidence in decision making (Innvaer et 
al. 2002). The authors of a recent systematic review argue that this assumption underpins a large amount of 
literature, particularly from international development and health, on research dissemination, uptake and 
knowledge ‘translation’ (Oliver, Lorenc, et al. 2014). 
 The ‘supply-demand’ paradigm similarly emphasises the factors that should be considered by those 
‘supplying’ research, in order to get it used by decision makers on the ‘demand side’ – for example, the role of 
research dissemination and communication strategies. This model underpins the BCURE programme, which is 
designed to address ‘demand-side’ constraints in the form of capacity gaps among decision makers to use 
evidence effectively (Newman et al. 2012). Similarly, models of ‘push and pull’ emphasise the links and 
exchanges between research producers and research users, and suggest that researchers need to make active 
efforts to ‘push’ their findings into the policy maker sphere (Brown 2012). 
These models all implicitly or explicitly conceptualise research producers and research users as two separate 
groups. In contrast, other models view the boundaries between these groups as blurred and indistinct. A 
secondary review found that the ‘two communities’ model is being replaced in the EIPM literature by a more 
‘dynamic and complex’ view of the links between research and policy (de Vibe et al. 2002). Smith and Joyce 
argue that the ‘two communities’ paradigm is wrong to imply that research and policy communities are either 
distinct from one another or relatively homogeneous – as ‘a variety of boundaries (epistemological, 
disciplinary and political) cut across professional differences’ and affect how knowledge is understood and 
used (Smith & Joyce 2012). 
 In opposition to the ‘two communities’ framework, a number of models portray evidence production and use 
as an interactive process. These are categorised by Best & Holmes (2010) as ‘second generation’ frameworks. 
Theories of ‘policy networks’ emphasise that researchers, policy makers and other groups (such as members 
of civil society and the media) often work together across professional divides, bound by shared value 
systems, political interests or specific problems – and drawing on evidence in various ways to do so (Smith & 
Joyce 2012; Morton 2012). These emphasise that – rather than existing in separate communities or on 
opposite sides of a supply-demand divide – civil society, the media, researchers and decision makers may all 
play a role in commissioning, producing and using research to influence policy (Best & Holmes 2010). These 
network models relate to Weiss’s interactive model of research use (see Box 1 above), which emphasises that 
policy development processes involve ‘a disorderly set of interconnections and back-and-forthness’ between 
different groups (Weiss 1979). 
‘Issue networks’ are one type of policy network, in which different actors with diverse interests and values 
come together around particular problems. For example, an issue network consisting of civil society, the 
media, the general public and government actors formed around the issue of sex-offender policy in the UK in 
2000 - suggesting that key actors in the government, civil society and the media were all both generators and 
users of various forms of evidence in a national debate about sex-offender community notification in 2000. 
The authors argue that this contradicts the ‘two communities’ theory, as actors formed a ‘kaleidoscopic 
picture … [which] defies description as two separate groups’ (Jung & Nutley 2008). ‘Policy communities’ 
represent another type of policy network, consisting of groups of specialists both inside and outside 
government who play a role in developing, testing and refining policy ideas. For example, Gabbay et al. (2003) 
present a case study of ‘communities of practice’ (CoPs) in the UK National Health Service, designed to bring 
together different people from various walks of life and professional backgrounds to achieve a policy task.
<<<PAGE=27>>>
www.itad.com/knowledge-and-resources/bcure 27 
 
Policy network models are reflected in a range of ideas within recent EIPM theoretical literature – including 
ideas of knowledge ‘co-production’ and collaboration, which depict actors from the policy and research worlds 
as working together to interpret and ‘construct’ evidence to inform decision making (see e.g. Jones 2009; 
Oliver 2012). Empirical evidence of these processes is discussed in a systematic review by Orton et al. (2011) 
examining the use of evidence in public health decision making. This review found evidence from several 
studies that ‘rather than being a neutral tool with which to inform decision making, evidence was in fact 
constructed through professional practice, and contributed to the construction of professional identity’. 
Gabbay et al. (2003) also find empirical evidence of knowledge co-construction within policy networks, 
discussed further in Section 3.2. 
Summary: The ‘two communities’ model views policy actors and researchers as two distinct groups, with 
different priorities, language and practices. However, ‘second generation’ models of knowledge transfer 
depict evidence production and use as an interactive process. Rather than viewing researchers and policy 
makers as existing across a distinct supply-demand divide, theories of ‘policy networks’ suggest that 
government and non-government actors often work together and draw on evidence to shape policy in a 
variety of ways. Some empirical evidence also suggests that policy networks play a role in ‘co-producing’ and 
‘constructing’ knowledge to inform decision making. This has clear links to the evidence presented in Section 
1.1 suggesting that research evidence is not neutral, but inevitably involves debate and contestation over 
what findings mean and how they should be used. 
Based on this evidence, we recommend that: the BCURE evaluation draw on interactive and network theories 
to consider how different actors across and beyond traditional research/policy boundaries affect the access, 
appraisal and use of evidence. It would also be interesting to consider whether and how networks within the 
BCURE programme interpret and construct knowledge, and the impacts this has on decision making. 
1.2.3. Theories relating to cognitive processes 
This section presents a final set of theories from the psychological literature, where evidence relating to 
cognitive processes and mental models provide insights into the interpretation and use of evidence in policy 
processes. A detailed summary of this large evidence base is beyond the scope of this review, but this section 
provides a very brief introduction to some of the evidence particularly relevant to EIPM thinking.  
Cognitive theories and models from psychological research emphasise the importance of mental models, 
contextual cues and social norms, which affect how people interpret evidence. Psychological theories are not 
widely referenced in the literature on EIPM; although they have long been acknowledged in business and 
management literature (for example see the website changingminds.org). However, theories from the 
psychology field are highly relevant to understanding how policy makers use and understand evidence in 
decision making. The 2015 World Development Report (WDR) synthesised some of this evidence, challenging 
the assumption inherent in classical economics (and in the EIPM literature based on the rational model 
discussed in Section 1.2.1 above) that people are ‘rational actors’ who weigh up costs and benefits and use 
this to take a reasoned view about what to do. Instead, ‘people are malleable and emotional actors whose 
decision making is influenced by contextual cues, local social networks and social norms, and shared mental 
models’ (World Bank 2015b). The WDR points out several principles of human cognition that are relevant to 
EIPM, including: 
 People ‘think automatically’. They process the huge amounts of information they have to assimilate by 
simplifying problems, filling in missing information based on assumptions about the world, and 
assessing situations ‘based on associations that automatically come to mind and belief systems that
<<<PAGE=28>>>
www.itad.com/knowledge-and-resources/bcure 28 
 
we take for granted’. Thinking automatically contrasts with the assumption implicit in rational models 
of EIPM, that people make policy decisions ‘deliberatively’ by carefully weighing up alternative 
options and making a balanced, reasoned choice. 
 
 People ‘think with mental models’ which help them make sense of the world around them. Mental 
models include social meanings, norms, concepts, categories, identities, stereotypes and worldviews 
drawn from individuals’ cultures and communities. A survey of World Bank officials conducted for the 
2015 WDR provides a clear example of how thinking with mental models can affect the appraisal of 
evidence. World Bank staff were presented with identical data in two different contexts and asked to 
identify the conclusions that best explained the data. The first context related to the effectiveness of 
skin cream, and the second to the question of whether minimum wage laws reduce poverty. Officials 
were less likely to get the answer right in the second context, in spite of their presumably greater 
knowledge of labour laws than of skin cream. The authors conclude that ‘faced with a demanding 
calculation, they interpreted new data in a manner consistent with their prior views, about which they 
felt confident’ (World Bank 2015b). 
The World Bank case can also be understood as an example of ‘confirmation bias’, the well-observed 
psychological tendency for people to disregard or disbelieve evidence that does not correspond with existing 
beliefs (Nickerson 1998). Another cognitive bias observed in the UK Civil Service is that of ‘anchoring effects’, 
in which ‘the first piece of information we receive irrationally governs our subsequent decisions’ (Hallsworth 
et al. 2011). Finally, the UK Cabinet Office has official guidance on how to avoid ‘over-optimism’ bias – the 
‘demonstrated, systematic, tendency for project appraisers to be overly optimistic’ in their estimations of 
costs, benefits, values and time profiles (Cabinet Office 2002). These biases are all well-established in the 
psychological literature, and are likely to affect the ways individuals understand, interpret and appraise 
evidence in policy processes. 
Summary: Cognitive theories and models from psychological research are highly relevant to EIPM debates, as 
they emphasise the importance of mental models, contextual cues and cognitive biases on shaping the ways 
people think. These reinforce theories and empirical evidence from political science, suggesting the powerful 
influence of non-rational cognitive processes and pre-existing beliefs on shaping individuals’ understanding and 
interpretation of evidence. 
Based on this evidence, we recommend that: The BCURE evaluation should explicitly recognise the role of 
cognitive processes and mental models in shaping individuals’ understanding and interpretation of evidence – 
for example in relation to training and individual behaviour change. 
1.2.4. How does evidence use contribute to policy quality? 
Central to the overarching BCURE theory is the assumption that research evidence makes policy ‘better 
quality’ than it would have been otherwise. Broadbent argues that ‘evidence-based policy has become a 
byword for policies considered scientifically sound, objective, long term in focus and – implicitly – “better” 
than policies not based on research-based evidence’ (Broadbent 2012). Wesselink et al. (2014) similarly argue 
that EIPM ‘as prescription’ has filtered into policy making in certain high-income countries including the UK; 
where ‘using evidence’ is now one of the core principles in the UK Civil Service model of ‘professional policy 
making’ (Hallsworth et al. 2011). However, as discussed in Section 1.2.1, the standards and guidelines to 
evidence use adopted by the UK Civil Service (which imply that good quality policy uses evidence to frame 
issues; articulate and develop solutions; weigh up policy options; and demonstrate policy effectiveness) are 
largely based on rational and linear conceptions of policy processes, which the majority of EIPM sources
<<<PAGE=29>>>
www.itad.com/knowledge-and-resources/bcure 29 
 
examined for this review reject. How can the idea of policy quality be reconciled with the theories discussed 
above, which suggest the fundamental importance of power, politics, networks and cognitive influences on 
evidence use? 
Newman et al. (2012) offer a more nuanced understanding of the role of evidence in promoting policy quality; 
implying that good quality policy is that which draws on ‘a broad range of research evidence; evidence from 
citizens and other stakeholders; and evidence from practice and policy implementation as part of a process 
that considers other factors such as political realities and current public debates.’ The authors explicitly align 
this definition with the ‘pluralism and opportunism’ model outlined in Section 1.2.1; which challenges the 
rationality of the policy making process and emphasises that policy making is often messy and opportunistic, 
but still ‘retains assumptions about the potential for research to contribute to better policy formulation.’ This 
definition also retains an assumption that evidence is – at least to some extent – an expression of objective 
fact, which policy actors can consider alongside other factors in order to make optimum decisions. 
Several of the theories discussed in this section pose a challenge to this latter assumption. Section 1.1 
articulated arguments that evidence does not ‘speak for itself’ – frequently large in its scope and 
contradictory in its findings, all evidence requires interpretation. For evidence to express (even to some 
extent) objective fact, different actors must therefore be expected to come to the same conclusions when 
they interpret the same evidence; but the ‘politics and legitimisation’ model discussed in Section 1.2.1 holds 
that power influences not only how evidence is used but also how different actors are able to understand it. 
Psychological theories similarly suggest that the people naturally interpret evidence in line with existing 
beliefs and values, rather than through logical processes of deliberative decision making. Finally, theories of 
policy networks suggest that networks of different actors play the role of co-constructing knowledge as well as 
analysing and utilising it. 
This review does not aim to demonstrate the ‘truth’ of these theories. However, they provide useful insights 
and lenses to help the evaluation examine the role of evidence in policy processes, and there is some 
empirical evidence to support them. As part of the remit of the BCURE evaluation is to examine and measure 
the impact of BCURE interventions on policy quality, this suggests that the evaluation must navigate a tension 
between the theoretical literature on EIPM, and existing understandings of policy quality. This may require 
the evaluation to adopt an iterative approach to the measurement of ‘policy quality;’ beginning with existing 
standards based on the policy cycle model and Newman et al's (2012) ‘pluralism and opportunism’ definition 
above; but looking to further develop these standards in light of the primary evidence collected, and taking 
the conceptual literature discussed in this section into account. 
Summary: Existing definitions of policy quality often link to rational and linear conceptions of policy processes, 
which the majority of EIPM sources examined for this review reject. Newman et al (2012) present a less 
rational and linear interpretation of (quality) evidence-informed policy; understood as that which considers ‘a 
broad range of research evidence; evidence from citizens and other stakeholders; and evidence from practice 
and policy implementation as part of a process that considers other factors such as political realities and 
current public debates.’ However, this definition retains implicit assumptions about the objectivity of 
evidence, which several theories discussed in this section challenge. There is therefore a tension between 
more recent thinking within the EIPM theoretical literature, and existing definitions of policy quality. The 
BCURE evaluation will need to navigate this, given its remit to measure improvements in policy quality as a 
result of BCURE interventions.
<<<PAGE=30>>>
www.itad.com/knowledge-and-resources/bcure 30 
 
Based on this evidence, we recommend that: The BCURE evaluation should adopt an iterative approach to 
measuring policy quality; beginning with existing standards discussed in this section but aiming to further 
develop these standards in light of emerging primary evidence and the conceptual literature. 
1.3. What is ‘capacity’ for EIPM and how do we ‘build’ it? 
The OECD-DAC define capacity as ‘the ability of people, organisations and society as a whole to manage their 
affairs successfully’ (OECD-DAC 2006). This and other similar definitions understand capacity as a multi-layered 
set of processes, incorporating the idea of resilience or sustainability – the ability of a society or sector to 
continue to develop important skills, behaviours, networks and institutions into the future (Kaplan 1999; Ubels 
et al. 2010). 
 
1.3.1. Theories of capacity development 
Support for capacity ‘building’ or ‘capacity development’ can be viewed as ‘what outside partners – domestic or 
foreign – can do to support, facilitate or catalyse capacity development and related change processes’ (OECD-
DAC 2006). A recent review conducted by Itad for the Global Environment Facility distinguished between 
‘traditional’ approaches to capacity development, which tended to focus rather narrowly on building the skills 
needed to conduct a specific task, and more recent interventions that hold the more nebulous aim of 
improving the ability of a society or sector to continue to develop necessary skills, behaviours, networks and 
institutions that enable communities to adapt and self-renew into the future.6 
 
Capacity development is about more than ‘skills’. Several leading behaviour change researchers argue that 
behaviour change requires a combination of positive intention, skills and absence of environmental 
constraints (Fishbein & Middlestadt 1994). The Kirkpatrick training evaluation model considers all these 
elements in its ‘four levels of training evaluation’ (Kirkpatrick Partners n.d.). 
 
 Level 1 – Reaction: the satisfaction of participants with a training activity; their degree of active 
involvement in and contribution to the training process; the relevance of the learning to participants’ 
day-to-day jobs. 
 Level 2 – Learning: participants’ knowledge, skills, attitude, confidence and commitment – before and 
after the training. 
 Level 3 – Behaviour: participants’ application of the training when back on the job: the extent to which 
learning has been applied in the policy makers’ native environment where factors beyond their control 
may constrain or support implementation. 
 Level 4 – Results: the effect of the training within the participants’ organisation or wider environment. 
 
Capacity development can be understood as ‘complex.’ Recent definitions of capacity also increasingly 
incorporate the concept of complexity (e.g. Baser & Morgan 2008). Complexity theories hold that that change 
in ‘complex’ settings does not happen in a rational, linear way that can be predicted in advance. Instead, 
individual behaviours and interactions between people combine and amplify one another in diverse and 
sometimes surprising ways, with consequences that no one could have predicted (Smith & Joyce 2012; 
Ramalingam 2013). Various elements of the complexity literature have implications for studying capacity 
development in the context of EIPM (Smith & Joyce 2012; Morton 2012; Ramalingam 2013): 
 
 
6 The review is unpublished, but findings are discussed in a series of blog posts here: http://www.itad.com/knowledge-and-
resources/capacity-development-2/
<<<PAGE=31>>>
www.itad.com/knowledge-and-resources/bcure 31 
 
 Complex systems consist of many components which interact in dynamic ways. The behaviour of a 
complex system (e.g. a government ministry or executive) results from interaction between its ‘parts’ 
(the individuals working for that organisation or institution). These interactions give rise to emergent 
properties of the system, which are more than the sum of the individual behaviours. 
 
 This emphasises the need to analyse whole systems, rather than 
breaking them into constituent parts. In the context of 
studying capacity for EIPM, a system cannot be understood by 
reducing it to the individuals within it and examining their 
individual decisions and behaviours around accessing, 
appraising and applying evidence. Instead, complexity theories 
suggest that EIPM can only be understood by studying whole 
systems, consisting of multiple, interacting relationships and 
the variety of actors involved in policy processes. This suggests 
the need for ‘holistic approaches to understanding change, 
such as case study approaches, action research or embedded 
researcher models’ (Morton 2012). Complexity approaches 
also link to the ‘policy network’ models discussed in Section 
1.2.2 above. For example, Morton (2012) argues that 
complexity theories encourage ‘a focus on networks of 
researchers and research-users utilising, reinterpreting and 
integrating knowledge with other knowledge within systems.’ 
 
 Context is crucially important. Complex systems are sensitive to initial conditions, which have long-
term consequences. This suggests the importance of context and historic factors that enable and 
block change (Morton 2012). ‘Path dependency’ is therefore a feature of complex systems, in which 
historical decisions shape subsequent choices and present barriers to change (Abeysinghe 2012). 
 
 Non-linearity and feedback loops in complex systems make it difficult to predict behaviour, and small 
actions can have big effects. The non-linear nature of complex systems and the importance of 
‘feedback loops’ (in which a specific change feeds back to either amplify or dampen further change) is 
likely to result in periods of significant, sudden change and periods of inertia (‘punctuated 
equilibriums’). Rather than policy change always happening ‘incrementally’ (Lindblom 1959), we 
therefore shouldn’t be surprised when research influences policy in ‘often unpredictable ways over 
various timeframes’ (Morton 2012) – for example where vast bodies of research have little impact, or 
when minor events suddenly lead to much higher research use. 
 
Thinking of capacity development for EIPM as a complex issue suggests the need to focus on the many 
influences on individual behaviour, the need to expect capacity change to be unpredictable and incorporate 
feedback loops, and the need to understand how networks and relationships affect capacity change.7 
 
Capacity development for EIPM can be understood as ‘multi-dimensional,’ requiring change on four ‘levels’. The 
above insights from behavioural change research and complexity theories imply that multiple initiatives are 
needed to work together holistically over time to support and catalyse capacity development (FAO 2010; 
Capacity.org n.d.). The BCURE Theory of Change categorises capacity development interventions for EIPM into 
four ‘levels’ (drawing on Ubels et al. 2010; Baser & Morgan 2008): 
 
7 Ibid. 
“Complexity theories 
suggest that EIPM can 
only be understood by 
studying whole systems, 
consisting of multiple, 
interacting relationships 
and the variety of actors 
involved in policy 
processes.”
<<<PAGE=32>>>
www.itad.com/knowledge-and-resources/bcure 32 
 
 
1. Individual change includes individuals’ development of skills and knowledge, but also includes the 
motivation, attitudes, commitment, values and personal incentives that affect individual behaviour. ‘Skills’ 
for EIPM as understood in the BCURE programme include the ability to search for and appraise evidence, 
as well as the ability to weigh evidence with other factors and use it to inform decision making. Individual 
change also includes the motivation, commitment, values and incentives that affect individual behaviour. 
 
2. Interpersonal and network change refers to the relationships between individuals and groups, and how 
these influence evidence interpretation and use. For example, ‘evidence champions’ within an 
organisation might encourage colleagues to change their attitudes or behaviours around evidence use. 
This also incorporates change within formal and informal communities (or networks) of individuals or 
organisations – such as professional communities providing access to or syntheses of evidence, and 
informal groups within organisations united by particular knowledge interests or personal relationships. 
 
3. Organisational change refers to change in the systems, policies and procedures, practices, culture or 
norms within an organisation, which support (or inhibit) evidence access, appraisal and application in 
decision making. 
 
4. Institutional change refers to change in the wider operating environment of individuals or organisations. 
This includes change within civil society and the media, as well as broader social change (e.g. in culture, 
norms, collective beliefs, attitudes, values) and change in external influencing factors (e.g. global events, 
political and economic factors, donor influence), which affect the use of evidence. 
 
1.3.2. Theories on the characteristics of adult learners, and principles of adult learning 
The final set of theories considered below relate to individual capacity change, examining theories of 
individual learning. This level of change is singled out for consideration given its strong emphasis within the 
BCURE programme – all BCURE projects involve some type of teaching or training. Training activities are 
always, implicitly or explicitly, underpinned by theories and ideas about how people learn and put their 
learning into use, and will be important to help the evaluation team understand how training activities lead to 
(or do not lead to) change in different BCURE contexts. Some of the main concepts and ideas within this 
literature are explored below. 
 
Unsurprisingly, there is no single accepted model of adult learning. Instead there are a ‘mosaic of theories, 
models, sets of principles, and explanations’ which attempt to explain how people learn. Two of the ‘pillars’ of 
adult learning theory are andragogy and self-directed learning (Merriam 2001), which both examine the 
characteristics of adult learners. 
 
Andragogy is one of the most widely-known frameworks, contrasting adult learning (andragogy) with 
educating children (pedagogy). Andragogy holds that adults are active and reflective learners, who learn best 
when engaged in the learning process and when they can put their learning into action. They need to know why 
they are learning, what the goal is and whether they can achieve it, and they expect immediate relevance to 
what they can learn. Adults also bring their previous experiences and competencies with them when they are 
learning. When this is dismissed, the likelihood of learning decreases (Knowles et al. 2005). 
 
Self-directed learning focuses on the role of learning as part of adults’ everyday life – ‘a process in which 
individuals take the initiative, with or without the help of others, in diagnosing their learning needs, 
formulating learning goals, identifying human and material resources for learning, choosing and implementing
<<<PAGE=33>>>
www.itad.com/knowledge-and-resources/bcure 33 
 
appropriate learning strategies, and evaluating learning outcomes’ (Knowles 1975). Self-directed learning is 
held to be widespread; occurring as an ordinary part of adults’ everyday lives; and systematic – not 
dependent on an instructor or a classroom (Tough 1967; 1971). 
 
Key principles of adult learning based on these two theories: Some authors have drawn on ideas from 
andragogy and self-directed learning to distil ‘key principles’ of adult learning for those designing training 
courses (Bryan et al. 2009; Lyon et al. 2011): 
1. Adults need to know why they are learning. 
2. Adults are motivated to learn by the need to solve problems. 
3. Adults’ previous experience must be respected and built upon. 
4. Adults need learning approaches that match their background and diversity. 
5. Adults need to be actively involved in the learning process. 
6. Adults need extended contact (rather than one-off training sessions) in order to assimilate learning. 
 
Issues of power and autonomy: The theories of andragogy and self-directed learning have been criticised from 
some angles for being overly technical, context-free, and focusing largely on the characteristics of adult 
learners, ignoring issues of power and the possibility that self-directed learning may steer people towards 
simply conforming to predominant interests (Collins 1988). Freire’s concept of ‘conscientisation’ is relevant 
here – emphasising the role of learning in developing the ability to transform the learner’s reality (Freire 
1972). Similarly, Brookfield emphasises the role of adult learning in helping adults understand the world and 
their own experiences in their own terms, as opposed to imposing ideas from the outside (Brookfield 1985). 
 
Theories about how and why people learn 
 
Smith (2003) categorises learning theories into four groups: 
 
1. Behaviourist theories: the learning process is about changes in behaviour, stimulated by the external 
environment. 
 
Behaviourist theories suggest that the purpose of education is to produce a desired behavioural change, and 
can be achieved through arranging the environment in such a way as to elicit the desired response. Learning is 
therefore something that can be measured and seen – the end-product of a process. The theory of 
reinforcement falls into this school, suggesting that a learner will repeat a desired behaviour if positive 
reinforcement is given in the form of material or non-material rewards (Dunn 2002). Many approaches to 
increasing EIPM through capacity development appear to be implicitly or explicitly based on behaviourist 
approaches, attempting to measure learning through observed behavioural change (see Section 3). 
 
However, behaviourist approaches have been criticised as a ‘blunt instrument’. For example they imply that 
learning must be observed through change. However, learning can also include less visible processes such as 
abstracting meaning and relating new knowledge to one’s own experience or the wider world, or interpreting 
and understanding reality in a different way (Dunn 2002). 
 
2. Cognitivist theories: whereas behaviourists focus on the environment, cognitivist theorists focus on learning 
as internal mental processes – such as insight, information processing, memory and perception. 
 
Cognitive theorists emphasise the role of education in developing individual capacity and skills to learn better, 
for example by structuring the content of learning activities in particular ways.
<<<PAGE=34>>>
www.itad.com/knowledge-and-resources/bcure 34 
 
Self-efficacy is an example of a cognitive theory that has implications for adult learning. Self-efficacy concerns 
people’s beliefs about their capability to perform a particular task or handle a particular situation. This theory 
is based on the principle that individuals are more likely to behave in a particular way if they possess high self-
efficacy; that is, performance and motivation are partly determined by how effective people believe they can 
be (Bandura 1977). This can result in ‘self-fulfilling prophecies’ – if a person is confident they will do well in 
something, they are more likely to try harder at it and therefore gain good results. Bandura argued that the 
most important source of self-efficacy is a person’s performance outcomes – judgements of how they have 
performed at a given task previously. Self-efficacy can also be developed vicariously – if someone similar to a 
person succeeds, this can increase a person’s self-efficacy (and vice-versa). Verbal persuasion, in the form of 
encouragement or discouragement, can also influence a person’s self-efficacy. Empirical evidence suggests a 
link between self-efficacy, motivation and outcomes such as work attendance, productivity and future 
employment (Bandura 1988; Eden & Avirma 1993). 
 
3. Humanist theories: the learning process is a ‘personal act to fulfil potential’, stimulated by a person’s 
affective and cognitive needs. 
 
Humanist theories hold that the purpose of education is to help people become self-actualised and 
autonomous. Andragogy and self-directed learning (discussed above) fall into this group, along with 
facilitation theory. This holds that learning occurs when the educator acts as a ‘facilitator’, establishing an 
atmosphere in which learners feel comfortable with new ideas and are encouraged to take responsibility for 
their own learning (Dunn 2002). 
 
4. Social and situational theories: the learning process is one of interaction and observation in social contexts 
and relationships between people. 
 
These theories hold that education is about promoting participation in communities of practice, in which 
conversation can occur.  
 
Social learning theories are a sub-set of social and situational theories. There are many definitions, one 
emphasising that social learning works through ‘people learning from observing other people’, through 
attending to a behaviour, remembering it as a possibility, and then rehearsing it in practice (Smith 1999). 
Another definition suggests that social learning is a change of ‘understanding’ which goes beyond individuals, 
resulting in collective change at a network or societal level. This occurs ‘through social interactions and 
processes between actors within a social network’ (Reed et al. 2010). A more radical model is that of ‘situated 
learning’ – in which learning is not seen in terms of the acquisition of knowledge by individuals but rather as a 
process of social participation. People join communities at the periphery, but as they become more 
competent they move to the centre of the community (Lave & Wenger 1991). This portrays learning as 
something that exists in the relationships between people, rather than as a ‘possession of individuals that can 
be found inside their heads’ (Murphy 1999). 
 
Summary and implications: Recent theories and definitions of capacity emphasise that it consists of a multi-
layered set of processes, which are about more than ‘skills’. Capacity development can be understood as 
complex (requiring an understanding of whole systems, and interventions at different levels of these systems), 
and multi-dimensional, involving change at individual, interpersonal, organisational and institutional levels. 
 
Theories of adult learning provide insights into how individual-level capacity is developed, which is important 
given the strong emphasis on training within the BCURE programme. There is no single accepted model of 
adult learning, but theories of andragogy and self-directed learning suggest several ‘key principles’ that may
<<<PAGE=35>>>
www.itad.com/knowledge-and-resources/bcure 35 
 
help inform EIPM training courses – for example, adults need to know why they are learning, and be actively 
involved in the learning process. Other learning theories emphasise issues of power and the role of learning in 
transforming the learners’ reality. Finally, behaviourist, cognitivist, humanist and social theories of learning 
provide a diverse set of models for understanding the mechanisms that link training to learning – including 
theories of self-efficacy and social learning which are further explored in Section 3. 
 
Based on this evidence, we recommend that: the BCURE evaluation should examine and measure capacity 
change at multiple different levels; viewing capacity for EIPM as a complex issue and recognising the 
importance of studying whole systems, considering context, and expecting capacity development and 
evidence used to be potentially unpredictable and involve feedback loops. It should also draw on the rich 
literature on learning theories to help understand what is happening within BCURE training activities to result 
in individual behaviour change. 
 
1.4.  Conclusions and implications for the BCURE evaluation 
 
This section has provided an overview of some of the main theories and concepts within the EIPM and 
capacity development literature. The discussion was structured around three questions, which help to unpack 
the assumptions underlying the BCURE programme theory. 
 
1. What is ‘research evidence’, and what makes it ‘good quality’? The literature emphasises the 
importance of other types of knowledge as well as research evidence, cautions against over-reliance 
on generic evidence hierarchies, and stresses that evidence is never neutral. 
 
2. What is ‘policy’ and how can evidence benefit policy making? This review understands policy as a 
deliberate plan of action to guide decisions and achieve desired outcomes, encompassing policy 
processes, policy decisions and actions, and policy implementation. The rational ‘policy cycle’ model 
has largely been superseded in the theoretical literature by models emphasising the non-linear nature 
of policy change, the importance of interactions between various networks of actors, and the role of 
power and politics in shaping evidence use. Insights from psychological literature also emphasise the 
importance of mental models, contextual cues and social norms, which affect how people understand 
and interpret evidence. However, definitions of policy quality considered in this review – such as 
those used by the UK Civil Service – retain close links to linear and rational models; or at least 
assumptions about the objectivity of knowledge and evidence interpretation, which several theories 
discussed in this section challenge. There is therefore a tension between several significant EIPM 
concepts, and existing definitions of policy quality. The BCURE evaluation will need to navigate this, 
given its remit to measure improvements in policy quality as a result of BCURE interventions. 
 
3. What is ‘capacity to access, appraise and apply evidence’, and how do we ‘develop’ it?  Recent 
empirically based definitions of capacity development suggest that it is complex and multi-
dimensional, incorporating change at individual, interpersonal, organisational and institutional levels. 
Theories of adult learning provide insights into how learning takes place at an individual level, which is 
important given the strong emphasis on training within the BCURE programme – for example, several 
‘key principles’ are suggested that may help inform and assess EIPM training courses and provide 
insights into the mechanisms that link training to individual behaviour change.
<<<PAGE=36>>>
www.itad.com/knowledge-and-resources/bcure 36 
 
Implications for the BCURE evaluation 
 
The theories discussed in Section 1.1 imply that the BCURE evaluation should consider the interplay between 
research evidence and other forms of knowledge in the context of the BCURE programmes, as well as the 
appropriateness of evidence used by decision makers (as well as its quality). The evaluation should also 
consider how far available evidence in BCURE programme contexts reflects the existing views and beliefs of 
researchers, and how debates on evidence play out within particular policy processes. 
 
The findings from Section 1.2 suggest the limitations of rational and linear models of policy change and 
evidence use in policy processes, and suggest that the BCURE evaluation should look beyond these to 
explicitly consider power and politics (in terms of actors and networks, institutions and discourse) when 
studying how evidence is used. There may also be value in drawing on theories of policy networks, to consider 
how different actors across and beyond traditional research/policy boundaries affect the access, appraisal and 
use of evidence. Theories and empirical evidence from the psychological literature suggest the importance of 
the evaluation explicitly recognising the role of cognitive processes and mental models in shaping individuals’ 
understanding and interpretation of evidence – for example, in relation to training and individual behaviour 
change. In addition, the tensions between the EIPM conceptual literature and existing understandings of 
‘policy quality’ suggest the value of the evaluation adopting an iterative approach to the study of ‘policy 
quality,’ beginning with existing standards (while recognising the tension between these and more recent 
theories of EIPM), but looking to further develop the concept of ‘policy quality’ in light of the primary 
evidence collected and the theories of power, politics, networks and complexity discussed in this section. 
 
Finally, the findings from Section 1.3 suggest that the evaluation should examine and measure capacity 
change at multiple different levels (individual, interpersonal, organisational and institutional). The evaluation 
can also potentially benefit from applying core concepts of complex systems thinking to the study of capacity 
change (such as expecting it to be unpredictable and to incorporate feedback loops, and focusing explicitly on 
the role of relationships and networks). The evaluation should also draw on the rich literature on learning 
theories to help understand how BCURE training activities help lead to individual behavioural change.
<<<PAGE=37>>>
www.itad.com/knowledge-and-resources/bcure 37 
 
2.  What factors promote and constrain evidence-informed policy making? 
Overview 
The BCURE programme responds to evidence that decision makers in low and middle-income countries often 
do not access, appraise or apply research evidence effectively in decision making. This section asks why this is 
the case. 
There is a large amount of evidence on the barriers to and facilitators of EIPM, synthesised in a number of 
secondary reviews. However, this evidence has been criticised for focusing on single elements of the policy-
making process and relying on the perceptions of research producers and users; rather than considering how 
evidence is actually used within policy processes as a whole. These criticisms resonate with the discussion in 
Section 1, which emphasises the importance of recognising the messy and political nature of evidence use in 
policy making. This section therefore focusses on synthesising some of the growing primary evidence on 
political, psychological, cultural and institutional factors promoting or constraining EIPM in different contexts; 
areas where evidence has been less frequently synthesised and which take into account theories of power, 
politics, networks, cognitive processes and complexity discussed in Section 1.2. 
 
The key findings can be structured according to the BCURE Theory of Change as follows: 
 
1. Individual-level factors:  Nine primary studies provide evidence to suggest that individual beliefs, attitudes 
and motivations to use evidence are connected to pre-existing beliefs, and to the norms and values that 
prevail within organisations or societies. For example, several studies suggest that evidence may be 
ignored or side-lined if it counters past experience – particularly if an issue is hotly debated. 
 
2. Interpersonal (relationship and network) factors: The large literature on ‘supply-side’ factors affecting 
EIPM suggests that evidence use is influenced by the type and nature of relationships between 
researchers and policy makers – although this literature falls outside the scope of this review. In addition, 
two primary studies indicated the importance of relationships and power within government 
organisations in affecting what kinds of evidence are seen as acceptable. 
 
3. Organisational factors: Eight primary studies suggest that organisational factors can affect individual 
motivation to use evidence, or present barriers to changes in individual behaviour. For example, if 
evidence is promoted or valued within an organisation, this can increase individual motivation for EIPM, 
and lack of time to access and appraise research partly reflects an organisation’s ‘culture’ of evidence use. 
 
4. Institutional factors: Seven primary studies provide evidence of non-governmental actors both promoting 
and hindering evidence use in policy processes. International donors can both encourage and constrain 
the effective use of evidence in decision making; private sector actors can exert pressure which ‘blocks’ 
evidence-informed decisions, and the media (and the general public) may present a barrier to EIPM. This 
paper did not delve into the broad literature on civil society and its role in influencing policy, but did 
consider secondary evidence suggesting that civil society can exert pressure on government to use 
evidence, build momentum behind ideas, and bring together different forms of knowledge. Finally, five 
primary studies suggest that institutional factors such as sudden change (e.g. crises or regime changes), 
levels of decentralisation and levels of democracy can all generate opportunities for or barriers to EIPM.
<<<PAGE=38>>>
www.itad.com/knowledge-and-resources/bcure 38 
 
The BCURE programme was designed based on evidence that decision makers in low and middle-income 
countries often do not access, appraise or apply research evidence effectively in decision making (DFID 2012). 
This section asks why this is the case. What factors prevent decision makers from using evidence, and 
conversely what factors facilitate evidence use? 
Summary of the evidence base on barriers to and facilitators of EIPM 
Our search found a large amount of evidence on factors promoting and constraining EIPM, including six 
secondary reviews published since 2010. These secondary studies are summarised in Table 3 below. 
Table 3. Recent secondary reviews synthesising barriers to and facilitators of EIPM 
Source Field Geographical 
context 
Objective No. of studies included 
(systematic reviews only) 
Clar, Campbell, 
Davidson, & Graham, 
2011 
Health Low and 
middle-income 
countries 
To assess the effects of interventions to 
improve the uptake of research into 
health policies in low and middle-income 
countries and identify the barriers and 
facilitators to the uptake of research 
evidence  
25 intervention and 29 
non-intervention 
studies 
Liverani, Hawkins, & 
Parkhurst, 2013 
Health Global To examine the influence of key features 
of political systems and institutional 
mechanisms on evidence use 
56 studies 
Newman, 2014 Development 
Studies 
Particular focus 
on low and 
middle-income 
countries 
To examine the evidence relating to 
whether research has positive impacts on 
socioeconomic development 
N/A 
Oliver, Innvar, Lorenc, 
Woodman, & Thomas, 
2014 
EIPM (multi-
disciplinary) 
23% of studies 
from low and 
middle-income 
countries 
Update of existing systematic review, to 
identify new barriers of and facilitators to 
the use of evidence by policymakers 
145 studies 
Orton, Lloyd-Williams, 
Taylor-Robinson, 
O’Flaherty, & 
Capewell, 2011 
Health High-income 
countries 
To synthesise empirical evidence on the 
use of research evidence by public health 
decision makers in settings with universal 
health care systems 
18 studies 
Wallace et al., 2012 EIPM (multi-
disciplinary) 
Mainly high-
income 
countries 
To review facilitators of evidence uptake 
by decision makers from systematic 
reviews and meta-analyses  
15 studies 
 
The factors most strongly supported in this secondary evidence are presented in Table 4 below. This is not 
intended as a meta-synthesis of this evidence, but simply a summary and signpost to the most frequently 
mentioned barriers and enabling factors referenced in the literature. For comprehensive and systematic 
summaries of this evidence, readers are encouraged to refer to the papers in Table 3 directly. 
 
Table 4. Commonly cited barriers to and enablers of evidence use in policy making 
Barriers Enablers 
Limited channels exist for policy makers and researchers 
to interact; there is a ‘gulf’ between researchers and 
decision makers (Orton et al. 2011); there are problems 
with engagement, collaboration or communication 
between stakeholders or there is inadequate 
dissemination (Clar et al. 2011)  
Trust, interaction and collaboration between researchers 
and policy makers. (Clar et al. 2011; Oliver et al. 2014; 
Orton et al. 2011). Research is presented clearly and 
presented through tailored dissemination efforts 
(Newman 2014). Interactive approaches and 
partnerships, knowledge brokering and exchange 
(Liverani et al. 2013) 
Research is not relevant for decision making, clear, 
presented in an appropriate format, or reliable. (Oliver, 
Innvar, et al. 2014; Orton et al. 2011) 
Research is clear, relevant for decision making and 
reliable. (Oliver, Innvar, et al. 2014; Wallace et al. 2012)
<<<PAGE=39>>>
www.itad.com/knowledge-and-resources/bcure 39 
 
 
Research is not available or accessible to decision makers. 
(Oliver, Innvar, et al. 2014) 
Research is available and accessible to decision makers. 
(Oliver, Innvar, et al. 2014) 
Organisational systems and support structures do not 
encourage use of research evidence in decision making 
(Newman 2014; Oliver, Innvar, et al. 2014)  
Organisational processes and systems encourage or 
enforce decision makers to consider and apply evidence. 
(Newman 2014; Orton et al. 2011) 
Lack of time and opportunity to use research (this is also 
an organisational factor). (Oliver, Innvar, et al. 2014; 
Newman 2014) 
Charismatic leadership, high-level or local champions, 
commitment and support (Clar et al. 2011) 
Low capacity to understand and use research evidence. 
Evidence suggests that although capacity gaps may be 
more extreme in low-income contexts they exist in high-
income contexts too. (Newman 2014; Orton et al. 2011; 
Oliver, Innvar, et al. 2014) 
 
Lack of resources, funding and investment in EIPM 
processes (Clar et al. 2011) 
 
High staff turnover undermines systematic use of 
evidence (Clar et al. 2011; Liverani et al. 2013) 
 
Institutional barriers to use of research evidence, e.g. 
relating to the nature of political systems and the political 
nature of specific issues (Newman 2014; Liverani et al. 
2013) 
 
 
In summary, the most frequently cited barriers are: poor engagement between researchers and policy makers 
and poor communication of research; an absence of supportive organisational systems and incentives for 
decision makers to use evidence (including a lack of time to read and use research); and a lack of capacity 
among decision makers to access, apply and appraise research. Less frequently referenced barriers include: 
insufficient funding and investment in EIPM, high staff turnover undermining systematic use of evidence, and 
institutional barriers such as the nature of political systems and priorities. 
 
The synthesis papers find that evidence use is facilitated by: positive and collaborative links between 
researchers and policy makers; ensuring relevant research is produced and made accessible to decision 
makers; and supportive organisational systems. One review also suggests the importance of local ‘champions’ 
of evidence use. 
 
Limitations of the evidence base on barriers and facilitators, and implications for this review 
Although only half of the studies in Table 3 explicitly focused on health, in practice the majority of the 
evidence discussed in the reviews derives from the health field; which implies the need for caution when 
thinking about how these barriers and enabling factors may apply to other policy areas. 
 
A more serious limitation was flagged by the authors of one of systematic reviews cited above (Oliver, Innvar, 
et al. 2014); who found that most studies examining barriers to and facilitators of evidence use focused on 
single elements of the policy making process, rarely considering the realities of the policy process as a whole or 
paying attention to policy makers’ priorities (Oliver, Lorenc, et al. 2014). Similarly, another systematic review 
examining the political and institutional influences on the use of evidence in public health policy emphasised 
the dearth of research in this area, finding only six studies that explicitly engaged with political theories or 
concepts (Liverani et al. 2013). Oliver et al. felt that, because most research in this area is ultimately 
conducted in order to find ways to increase research uptake, this ‘skews the debate by focusing on 
exceptional cases of research use in policy making, rather than the normal discharging of statutory business’.
<<<PAGE=40>>>
www.itad.com/knowledge-and-resources/bcure 40 
 
In addition, most of the evidence summarised in Table 4 is based on the perceptions of stakeholders (usually 
researchers and/or policy makers), gathered through surveys or interviews (Clar et al. 2011; Oliver, Innvar, et 
al. 2014; Newman 2014). Oliver et al. stress the limitations of this perception data – arguing that without 
observation of how evidence is actually used in practice, lists of barriers and enabling factors ‘cannot on their 
own lead us to an improved understanding of the role of evidence in the jigsaw of the policy process’ (Oliver, 
Lorenc, et al. 2014). 
 
These criticisms resonate with the discussion in Section 1.2 of this study, which outlined a range of theories 
suggesting the importance of politics and power in EIPM, and the need to acknowledge the complexity and 
range of actors involved in policy processes, as well as the mental models and cognitive biases that influence 
evidence interpretation. We therefore decided to focus this section on synthesising some of the growing 
primary evidence examining political, psychological, cultural and institutional factors promoting or constraining 
EIPM in different contexts; an area where evidence has been less frequently synthesised. This moves beyond 
the barriers and enablers in Table 4 above to the types of factors the theoretical evidence discussed in Section 
1 suggests may be crucially important – taking into account theories of power, politics, networks, cognitive 
processes and complexity. 
 
Nature and limitations of the evidence discussed in this section 
 
This section synthesises evidence from 22 primary non-intervention studies, five theoretical or conceptual 
papers and a number of secondary reviews (on top of the reviews discussed above), detailed in Table 5 and 
Table 6 below. It also draws in a more limited way on four primary intervention studies, which are presented 
in more detail in Section 3 (Dobbins, Robeson, et al. 2009; Nutley et al. 2013; Peirson et al. 2012; Yost et al. 
2014). 
 
It is outside the scope of this review to provide a full or systematic synthesis of the broad evidence base 
relating to the political, psychological, cultural and institutional factors influencing evidence use in policy 
processes. Rather, this section presents some of the main themes from the literature located through our 
search strategy, in relation to the BCURE Theory of Change. The BCURE evaluation team will interrogate these 
factors further through primary research, to examine whether and how far they influence the success of 
BCURE interventions in different contexts. 
 
Table 5. Summary of primary non-intervention studies discussed in Section 2 
Source Field Geographical context Research methods 
Abeysinghe, 2012 Development Studies Global (World Health 
Organisation) 
Case study: document review 
Armstrong et al. 
2013 
Public Administration Canada Case study of design process for EIPM intervention  
Broadbent, 2012 Development Studies Ghana, Uganda, 
Zambia, Sierra Leone 
Case study: media review, literature review, semi-
structured interviews with around 100 participants 
DFID, 2013 Development studies UK Survey with 552 DFID staff members and focus group 
discussions  
El-Jardali et al. 2014 Development studies Lebanon Case study: media review, key informant interviews 
and document review 
Flitcroft et al. 2011 Health Australia Case study: document analysis and key informant 
interviews 
Hallsworth & Rutter, 
2011 and Hallsworth 
et al. 2011 
Public Administration UK 70 interviews, survey, and analysis of 60 policy 
evaluations
<<<PAGE=41>>>
www.itad.com/knowledge-and-resources/bcure 41 
 
Haynes et al., 2011 Health Australia Interviews with 32 civil servants, ministers and 
advisers 
Hufen & Koppenjan, 
2014 
Public Administration Netherlands Three case studies: literature review and 51 interviews 
Hunsmann, 2012 Health Tanzania 92 interviews with AIDS policymakers and 
observations of national policy meetings 
ICAI 2014 Development studies UK (DFID) Document review; analysis of DFID staff surveys; semi-
structured interviews and FGDs with 92 individuals 
Jones & Pellini, 2009 Development Studies Nepal, Peru, Serbia Synthesis of three case studies: literature review and 
key informant interviews 
Jung & Nutley, 2008 EIPM UK Case study: documentary analysis and seven 
stakeholder interviews 
N. Jones, 2011 Development studies Seven countries in East 
Africa and Southeast 
Asia 
Surveys, in-depth interviews and focus group 
discussions 
Pellini et al., 2013 Development Studies Philippines Case studies - Policy review, focus group discussion, 
semi-structured interviews 
Porter & Feinstein, 
2013 
Development studies Ethiopia, Rwanda, 
Malawi, Zambia, Ghana 
Case studies: desk review, semi-structured interviews 
with 77 agencies 
Ritter, 2009 Health Australia 31 key informant interviews 
Smith & Joyce, 2012 EIPM UK  Two case studies: semi-structured interviews with 84 
informants 
Sumner & Harpham, 
2008 
Development Studies India, Vietnam Comparative case studies 
Trostle et al. 1999 Health Mexico Interviews with 67 researchers and policy makers 
Waldman, 2014 Development Studies Afghanistan, Nepal, 
Sierra Leone 
52 in-depth interviews and field visits 
 
Table 6. Summary of secondary and theoretical literature discussed in Section 2 
Source Field Geographical context Type of evidence 
Perkin & Court, 2005 Development Studies Global, particularly lower-income 
contexts 
Other secondary review 
Pollard & Court, 2005 Development Studies Global, including lower-income contexts Other secondary review 
World Bank, 2015 Development Studies Global, including lower-income contexts Other secondary review 
Walter et al. 2005 EIPM Global, including lower-income contexts Systematic Review 
Beck et al. 2005 Public Administration UK Theoretical/ conceptual 
Du Toit, 2012 Public Administration South Africa Theoretical/ conceptual 
H. Jones, 2009 Development Studies Global Theoretical/ conceptual 
N. Jones et al. 2009 Development Studies Global Theoretical/ conceptual 
 
The evidence in this section is analysed according to the four levels of capacity change in the BCURE Theory of 
Change and discussed in Section 1.3.1: individual, interpersonal, organisational and institutional. 
 
2.1. Individual-level factors affecting EIPM 
Individual-level factors refer to individuals’ skills, knowledge, motivation, attitudes, commitment, values and 
personal incentives that affect how they use evidence in decision making. Section 1.2.1 discussed political 
theories relating to ‘discourse’, which emphasise that knowledge in the form of ‘rules of thumb’, logic or 
common sense in a society can shape what decision makers can understand or articulate, and therefore the 
decisions they make. It also introduced ideas from psychological literature, including confirmation bias and 
mental models, which affect how people understand and interpret evidence. In line with these theories, this 
review found several studies from lower- and higher-income contexts suggesting that individual beliefs,
<<<PAGE=42>>>
www.itad.com/knowledge-and-resources/bcure 42 
 
attitudes and motivations to use evidence are connected to pre-existing beliefs, and to the norms and values 
that prevail within organisations or societies. 
Evidence may be ignored or side-lined if it counters past experience – particularly if an issue is hotly debated. 
Several studies examined for this review found that policy makers were more likely to trust research that 
confirmed a policy maker’s pre-existing opinions or experiences, including among DFID advisers in Afghanistan 
(Waldman 2014). This is sometimes known as ‘path dependency’, as described in an observational study of 
the management of the 2009 H1N1 pandemic by the WHO. The WHO emphasised vaccines as a protective 
measure based on its historical achievements with vaccines, which had given rise to a particular ‘discourse’ 
within the organisation in which ‘it was taken for granted that vaccines would provide the most effective 
control measure’. This belief did not take into account contemporary research suggesting that other health 
measures were likely to have greater efficacy. The study suggested that this outcome was partly a result of 
the inherent scientific uncertainty surrounding the case of H1N1, meaning that the situation was open to 
multiple interpretations (Abeysinghe 2012). 
As well as past experience affecting the cognitive processing of 
evidence, deeply held values and beliefs may affect the extent to 
which evidence is considered in a rational, deliberative way. Ten 
studies considered in a systematic review suggested that entrenched 
values and beliefs about emotive topics (including breastfeeding in 
the US, male circumcision in Ghana and the rejection of a link 
between HIV and AIDS in South Africa) biased the selection and 
interpretation of evidence in these contexts (Liverani et al. 2013). 
These findings echo the results of the World Bank survey conducted 
as part of the 2015 World Development Review discussed in Section 
1.2.3 above, in which officials were more likely to misinterpret data 
when it related to an issue they held a strong opinion about 
(minimum wage legislation), than when it related to a less emotive 
issue (skin cream) (World Bank 2015b). 
Finally, one observational study of policy makers in Australia found that ‘issue polarisation’ dictates the extent 
to which research or researchers are used technically or politically. Where policy was strongly opposed or 
debated, researchers with ‘impressive rhetorical skills’ and a good overview of their field were used to 
‘persuade ministers, stakeholders and the public during policy agenda setting and formation’. However, once 
overall policy directions had been agreed, researchers were used in a more technical sense to advise on 
intervention design and evaluation (Haynes et al. 2011). 
This evidence links clearly to the psychological theories discussed in Section 1.2.3. These suggest that people 
make sense of the world around them based on their pre-existing mental models, and so are highly subject to 
confirmation bias – the tendency to disregard or disbelieve evidence that does not correspond with existing 
beliefs. 
Beliefs about what counts as ‘good’ evidence can mean that useful knowledge is ignored or discounted. An 
observational study of UK health inequalities policy in the 2000s found that implicit faith in quantitative over 
qualitative data among health policy makers resulted in qualitative work on the social determinants of poor 
health being ignored or discounted in decision making (Smith & Joyce 2012). This corresponded with a greater 
value being attributed to medical expertise than social science expertise – meaning that academics with a 
health background had higher credibility than social scientists. Similarly, a study of the use of knowledge in 
“Deeply held values 
and beliefs may 
affect the extent to 
which evidence is 
considered in a 
rational, deliberative 
way.”
<<<PAGE=43>>>
www.itad.com/knowledge-and-resources/bcure 43 
 
urban resilience policy making in the Philippines found that local knowledge was often discounted in 
situations where it could add value, for example knowledge about when the colour of the river might indicate 
flooding upstream. The authors concluded that ‘while there is potential for community knowledge to become 
inputs to policy, it does not happen due to the perception that these forms of knowledge are not scientific 
enough’. However, a lack of funds and capacity meant that more rigorous localised data needed for disaster 
preparedness was not being collected (Pellini et al. 2013). This finding suggests that promoting narrow 
definitions of evidence or research quality (discussed in Section 1.1) could actually hinder the effective and 
appropriate use of evidence in decision making. 
Where evidence is valued, this can encourage its use as a ‘weapon’ to confer legitimacy on a decision. 
Conversely, where evidence is less valued, this can encourage deliberate attacks of EIPM concepts. Three 
observational studies relating to the UK’s DFID found evidence of ‘tactical’ uses of evidence (discussed in 
Section 1.2.1). An evaluation examining how DFID learns found that staff occasionally faced pressure to 
provide selective evidence to justify decisions: ‘Interviewees (including heads of professional cadres) told us 
that it is common to find evidence to justify a decision, rather than use evidence to arrive at a decision.’ This 
finding is echoed in an internal DFID staff survey, in which around 7.5% of respondents made the same point 
(ICAI 2014; DFID 2013b). Finally, an observational study of DFID advisers in fragile states also found evidence 
that research was required for ‘ammunition’ – a ‘useful weapon’ that could ‘add weight, credibility and 
persuasiveness to support a line on a specific issue, especially when deployed during 11th-hour negotiations’ 
(Waldman 2014). All three studies also emphasise the high value placed on evidence within DFID, suggesting a 
risk that organisational incentives to use evidence in decision making may actually promote its ‘symbolic’ use 
to support pre-existing positions (discussed in Section 1.2.1). 
Another primary study suggests that in some contexts it is not just evidence that can provide legitimacy but 
the idea of evidence. This paper synthesises four observational case studies examining the use of research 
evidence in African policy debates, finding that some actors in Uganda and Ghana used the terminology of 
EIPM to confer legitimacy on their actions. The author finds that ‘although it might not be referenced well, be 
read or indeed even exist, the idea of research and evidence is important, and establishing its role – even if 
this is nominal – does function to pepper the policy debate with a concern for research and evidence’ 
(Broadbent 2012). 
Conversely, Broadbent’s study also found evidence of policy makers in Sierra Leone attacking EIPM language 
and concepts, to ‘win points’ in a debate. This is because in this context evidence and written research were 
negatively associated with foreign actors and ‘Western’ ideas, while orally communicated evidence and ‘local 
knowledge’ were positively associated with concepts of tradition and culture. Non-use of research evidence 
was therefore painted as a defence of national identity. The author argues that this suggests the limitations of 
explaining away non-use of evidence in terms of a ‘lack of capacity’ which can be ‘filled’ – although this is 
certainly part of the problem. Rather, it suggests that there may be strong political incentives to reject EIPM 
ideas (Broadbent 2012). 
Certain evidence findings may be viewed as ‘unacceptable’ in particular contexts and so ignored. Two studies 
provide examples of evidence being viewed as unacceptable for political or financial reasons. For example, 
one observational study of urban resilience policies in the Philippines found that it was not always politically 
possible to act on evidence suggesting which locations were at risk of flooding, implying the need to relocate 
people. ‘Any mayor attempting such would run headlong into a wall of protests and claims of human rights 
violations, or intense lobbying from wealthy landowners and their politicians’ (Pellini et al. 2013). In a similar 
vein, interview respondents in Waldman’s observational study of DFID use of evidence in fragile contexts felt 
there was an ‘overall conservative tendency’ in DFID causing officials to ignore overtly critical research. If
<<<PAGE=44>>>
www.itad.com/knowledge-and-resources/bcure 44 
 
findings suggested that ‘everything you’re doing is wrong’ or recommended an ‘overhaul’ of existing 
programmes they were likely to be resisted. This was linked to the observation that ‘existing commitments are 
hard to abandon and projects are often implemented in partnership with other donor partners’ (Waldman 
2014). Both studies align with the ‘pluralism and opportunism’ paradigm of EIPM discussed in Section 1.2.1 – 
suggesting the messy and opportunistic nature of policy making, and the need to balance the competing 
interests of various groups. They also highlight some of the potential conflicts between different forms of 
evidence relevant to policy decision making discussed in Section 1.1. Research evidence may actively conflict 
with citizen views, or with process knowledge regarding the best way to implement activities. 
A theoretical study from South Africa suggests that the unacceptability of evidence may be manifested in 
deeper and more subtle ways, reflecting the history and culture of a society. The author argues that the 
articulation of the ‘two economies’ paradigm by President Mbeki (which suggested that a section of society 
had been ‘left behind’ economically, despite South Africa’s rapid economic growth) suddenly made certain 
types of evidence acceptable when previously it was not (du Toit 2012). This made it politically acceptable for 
researchers and decision makers to explicitly link poverty to structural aspects of the economy, whereas 
before this was rejected based on the emotive and accepted view that poverty was a legacy of apartheid. This 
new paradigm therefore allowed the reframing and re-evaluation of existing evidence on poverty and 
inequality, informing new poverty interventions including a Community Works Programme. This resonates 
with theories about power and discourse discussed in Section 1.2.1 – which suggest that ideas and concepts 
viewed as ‘common sense’ in a particular society determine what policy makers can understand and 
articulate, and therefore the policy ideas they are likely to adopt. 
Summary of individual-level factors: This review found nine primary observational studies from lower- and 
higher-income contexts which provide evidence that individual beliefs, attitudes and motivations to use 
evidence (and how to use it) are connected to pre-existing beliefs, and to the norms and values that prevail 
within organisations or societies. For example, several studies suggest that evidence may be ignored or side-
lined if it counters past experience – particularly if an issue is hotly debated. Two studies suggest that beliefs 
about what counts as ‘good’ evidence may result in useful knowledge being discounted; and two further 
studies found that certain evidence findings may be viewed as ‘unacceptable’ in particular contexts and so 
ignored. The status of evidence itself also appears important: three studies suggest that where evidence is 
valued, this can encourage its use as a ‘weapon’ to confer legitimacy on a decision; while another study found 
that where evidence is less valued this can lead to deliberate attacks of EIPM concepts for political gain. 
2.2.  Interpersonal factors affecting EIPM 
Interpersonal factors are about the relationships between individuals and groups (for example in an 
organisation or a network), and how these influence evidence use. Much of the literature on interpersonal 
factors derives from literature on research uptake and knowledge transfer. This relates to relationships 
between researchers and policy makers, and the ‘supply side’ factors which make specific research findings 
more or less likely to be acted on by decision makers. This falls outside the scope of this review, which 
focusses instead on the ‘demand side’ factors which help or hinder decision makers from accessing and using 
evidence in policy making processes. However, it is worth considering briefly some of the evidence suggesting 
the importance of promoting researcher-policy maker partnerships. 
Evidence use is influenced by the type and nature of relationships between researchers and policy makers. This 
was one of the main factors highlighted in secondary reviews of the enablers to and barriers of evidence use, 
discussed above and outlined in Table 4. For example, one systematic review of strategies to promote 
evidence-based practice found that formal and informal linkage mechanisms allow partnerships between
<<<PAGE=45>>>
www.itad.com/knowledge-and-resources/bcure 45 
 
researchers and policy makers to adapt and renegotiate research findings within their own contexts, ‘tinker’ 
with research, and engage in collaborative reflection. The possibility of these partnerships are constrained by 
limited time and energy to establish effective working relationships, and differences in culture, goals, 
information needs, power, reward systems, and language between researchers and policy makers (Walter et 
al. 2005). Similarly, an observational study examining how Australian drug policy makers access evidence also 
stressed the importance of personal relationships and trust. Bureaucrats were found to consult small groups 
of trusted experts by phone to provide research information and opinion, in order to get a quick synthesis of 
evidence. In this case, trust was found to be more important than expert knowledge (Ritter 2009). 
Two observational studies from the UK also suggest that evidence use can be influenced by the nature of 
relationships within government organisations. An observational study of UK civil servants and ministers found 
that civil servants were often reluctant to use evidence to challenge ministers, ‘conscious of the need to 
create and maintain a “good relationship”’. The study suggests that this reluctance is partly a result of 
limitations in support structures (systems and processes to enable civil servants to challenge their ministers 
without compromising relationships), without which the easiest way to keep everyone happy is to ‘give the 
minister what they want’ (Hallsworth et al. 2011). This finding was echoed in a recent observational study of 
how DFID learns, in which some interview respondents said they have been told they ‘can’t say that’ about 
particular pieces of fact-based advice because it would be unacceptable higher up the organisation (ICAI 
2014). This resonates with the ‘politics and legitimisation’ model of policy processes discussed in Section 1.2.1 
above, suggesting that institutional-level power affects who is able to participate in decision making, and 
shapes the strategies, beliefs and actions of individuals within it. 
2.3. Organisational factors affecting EIPM 
Organisational factors relate to the systems, policies and procedures, practices, culture and norms within an 
organisation that promote or inhibit evidence use in policy making. Eight primary studies – mainly from high-
income settings – suggest that organisational factors can affect individual motivation to use evidence and 
present barriers to changes in individual behaviour. 
If evidence is promoted or valued within an organisation, this can increase individual motivation for EIPM. One 
observational study found evidence of a ‘distinct culture in DFID that places a premium on keeping up with 
the latest research, in part to maintain credibility amongst colleagues.’ This was found to influence the 
personal interest and motivation of DFID advisers in Afghanistan, Nepal and Sierra Leone to keep up to date 
with academic debates on state-building (Waldman 2014). Four intervention studies, discussed further in 
Section 3.3 below, also found that organisational tools and systems designed to promote EIPM (such as 
guidelines, templates and procedures for incorporating evidence into programme design) can motivate 
individuals to use evidence more in their day-to-day work (Yost et al. 2014; Nutley et al. 2013; Peirson et al. 
2012; Dobbins, Robeson, et al. 2009). More limited intervention evidence suggests that tools may also 
increase the value individuals place on evidence (Yost et al. 2014; Nutley et al. 2013). These findings link to 
the theories discussed in Section 1.3 above which emphasise the multi-dimensional nature of capacity; in 
particular emphasising the interaction between individual skills and motivation to use evidence and 
organisational-level capacity. 
Lack of time to access and appraise research partly reflects an organisation’s ‘culture’ of evidence use. Time was 
one of the main obstacles to evidence use mentioned in the literature, as outlined in Table 4 above. In one 
systematic review, 42 studies from both low- and high-income contexts referenced this barrier (Oliver, Innvar, 
et al. 2014). Some papers suggest that lack of time may link to organisational values and norms around 
evidence use – for example whether individuals are given the permission and space in their working days to
<<<PAGE=46>>>
www.itad.com/knowledge-and-resources/bcure 46 
 
spend time finding and reading research papers. For example, a 
systematic review found two studies (both from the health field) 
reporting that collection and appraisal of research was seen to be 
‘non-work’ among those who needed to take action – implying that 
lacking time to appraise research may be linked to an organisational 
culture that does not prioritise EIPM (Orton et al. 2011). A survey of 
local government policy makers in Australia which also stressed time 
as a barrier similarly found that searching for and reviewing evidence 
was not considered to be a necessary function in organisational 
cultures (Armstrong et al. 2013). 
Hierarchical management of information, organisational silos and poor organisational memory can limit access 
to research and evidence use. A case study from Mexico found that the hierarchical management of 
information within centralised government organisations prevented research from arriving at relevant 
organisational levels, meaning that policy makers found it difficult to access evidence (Trostle et al. 1999). 
Three studies from the UK, Canada and New Zealand discussed in a systematic review found that divisions of 
responsibilities and ‘institutional silos’ can also limit consideration of evidence. For example, job boundaries 
can make it very difficult to engage with ideas beyond a person’s immediate area of responsibility, or consider 
multi-disciplinary evidence and engage in horizontal thinking across different sectors (Liverani et al. 2013). 
Finally, Waldman’s (2014) study of DFID advisers found that high staff turnover and trends of decreasing staff-
to-funding ratios were believed to result in poor institutional memory within DFID, which was believed to 
reduce effective use of evidence. 
Summary of interpersonal and organisational factors: Much of the literature on interpersonal factors falls 
within the ‘supply side’ of EIPM and is not considered in depth in this review. This includes a large amount of 
evidence, summarised in secondary synthesis papers and outlined in Table 4, suggesting that evidence use is 
influenced by the type and nature of relationships between researchers and policy makers. This review also 
found two observational studies from the UK emphasising the importance of relationships and power within 
government organisations in affecting what kinds of evidence are acceptable. 
Eight primary studies and three systematic reviews – mainly from high-income contexts – provide evidence 
suggesting that organisational factors can affect individual motivation to use evidence, or present barriers to 
changes in individual behaviour. For example, if evidence is promoted or valued within an organisation, this 
can increase individual motivation for EIPM, and lack of time to access and appraise research partly reflects an 
organisation’s ‘culture’ of evidence use. Hierarchical management of information, organisational silos and poor 
organisational memory can also limit access to research and evidence use. 
2.4. Institutional factors affecting EIPM 
Institutional factors relate to the wider environment in which individuals and organisations operate, and how 
this affects the use of evidence in decision making. This includes the role of external actors (such as 
international donors and civil society), and the influence of external factors such as crises, global events, 
political and economic change, and donor influence. This study found a large number of studies suggesting 
that institutional factors play an important role in both enabling and constraining evidence use within a wide 
variety of contexts. This evidence has been categorised below in terms of factors relating to non-
governmental actors (including donors, the media and civil society), and the political environment and 
external events. 
 
“Lacking time to 
appraise research may 
be linked to an 
organisational culture 
that does not prioritise 
EIPM.”
<<<PAGE=47>>>
www.itad.com/knowledge-and-resources/bcure 47 
 
Non-governmental actors 
Seven primary studies provide evidence suggesting that international donors can both promote and constrain 
the effective use of evidence in decision making; private sector actors can exert pressure which ‘blocks’ 
evidence-informed decisions, and the media (and the general public) may present a barrier to EIPM. This 
evidence highlights the messy and opportunistic nature of policy processes, and also provides insights into the 
power wielded by various groups working together or against one another to advance their interests through 
the political and tactical use of evidence. This resonates with both the ‘pluralism and opportunism’ and 
‘politics and legitimisation’ models of EIPM discussed in Section 1.2.1. 
International donors may both promote and constrain the effective use of 
evidence in decision making. Some writers argue that the concept of 
EIPM has been promoted or ‘exported’ by the international 
development community into low and middle-income country contexts 
– such that EIPM has become a ‘by-word’ for more scientifically sound 
and ‘better’ policies than those not centred around research evidence 
(Broadbent 2012; du Toit 2012). Donor commitment to EIPM may 
result in more evidence-informed policies being adopted in recipient 
countries; for example, one systematic review highlighted that donor 
priorities may result in the promotion of interventions with strong 
evidence bases. However, the study also suggests that this may result 
in the neglect of local context, needs and capabilities (Liverani et al. 
2013). In addition, Broadbent’s observational study of four African 
countries argues that the promotion of EIPM by the international 
development community has led to the terms ‘research’ and ‘evidence’ 
being ‘brandished with satisfaction, in the near-certainty that an 
argument will be applauded as long as it uses the well-established 
concepts’, even if in fact evidence has not been used or understood at 
all. Still, although this situation is far from ideal, Broadbent argues that 
‘a stated concern for research-based evidence and evidence-based 
policy is better than none at all’ (Broadbent 2012). 
Donor priorities may also act against EIPM, for example as a result of funding pressures. One observational 
study examining HIV policy making in Tanzania describes how, despite interviewees unanimously agreeing on 
the importance of empirical cost-effectiveness data, it played very little role in decisions about HIV policy in 
the late 2000s. One interviewee described how, following the creation of PEPFAR and the Global Fund, 
‘money was literally poured into this country like anything’. As a result, there were no incentives to use cost-
efficiency data, and in some cases low-cost programmes were actually not implemented because 
organisations faced pressures to spend their rapidly increasing budgets quickly. In the absence of an 
environment in which costs mattered, cost-effectiveness data was no longer politically relevant (Hunsmann 
2012). 
Private sector actors can exert pressure which ‘blocks’ evidence-informed decisions. A systematic review 
discussed evidence of financial and corporate interest groups exerting pressure to either take up or ignore 
research findings based on commercial interests, and another study arguing that ‘the lack of pressure from 
organised lobbies in Laos facilitated the use of evidence for health policy on essential medicines’ (Liverani et 
al. 2013). One theoretical paper argues that private sector influence results from a combination of strong 
economic interests among private sector actors and secretive policy making processes (Jones et al. 2009). 
“Donor commitment to 
EIPM may result in 
more evidence-
informed policies being 
adopted in recipient 
countries…but donor 
priorities may also act 
against EIPM, for 
example as a result of 
funding pressures.”
<<<PAGE=48>>>
www.itad.com/knowledge-and-resources/bcure 48 
 
Another primary observational study found that private sector interests can pose a particular risk in post-
conflict countries. For example, in Serbia ‘private sector actors have played a major role in financing political 
parties to support their own interests, and in part account for the very high level of party fragmentation in the 
country’. In Nepal a new range of laws were passed promoting greater transparency and accountability 
immediately after the end of the conflict, but ‘implementation of these laws and awareness thereof remains 
weak – suggesting that economic interests are still retarding governance reforms’ (Jones & Pellini 2009). 
The media (and the general public) may act against EIPM. There is sometimes an assumption in EIPM literature 
that a free media is an important promoter of EIPM, for example through ‘offering platforms for critical 
review of scientific results’ (Hufen & Koppenjan 2014). However, one example from the UK illustrates that the 
media may also act as a barrier to EIPM. This observational study of debates on sex offenders examined the 
influence of a national newspaper (the News of the World) over a policy process. The campaign promoted 
demand from the general public (gathered through opinion polls) for greater openness about the identities of 
sex offenders released from prison and now living in communities. The government responded to media 
pressure to review the policy, drawing on various evidence sources (including research conducted by civil 
society groups which shared a sense of alarm about the idea of community notification). The government 
explored the feasibility of sharing some information on sex offenders with members of the public – ultimately 
drawing on research evidence to support their decision not to adopt the scheme. This is therefore an example 
of the media and the general public calling for a policy that was not evidence-informed (in the sense that 
research did not suggest a positive impact on reoffending rates) (Jung & Nutley 2008). Broadbent’s study of 
African policy debates also illustrated that citizen views in Uganda and Ghana were laden with stereotypes 
and discriminatory attitudes towards sex workers and street hawkers, therefore acting as a barrier to more 
inclusive policies informed by research evidence (Broadbent 2012). This suggests a potential tension between 
high quality research evidence and the role of citizen voice and participation in development processes.  
 
Civil society may play a number of different roles in relation to EIPM, including putting pressure on government 
to use evidence, building momentum behind ideas, and bringing together different forms of knowledge. This 
paper did not delve into the broad literature on civil society and its role in influencing policy, which is likely to 
have significant insights relevant to EIPM. However, it did consider four primary observational studies and 
four secondary and theoretical papers referring to links between civil society and evidence use. 
There are a number of different ways to conceptualise the relationship civil society organisations (CSOs) might 
have with policy making and EIPM. Coston describes eight kinds of CSO-policy relationships – from that of 
‘repression’, through to relationships of ‘rivalry and competition’, to ‘contracting and cooperation’ and finally 
‘complementarity and collaboration’. These range ‘from NGOs being wholly alienated from formal policy 
processes and concentrating on what they can achieve on their own terms, to NGOs whose arguments are so 
closely aligned with those of government that they are simply pushing at an open door’ (Coston 1998, in 
Pollard & Court 2005). In relation to EIPM, this suggests that civil society may produce evidence for their own 
purposes (conducting research, collecting citizen voices, synthesising findings), campaign for policies based on 
evidence, and/or co-produce policies in collaboration with government actors, utilising evidence to a greater 
or lesser degree – the latter role echoing the theories of policy networks discussed in Section 1.2.2. 
 CSOs can put pressure on government actors to acknowledge or release evidence. Broadbent found 
evidence from Zambia in which the government’s refusal to comment on a biotechnology policy, including 
on the subject of research, ‘in effect halted the policy debate’ – presumably a good thing for the 
government, which was facing tensions over the issue. The government was able to do this in part due to 
a lack of demand for evidence on the part of civil society and other actors (Broadbent 2012). Jung &
<<<PAGE=49>>>
www.itad.com/knowledge-and-resources/bcure 49 
 
Nutley's (2008) observational study from the UK emphasises how a civil society organisation played an 
important role in UK debates on sex-offender policy, by conducting academic research into the policy 
option promoted by the media. This was fed into policy debates and ultimately shaped the government’s 
decision not to adopt the media’s preferred policy of community notification. 
 
 CSOs can help build momentum behind ideas. A literature review examining how CSOs use evidence to 
influence policy processes found evidence that CSOs can influence policy through generating a ‘tipping 
point’ – using evidence to build momentum behind an idea, and crystallising evidence as a policy narrative 
to create a window for change. The review emphasises that this requires effective communication of 
evidence, and the use of relevant, appropriate and timely evidence by CSOs (Pollard & Court 2005). Two 
further papers discussed the potential role of CSOs in seeking alliances with international actors around 
particular issues, which can put additional pressure on national governments (Jones 2009; Perkin & Court 
2005).  
 
 CSOs can play a role in bringing together the different forms of knowledge discussed in Section 1.1, 
including citizen views. A literature review found that, through fusing research evidence with ‘political and 
cultural’ knowledge, CSOs can gain legitimacy among both policy makers and local people at the same 
time. For example, ‘an Indonesian CSO, lobbying to reformulate the government’s birth control 
programme into a family welfare programme, deliberately integrated its findings on the effectiveness of 
this approach with passages from the Qu’ran and Hadith. This inflected the proposal with a call to respect 
the interests of the Muslim majority, who had recently been under pressure from Christian, Confucian, 
Hindu and Buddhist groups. Drawing out the political aspect of this evidence made it more attractive for 
the government, because they could act upon it as a statement of support for Muslims.’ This review also 
stresses the potential of CSOs to help policy makers access evidence from the grassroots – citing one 
example from Bolivia when a CSO was able to use the Catholic Church and its widespread grassroots 
presence to conduct dialogue on the Bolivian Poverty Reduction Strategy Paper (PRSP) process (Pollard & 
Court 2005). 
 
 Trust appears to be an important consideration for CSO influence on EIPM. A theoretical paper cites 
evidence suggesting that CSO influence is limited by a low level of policy maker trust in civil society (Jones 
et al. 2009). A secondary review also cites evidence from Indonesia and Cambodia, suggesting that CSOs 
can influence policy makers with evidence-based recommendations in situations where the involvement 
of CSOs in policy making improves the legitimacy of policy (and therefore the legitimacy of MPs). 
However, this is often hampered by mistrust: policy makers concerned that CSOs may be influenced by 
international donors, and CSOs concerned that policy making may be working behind closed doors. The 
review cites evidence from Cambodia, where only 20% of CSOs reported any link with MPs, and MPs see 
CSOs as ‘pessimistic’, ‘donor-driven’, ‘manipulative’ and ‘biased towards the opposition’ (Jones 2011). 
 
 The influence of CSOs in EIPM depends on their position and role in society. A secondary review found that 
the credibility of the evidence used by CSOs is an important predictor of policy influence. ‘CSOs need to 
be adept at adapting the way they use evidence to maintain credibility with local communities and with 
policy makers, combining their tacit and explicit knowledge of a policy context’. However, this review also 
found that ‘overall, the important factor in whether CSOs can use evidence to influence policy is how well 
they are integrated within a policy process’ (Pollard & Court 2005). A theoretical paper points out that 
contracting political space for CSOs in some contexts will have a knock-on effect on CSO influence on 
EIPM – for example, in Zambia, Uganda, Ethiopia and Nicaragua, laws curb the scope of advocacy work 
(Jones et al. 2009).
<<<PAGE=50>>>
www.itad.com/knowledge-and-resources/bcure 50 
 
Two examples from primary studies illustrate this point. A study in the Philippines found limited scope for 
civil society to get involved in the crafting of urban resilience policy. Civil society involvement was largely 
limited to disaster response, a historical role that was ‘institutionalised as a formal routine’, despite civil 
society potential to add value to policy making processes (Pellini et al. 2013). In contrast, another study 
discusses the Energy Bill in the Kenyan Parliament, for which evidence generated by CSOs was fed in to 
workshops with parliamentarians, and legislators were brought together in a CSO-led forum to discuss 
energy issues before the Bill was passed. Jones claimed this resulted in a ‘more comprehensive bill, which 
took into account the interests of local communities’. The role of CSOs here was partly a collaborative 
one, as evidence fed in by CSOs gave parliamentarians a ‘stronger voice to push for legislative reforms’ as 
well as knowledge to critique government policy (Jones 2011). This role clearly depended on a policy 
environment where CSOs were able to produce evidence and access policy makers to communicate it. 
This study also found that links between the media and CSOs was important in facilitating exchange 
between CSOs and legislators. 
The literature on civil society considered for this review did not reference any negative effects of civil society 
influence on the use of evidence in policy processes. However, it seems plausible that civil society is not 
always a force for good in EIPM, given the discussion above on the potentially negative role of international 
donors, the private sector and the media on evidence use in decision making. Given the small number of 
papers it was possible to consider on civil society in the time available for this review, it is unclear whether 
this represents an evidence gap; but this may be an interesting area for further research. 
 The political environment and external events 
Five primary observational studies and a number of secondary and theoretical papers – from high- and low-
income contexts – suggest that institutional factors such as sudden change (e.g. crises or regime changes), 
levels of decentralisation and levels of democracy can create opportunities for or barriers to EIPM. 
Change in the institutional environment – such as crises, regime changes, democratisation and external events – 
can create new opportunities for or new barriers to EIPM. One study argues that crises can create windows of 
opportunity, engendering a new willingness among policy makers to break stalemates or take painful but 
necessary steps. The bigger the crisis, the stronger the opportunity for research to shape underlying 
discourses and values. For example, during regime change in Singapore, ideas associated with the old regime 
were discredited and disorganised, opening space for new attitudes towards knowledge and creating a more 
conductive environment for research use (Jones et al. 2009). Similarly, three studies discussed in a systematic 
review (relating to South Africa and Uruguay) found that the process of democratisation created a new model 
more open to the uptake of research findings, including new appointments of researchers and establishments 
of research institutes (Liverani et al. 2013). 
Opportunities to consider different types of evidence can be opened up by smaller-scale events too. One 
observational study discussed cases from the UK, in which celebrity chef Jamie Oliver’s campaign to improve 
school meals and Ireland’s decision to implement a ban on smoking in public places created opportunities for 
research to influence debates on nutrition in schools and public smoking (Smith & Joyce 2012). 
Pellini’s study of the use of evidence in urban resilience interventions in the Philippines found that the actual 
experience of disaster was a necessary condition for policy action; the mere ‘presence of these threats to 
citizens and their economic interests does not result in concrete policy actions’. The authors suggested that 
there must be opportunities for political gain in order for better, evidence-informed resilience policies to be
<<<PAGE=51>>>
www.itad.com/knowledge-and-resources/bcure 51 
 
created in advance of a crisis; for example, one prominent political figure had managed to create a political 
constituency around disaster preparedness (Pellini et al. 2013). 
These findings link to theories of ‘policy spaces’ and ‘policy streams’ discussed in Section 1.2.1 above, which 
emphasise the importance of ‘windows of opportunity’ in policy making processes which can create moments 
and spaces for evidence to be used. However, crises can also hinder the consideration of evidence. One case 
study examined the implementation of a voluntary health insurance health policy in Lebanon, triggered by the 
sudden abolition of post-retirement medical plans by a major national company, and which left many citizens 
without medical coverage. Despite interview respondents stating they valued evidence, the implementation 
of the resulting policy was ultimately a ‘quick political decision’ that did not take account of available 
evidence. Interview respondents stressed that the extreme pressure to tackle the crisis resulted in a policy 
that was publicly popular despite evidence suggesting it was unworkable. This was enabled by a political 
system that, although democratic, lacked participatory and transparent policy making processes, and allowed 
the government to issue a decree despite the reservations of the Ministry of Finance. An absence of systems 
and procedures for the consideration of evidence in policy processes may have also been a contributory 
factor (El-Jardali et al. 2014). This demonstrates how a lack of institutional capacity can hinder EIPM even 
where individuals have the capacity and motivation to use evidence, adding empirical weight to the multi-
dimensional model of capacity development described in Section 1.3. 
Finally, a study of evidence use in post-conflict environments found that a knowledge gap opened up upon 
regime change, as the technical reputation of intellectuals could not be ‘disentangled from their role in 
previous authoritarian regimes’. Intellectuals associated with governments who presided over the conflict 
(and which were ousted from power) were discredited following the end of conflict in Nepal, Peru and Serbia. 
In Nepal particularly this may have been compounded by an absence of a civil society voice (Jones & Pellini 
2009). 
Levels of organisational and political decentralisation can affect use of evidence in decision making. A 
systematic review found evidence that a concentration of power in centralised systems (e.g. the UK National 
Health Service prior to 1990 reforms) can prevent pluralistic debate, and therefore the need for evidence to 
support competing views. Conversely, in decentralised political systems, there may be more need for research 
as legitimation or ammunition to justify political decisions (Liverani et al. 2013). One observational study of 
the BSE public health crisis in the UK found that, in a centralised system in which government agencies 
controlled expert advice with little public oversight, pressure and expert interest groups were able to shape 
policy decisions and undermine the credible assessment of public health risks (Beck et al. 2005). 
However, an observational study of evidence use in the Philippines described how a culture of evidence use 
did not emerge upon decentralisation, despite legislation being in place to strengthen local government 
capacity as part of the decentralisation process. This was in part due to limited budgets for Local Government 
Units to conduct research, few links between academic institutions and local decision making bodies, and the 
persistence of nationally provided policies – reflecting a history of reliance among local government actors on 
central government data (Pellini et al. 2013). 
Levels of democracy and the role and power of national actors outside central government can affect the use of 
evidence. The studies reviewed for this section do not suggest a clear and obvious link between democracy 
and use of evidence in decision making. For example, one comparative observational study examines 
evidence use in India and Vietnam, finding that the levels of democracy or autocracy were not necessarily a 
key factor in influencing the use of evidence in policy making (Sumner & Harpham 2008). Another study 
examining demand for and supply of evaluation in five sub-Saharan African countries drew a distinction
<<<PAGE=52>>>
www.itad.com/knowledge-and-resources/bcure 52 
 
between development patrimonial states and neopatrimonial states. Development patrimonial states 
(Ethiopia and Rwanda) were characterised by strong centralised leadership with limited scope for the 
influence of external actors. The authors found relatively high demand for evidence, based on incentives to 
achieve developmental outcomes in order to maintain the legitimacy of government. In addition, ministries 
were generally technocratic in nature, with some (albeit limited) capacity to appraise and use evidence. 
Neopatrimonial states (Malawi, Zambia and Ghana) were characterised by patronage-based decision making, 
multiple interest groups competing for influence and power, and more disordered policy processes. This 
provided more diverse entry points for evidence to be used to influence policy processes. However, capacity 
was still weak to manage and understand evaluations (Porter & Feinstein 2013).  
Four studies discussed in a systematic review also pointed to the potential biases that may result from 
‘processes of democratic deliberation’ – including ‘opportunistic use of evidence to delay decision making, to 
legitimate particular policy positions or to discredit opponents in political debates’ (Liverani et al. 2013). For 
example, a qualitative study from Australia found that evidence became more contested around an election 
campaign, amplifying tensions between stakeholders who controlled selection of evidence for policy (experts, 
bureaucrats and advisers). The Health Minister’s advisers developed plans to roll out a national bowel cancer 
screening programme, which ignored much of the evidence gathered in early stages of policy making and 
later proved wholly unrealistic. The authors concluded that, in the search for alternative ideas in the heat of 
an election campaign, adherence to evidence may play a secondary role (Flitcroft et al. 2011). 
Summary of institutional factors: The review discussed 12 primary observational studies and several secondary 
and theoretical studies relating to institutional factors affecting evidence use. Seven primary observational 
studies suggest that non-governmental actors often play an important role in relation to EIPM. International 
donors may both promote and constrain the effective use of evidence in decision making, private sector 
actors can exert pressure which ‘blocks’ evidence-informed decisions, and the media (and the general public) 
may present a barrier to EIPM. Civil society may play a number of different roles in relation to EIPM, including 
putting pressure on government to use evidence, building momentum behind ideas, and bringing together 
different forms of knowledge. Finally, five primary observational studies suggest that institutional factors such 
as sudden change (e.g. crises or regime changes), levels of decentralisation and levels of democracy can all 
create opportunities for or barriers to EIPM. 
2.5. Conclusions and implications for the BCURE evaluation 
This section has examined the factors that prevent decision makers from using evidence, and the factors that 
facilitate evidence use. It began by summarising some of the evidence already synthesised in secondary 
reviews, providing a signpost to the most frequently mentioned barriers and enabling factors referenced in 
the EIPM literature. It then moved on to examine some of the primary evidence specifically relating to 
political, psychological, cultural and institutional factors promoting or constraining EIPM in different contexts; 
an area where evidence has been less frequently synthesised and which takes into account theories of power, 
politics, networks, cognitive processes and complexity discussed in Section 1.2. The main findings are as 
follows: 
 
 Individual beliefs, attitudes and motivations to use evidence (and how to use it) can be connected to pre-
existing beliefs, and to the norms and values that prevail within organisations or societies. Evidence may 
be ignored or side-lined if it counters past experience, and beliefs about what counts as ‘good’ evidence 
may result in useful knowledge being discounted – echoing cognitive theories discussed in Section 1.2.3 
which suggest that people make sense of the world using pre-existing mental models, and so are highly 
subject to confirmation bias. Some studies also found that evidence findings may be viewed as
<<<PAGE=53>>>
www.itad.com/knowledge-and-resources/bcure 53 
 
‘unacceptable’ in particular policy contexts and so ignored. This links to the ‘pluralism and opportunism’ 
paradigm of EIPM discussed in Section 1.2.1 – suggesting the messy and opportunistic nature of policy 
making and the role of evidence within it. 
 
 Interpersonal relationships and power within government organisations can affect how (and what types 
of) evidence is acknowledged and communicated. This resonates with the ‘politics and legitimisation’ 
model of policy processes discussed in Section 1.2.1, suggesting that institutional-level power affects who 
is able to participate in decision making, and shapes the strategies, beliefs and actions of individuals 
within it. Two studies also suggest that evidence use may also be influenced by the type and nature of 
relationships between researchers and policy makers. 
 
 Organisational factors can affect individual motivation or ability to use evidence in their work. Individual 
motivation for EIPM may be increased if evidence is promoted or valued within an organisation – 
although conversely, some studies also suggested that organisational incentives to use evidence in 
decision making may actually promote its ‘symbolic’ use to support pre-existing positions (discussed in 
Section 1.2.1). Lack of time to access and appraise research may reflect an organisation’s ‘culture’ of 
evidence use; and hierarchical management of information, organisational silos and poor organisational 
memory can limit access to research and evidence use. The importance of organisational factors on 
individual decisions to use evidence resonate with the theories discussed in Section 1.3 on the multi-
dimensional nature of capacity; in particular emphasising the interaction between individual skills and 
motivation to use evidence and organisational-level capacity. 
 
 A wide range of institutional factors also prevent or facilitate EIPM. The literature provides insights into 
the power wielded by various groups working together or against one another to advance their interests 
through the political and tactical use of evidence, resonating with both the ‘pluralism and opportunism’ 
and ‘politics and legitimisation’ models of EIPM discussed in Section 1.2.1. International donors may both 
promote and constrain the effective use of evidence in decision making depending on their own priorities, 
private sector actors can exert pressure which ‘blocks’ evidence-informed decisions, and the media (and 
the general public) may present a barrier to as well as promotor of EIPM. Civil society can put pressure on 
government actors to use evidence, build momentum behind ideas, and bring together the different 
forms of knowledge relevant to policy decision making discussed in Section 1.1. The influence of CSOs on 
EIPM depends on their position and role in society. Institutional factors such as sudden change (e.g. crises 
or regime changes), levels of decentralisation and levels of democracy can also generate opportunities for 
or barriers to EIPM. 
Implications for the BCURE evaluation 
 
These findings underscore the importance of examining the specific context within which each BCURE 
intervention works. In order to understand the factors that might enable or prevent change as a result of 
BCURE activities, the evaluation team will need to investigate these contextual factors – for example looking 
at how individual beliefs, attitudes and motivations link to organisational features and social norms; and 
thinking about the wider institutional context, including the role of international donors, private sector actors, 
the media and civil society, and the influence of historical events and levels of decentralisation and democracy 
on the ways in which evidence is used and understood. The influence of these factors on the success of 
BCURE programme interventions will be explicitly considered as part of the evaluation.
<<<PAGE=54>>>
www.itad.com/knowledge-and-resources/bcure 54 
 
The findings in this section also highlight the interrelationships between individual, organisational and 
institutional factors – for example the influence of organisational systems on individual values, or the ways in 
which ideas about evidence in wider society shape how it is talked about and the types of knowledge 
considered important. Echoing findings in Section 1.3.1, this suggests the value of examining capacity for EIPM 
as a system. The empirical evidence discussed in this section also reiterates the overall implications of Section 
1, suggesting the value of incorporating theoretical insights on power, politics, networks and complexity into 
the study of BCURE interventions, and considering capacity change as a multi-dimensional issue.
<<<PAGE=55>>>
www.itad.com/knowledge-and-resources/bcure 55 
 
3. What is the evidence on how to build capacity for evidence-informed policy 
making? 
Overview 
This section examines the evidence on how to build capacity among decision makers for EIPM – looking at 
what works, for whom, in what circumstances, and why. Following the principles of realist synthesis, it 
discusses the mechanisms through which EIPM interventions lead to particular outcomes in different contexts, 
along with the features of interventions that either enable or hinder change. 
This section discusses 15 primary intervention studies, all of which describe interventions aiming to develop 
capacity for evidence use or public sector decision making in health contexts. Around half of the studies relate 
to lower and middle-income countries. The majority have observational designs, and a rapid quality 
assessment deemed them all medium-high quality. Most primary intervention studies did not contain explicit 
information on mechanisms – and so identifying these involved reading between the lines, looking for 
common themes and making links to the literature discussed in Sections 1 and 2. This section also draws on 
relevant evidence from a number of non-intervention primary studies and secondary reviews. 
Despite this small evidence base, useful lessons can be distilled from these studies on how and why different 
interventions may have resulted in (or not resulted in) change; and the contextual and intervention factors 
that helped or hinder programme success. The evidence (and its gaps) also has implications for the BCURE 
evaluation, and more broadly for the study of capacity development for EIPM. The findings are summarised in 
boxes throughout this section, and in a simplified form in the conclusion. Three of the main insights are as 
follows: 
1. A number of capacity development interventions at individual, interpersonal and organisational levels 
may work through promoting self-efficacy: improving participants’ beliefs (or confidence) in their 
capability to perform a certain task or handle a particular situation. Training, knowledge brokers, and tools 
and systems may all improve self-efficacy in different ways. However, the concept of self-efficacy is just 
one way of understanding how learning happens, suggesting the potential merit of bringing learning 
theory (discussed in Section 1.3.2) more explicitly into capacity development interventions. 
 
2. Although only a small number of studies discussed interpersonal-level interventions, these pointed 
towards a number of different mechanisms. Knowledge brokers and champions may promote EIPM 
through ‘cheerleading’, through being ‘transformational leaders’, or ‘network facilitators’, or through 
exhibiting role-modelling behaviours and thus promoting ‘social learning’. One study suggests that 
networks may enable ‘social processing’ – in which beliefs within a group shift towards a consensus – and 
this may lead away from EIPM as well as towards it. These different mechanisms may respond in different 
ways to particular intervention strategies and contextual conditions; suggesting the importance of 
unpicking what exactly it is a knowledge broker, champion or network is expected to do. 
 
3. A small number of studies suggest that organisational tools and systems may work through facilitating 
behaviour change (making a person’s job easier), or reinforcing it (through for example rewards, audit or 
feedback). One study suggests that EIPM tools may also lead to change by increasing the value staff place 
on evidence, through convincing them of the benefits that data can bring to decision making. A virtuous 
circle may emerge, in which increased use of evidence leads to greater demand based on an appreciation 
of its value.
<<<PAGE=56>>>
www.itad.com/knowledge-and-resources/bcure 56 
 
This section adopts a realist approach to examine the evidence on how to build capacity among decision 
makers for EIPM – looking at what works, for whom, in what circumstances, and why (Pawson & Tilley 1997). 
It synthesises evidence from primary intervention studies aiming to improve capacity for evidence use or 
decision making. 
Purpose and structure of this section 
This section describes the outcomes of capacity-building interventions, the varied mechanisms through which 
interventions appeared to lead to these outcomes in different contexts, and the features of the interventions 
that either enable or hinder change. This evidence will be used by the evaluation team to develop ‘context-
mechanism-outcome configurations’ – theories to help explain how and why specific BCURE interventions 
(such as training, mentoring and organisational systems development) might lead to change in different 
contexts. These configurations will be empirically tested through the realist evaluation of the programme, in 
order to draw conclusions on works to build capacity for EIPM, for whom, in what circumstances and why. 
The evidence in this section is categorised in line with the BCURE Theory of Change; examining in turn 
capacity development interventions targeting individual-level, interpersonal-level, organisational-level and 
institutional-level change (as described in Section 1.3). Several interventions were multifaceted, aiming at 
more than one of these levels, and so are discussed across several sections. 
Nature and limitations of the evidence discussed in this section 
This section draws on 15 primary intervention studies as well as relevant evidence from non-intervention 
primary studies and secondary reviews. The evidence base discussed in each sub-section is summarised in 
Tables 7-10. 
The discussion in this section has a number of limitations: 
1. The findings are based on a limited number of primary intervention studies, most of which relate to 
training interventions. Some of the studies found were not included, either because they did not 
provide information about how and why interventions led to change or because they were deemed 
insufficient quality. The review originally intended to look beyond the literature specific to EIPM, to 
examine evidence from wider capacity development interventions that could provide relevant 
insights. However, in practice this was limited by time. This section therefore provides a detailed but 
partial overview of the primary intervention evidence base. 
 
2. Most of the intervention studies discussed in this section relate to training interventions. The 
evidence on other forms of capacity building is limited – including evidence on networks, 
organisational systems, knowledge brokers and champions. The findings relating to these 
interventions are therefore based on a very small number of studies. 
 
3. All of the primary intervention studies incorporated in this section relate to interventions in the health 
field. This raises a risk that the findings may not be generalisable to other fields, which may have 
smaller, more diverse and more contested evidence bases; although the inclusion of non-intervention 
evidence and secondary literature from other fields mitigates this risk somewhat. In addition, 
although around half of the studies relate to lower and middle-income contexts, many of the studies 
with the richest information on mechanisms and contextual/intervention features derive from higher-
income countries. This suggests the need for caution in applying the findings to lower-income 
contexts.
<<<PAGE=57>>>
www.itad.com/knowledge-and-resources/bcure 57 
 
 
4. Most studies provided significant detail on the outcomes of interventions, and discussed (in greater or 
lesser detail) the features of the interventions that appeared to influence results. However, very few 
of the studies explicitly discussed the mechanisms through which interventions resulted in change, or 
considered the contexts of the intervention in any great detail. Identifying mechanisms and relevant 
contextual factors therefore involved reading between the lines, looking for common themes and 
making links to the theoretical literature discussed in Section 1, to tease out how and why 
interventions seemed to work (or not work) (Pawson 2006b). As the studies analysed below were not 
written for this purpose, there is some risk that certain mechanisms will have been misinterpreted, or 
certain contextual or intervention factors overplayed or overlooked. In addition, few studies provide 
enough detail to derive any insights about who benefits or fails to benefit from specific capacity 
development interventions. 
 
5. The intervention studies considered below often draw (implicitly or explicitly) on rational and linear 
models of policy change. Few studies made reference to more recent theories of EIPM discussed in 
Section 1.2, which emphasise the messy, contested and political nature of evidence use in policy 
making, and which have broadly superseded rational and liner conceptions of evidence use in policy 
within the theoretical literature. This results in some disconnect between the conceptual discussion in 
Section 1 and the synthesis of primary studies provided below. 
3.1. Individual change 
Individual-level change includes individuals’ development of skills and knowledge, as well as improvements in 
motivation, attitudes, commitment, values and personal incentives that affect individual behaviour. This 
section considers evidence from 11 primary intervention studies and one secondary review, summarised in 
Table 7 below. 
Table 7. Summary of evidence relating to individual-level interventions 
Source Field Geographical 
context 
Type of evidence Research approach and methods Quality 
(/12) 8 
Jacobs et al. 
2014 
Health United States Primary 
intervention 
study 
Quasi-
experimental 
Survey: pre- and post- workshop 
assessments with control group 
11 
Matovu et 
al. 2013 
Health Uganda Primary 
intervention 
study 
Observational Quarterly self-assessment and 
evaluation forms completed by 
participants; mentors' assessments 
6 
Pappaioano
u et al. 
2003 
Develo
pment 
Studies 
Bolivia, 
Cameroon, 
Mexico, 
Philippines 
Primary 
intervention 
study 
Observational Case study of DDM project 
implementation 
6 
Peirson, 
Ciliska, 
Dobbins, & 
Mowat, 
2012 
Health Canada Primary 
intervention 
study 
Observational Case study: 27 semi-structured 
interviews and FGDs with 70 staff 
members; and document review 
12 
Pettman et 
al. 2013 
Health Australia Primary 
intervention 
study 
Observational Pre- and post- training surveys plus six 
month follow-up 
8 
Rolle et al. 
2011 
Health Ethiopia Primary 
intervention 
study 
Observational Post-module surveys and end-of-course 
survey plus 1 FGD with 10 trainees 
12 
 
8 See Methodology for details on the quality assessments of primary intervention studies.
<<<PAGE=58>>>
www.itad.com/knowledge-and-resources/bcure 58 
 
Rowe et al. 
2010 
Health Liberia Primary 
intervention 
study 
Observational Post-course survey 10 
Tomatis et 
al. 2011  
Health Peru Primary 
intervention 
study 
Quasi-
experimental 
Pre- and post-course surveys with 220 
course participants 
10 
Uneke et al. 
2012b 
Health Nigeria Primary 
intervention 
study 
Observational Pre- and post-workshop survey and 
focus group discussion 
10 
Waqa et al. 
2013 
Health Fiji Primary 
intervention 
study 
Observational Process evaluation involving pre-training 
semi-structured interviews and survey 
to assess baseline capacity. Records of 
training outcomes (e.g. production of 
policy briefs, use of templates) 
9 
C. J. J. 
Uneke et 
al., 2011 
Health Nigeria Primary non-
intervention 
study 
Observational Survey of self-reported EIPM capacities  
Walter, 
Nutley, & 
Davies, 
2005 
EIPM Global (mainly 
developed 
countries) 
Secondary 
review 
Systematic 
Review 
  
 
Most of the studies discussed in this section report on the outcomes of training interventions, aiming to 
improving the capacity of public sector workers to use evidence in developing and implementing policy. The 
majority of studies derive from lower-income contexts. 
Training was generally formal, longer term, and targeted at government officials to increase knowledge and 
technical or soft skills. 
Nature of training: Interventions mainly consisted of standalone training courses, mostly in low-income 
contexts. Some interventions combined training with other support, such as mentoring (Waqa et al. 2013) or 
the employment of a ‘knowledge broker’ (Peirson et al. 2012). 
Purpose of training: Some courses specifically aimed to increase EIPM (e.g. Pettman et al. 2013; Tomatis et al. 
2011). For example, one course involved training in how to ask ‘answerable questions’, find evidence to 
answer these questions, assess the trustworthiness of evidence, integrate evidence with expertise and other 
factors, and evaluate activities to generate evidence to feed back into the process (Pettman et al. 2013). 
Other courses aimed to build wider skills of relevance to EIPM, including management and problem-solving 
skills (Rowe et al. 2010), analytical skills for health sector management (Rolle et al. 2011) and ICT skills (C. J. 
Uneke et al. 2011). 
Length of training: Most training courses were longer term. Three were short, lasting between 1 and 5 days, 
and delivered in either a single burst or in a modular format over time (Jacobs et al. 2014; Tomatis et al. 2011; 
Pettman et al. 2013). The remaining courses were either integrated into longer-term multifaceted capacity 
development interventions (Peirson et al. 2012; Waqa et al. 2013), or conducted in intensive bursts of 1–2 
weeks over several months (Rolle et al. 2011; Pappaioanou et al. 2003; Matovu et al. 2013). 
Target groups: Participants in the training courses were mainly government officials, usually health officials, 
working at a national or sub-national level. Some interventions also targeted in-service health professionals 
and NGO workers (e.g. Matovu et al. 2013). There was very little discussion within the studies on whether 
some participants benefitted more than others from the training, and if so why.
<<<PAGE=59>>>
www.itad.com/knowledge-and-resources/bcure 59 
 
Most studies provided evidence from self-assessments to suggest EIPM-related skills had improved, which can 
be understood through the mechanism of ‘self-efficacy’. 
The majority of the studies provided evidence (mainly from pre- and post-course surveys, and in some cases 
only post-course surveys) that participants felt their EIPM-related skills had improved (Pettman et al. 2013; 
Rowe et al. 2010; Rolle et al. 2011; Tomatis et al. 2011; C. J. Uneke et al. 2011). Surveys mainly measured 
improvements in skills or knowledge; although a small number assessed broader capacity change (as 
discussed in Section 1.3.1) such as attitudes (Pettman et al. 2013) or ‘competencies’ (C. J. J. Uneke et al. 2011; 
Jacobs et al. 2014). However, self-assessments are not necessarily the most reliable measure, as individuals 
may over-estimate improvements in capacity (known as ‘self-esteem bias’) (Deans & Ademokun 2011) and 
most studies did not triangulate self-assessments with other forms of skills assessments. One study from the 
US reported perceived increases in EIPM within participants’ wider organisations, which may be subject to the 
same bias (Jacobs et al. 2014). Only a minority of studies provided more objective measures of skill increase or 
behaviour change – such as the production of policy briefs (Waqa et al. 2013); improvements in test scores and 
observed data-based recommendations/conclusions (Pappaioanou et al. 2003), or enhanced organisational use 
of evidence as demonstrated through the development of EIPM processes and procedures (Peirson et al. 
2012). 
None of the studies considered in this section explicitly link training approaches to any formal models of 
learning and individual skills development, such as those 
discussed in Section 1.3.2. It was the therefore not clear how 
(through which mechanisms) the courses expected to result in 
individual learning, and which (if any) theories of adult learning 
they were based on. However, several studies imply that 
training increases participants’ confidence in their ability to 
apply EIPM-related skills, which can be understood in terms of 
the concept of self-efficacy discussed in Section 1.3.2 (e.g. 
Jacobs et al. 2014; Pappaioanou et al. 2003; Rolle et al. 2011). 
Self-efficacy relates to a person’s beliefs about their capability 
to perform a particular task or handle a particular situation. For 
example, one training course implemented in Bolivia, Cameroon, Mexico and the Philippines resulted in teams 
reporting a ‘feeling of empowerment’ from the training, in that it enabled them to use data to identify and 
solve important health problems in their communities (Pappaioanou et al. 2003). As discussed in Section 1 
above, self-efficacy is one concept within a wide range of adult learning theories, and is therefore certainly 
not the only way to conceptualise the how training leads to behaviour change. However, there is little explicit 
detail in the primary studies examined here to provide an insight into what other mechanisms might be at 
work. 
Some studies suggest that training may also contribute to interpersonal and organisational change. 
One study suggested that the course played a role in ‘paving the way’ to ‘discuss, promote and facilitate 
integration’ of EIPM concepts in participants’ day-to-day work – not only through developing skills, but ‘raising 
awareness among agency leadership’ which meant leaders become more supportive of new efforts to integrate 
EIPM into programme activities (Jacobs et al. 2014). Similarly, a study from Canada found that training 
(combined with mentoring and knowledge brokering interventions) resulted in staff becoming more 
comfortable and familiar with EIPM, as the ‘language’ of EIPM permeated throughout the organisation 
(Peirson et al. 2012). The latter study also found that training helped to strengthen internal relationships 
between staff, which links to findings in Section 3.2 around interpersonal mechanisms promoting EIPM. These 
“Self-efficacy relates to 
a person’s beliefs about 
their capability to 
perform a particular 
task or handle a 
particular situation.”
<<<PAGE=60>>>
www.itad.com/knowledge-and-resources/bcure 60 
 
interlinkages between individual-level training and change at interpersonal and organisational levels suggest 
the relevance of considering capacity development as multi-dimensional, as discussed in Section 1.3.1. 
Self-efficacy may be enabled in some contexts through use of practical and work-based projects, and linking 
content directly to participants’ professional roles. 
The studies contain limited evidence on how the style of teaching influenced course outcomes. However, 
several studies used some combination of classroom-based training and on-site projects which were linked to 
self-reported skill increases, such as incorporating projects in which participants were required to use their 
new skills to implement a work-based project or develop action plans and budgets (Rowe et al. 2010; 
Pappaioanou et al. 2003; Rolle et al. 2011; Matovu et al. 2013). For example, participants in a leadership 
course in Ethiopia consistently reported that the course was increasing their skills and confidence because the 
content was directly applicable to their work (Rolle et al. 2011). Another in Uganda was structured so 
managers could return to their institutions between modules to apply learning, which was viewed as an 
important feature of the approach (Matovu et al. 2013). 
This provides some evidence to suggest that this style of training, along with the direct applicability of training 
content to participants’ roles, helped enable the mechanism of self-efficacy. This approach may also be 
associated with longer-term training, although there is insufficient evidence to judge how the length of a 
training course affects outcomes. Although an applied model of classroom-based training plus work-based 
projects may not be possible for shorter courses, one study of a short course on EIPM in Australia similarly 
emphasised the importance of tailoring the course to policy decision making contexts, which was associated 
with increased post-course ratings of self-reported practice, knowledge, confidence and attitudes as the 
course content shifted over time (Pettman et al. 2013). Similarly, evidence from Fiji suggests the importance 
of ensuring course participants will have the opportunity to apply EIPM skills as part of their roles – in this case 
it was found that more senior participants were more likely to have the ability to use their skills in an 
organisational setting (Waqa et al. 2013). Another study suggested that training was successful due in part to 
a locally recognised institutional need for capacity development; the decentralisation of health systems had 
opened up an ‘immediate need to strengthen capacity’ at sub-national levels, which was met by the very 
hands-on training programme (Pappaioanou et al. 2003). These findings all resonate with the learning theory 
of andragogy discussed in Section 1.3.2, which suggests that adults learn best when they can put their 
learning into practice. 
A cross-sector review of interventions aiming to promote evidence-based practice also found that passive 
approaches and interventions of one day or less were unlikely to result in improved skills and knowledge for 
EIPM. These findings link to one of the core principles of adult learning identified in Section 1.3.2 – that adults 
need extended contact in order to assimilate learning. Courses that involved individual instruction, supportive 
materials and opportunities to test practice were more likely to result in increases in skills (Walter et al. 2005). 
Several studies emphasise the importance of supportive organisations, and follow-up support to promote 
sustained behaviour change. 
A number of contextual factors identified in the primary intervention studies related to the nature of training 
participants’ organisations and work commitments. Several studies stressed the importance of participants 
having supportive organisations – particularly in terms of managers being aware of and supportive of 
participation, or being willing to adjust workloads to enable participants to fully engage with course activities 
(Waqa et al. 2013; Jacobs et al. 2014; Tomatis et al. 2011). In a study from Fiji, this was seen as one of the 
factors enabling participants to achieve a course outcome (producing a policy brief), as other work
<<<PAGE=61>>>
www.itad.com/knowledge-and-resources/bcure 61 
 
commitments proved a major obstacle to the ‘larger than expected’ proportion of participants who did not 
complete the policy brief (Waqa et al. 2013). Participants in the Ethiopia Leadership course had to balance the 
course with routine work assignments, partly due to a shortage of public health professionals in regional 
offices. Future courses were shorter in length, resulting in higher retention rates (Rolle et al. 2011). 
Time and other commitments were common obstacles noted in several studies. Two studies from HICs found 
that lack of time was a major reason cited by training participants for not implementing their new knowledge 
(Jacobs et al. 2014; Peirson et al. 2012). In one case, a ‘culture of doing’ in the organisation resulted in staff 
feeling overwhelmed with the day-to-day demands of their jobs and unable to make space to consider 
evidence (Peirson et al. 2012). These findings link to evidence discussed in Section 2.3, which suggested that 
individual lack of time to use evidence can reflect an organisational culture that does not sufficiently value or 
encourage evidence use. 
A study from Uganda also found that work commitments presented a major risk to longer-term, modular 
courses, as trainees would often get absorbed back into routine workplace tasks. The intervention feature of 
post-training support visits were considered essential to mitigate this, through assisting participants in 
conducting successful work-based projects using their new skills (Matovu et al. 2013). Another study also 
emphasised the importance of post-workshop assistance in the form of ongoing mentoring support, finding 
that without ‘supportive follow-up and supervised application of skills, participants frequently continued to 
use the same work practices that they had used before’. The authors discuss an example from Cameroon 
where, just after a workshop on epidemics, decision makers were notified that an actual epidemic of bacterial 
meningitis might be occurring. Participants were ready to leave for the weekend and start the response on 
Monday, but the visiting trainers worked with the Cameroonian colleagues over the weekend to initiate an 
immediate response. The positive effects of this ‘emphasised for the trainees the importance and 
effectiveness of timely action’ (Pappaioanou et al. 2003). 
A study from Uganda and one from Liberia found that the interventions actively engaging participants’ 
organisations to secure support and permission for trainees proved an important predictor of success in the 
completion of course projects (Matovu et al. 2013; Rowe et al. 2010). One study emphasised that gaining this 
buy-in took time; and later interventions reduced drop-out rates by limiting admittance to trainees whose 
institutions made active commitments to support trainees (Matovu et al. 2013). 
Sustainable or longer-term change may be promoted by secure funding for ongoing training, a clear institutional 
‘home’ for new training courses, and/or a ‘training of trainers’ approach. 
In one US study, one of the two most significant reasons cited by participants for not utilising knowledge 
gained from training was lack of funding for ongoing training (Jacobs et al. 2014). These findings are supported 
by a study of a multifaceted EIPM capacity development programme in Canada, which found that a decision to 
commit long-term core funding to training was critical to the strategy’s success (Peirson et al. 2012). Another 
study emphasised the importance of existing institutionalised training programmes which could provide a 
‘home’ and continued funding for the training in future. For example, in Mexico and the Philippines a ‘capable 
core group of applied epidemiologists’ already existed who could assimilate the new training into their health 
systems. However, in Bolivia there were no similar applied training programmes, and as a result it proved 
difficult to sustain capacity development efforts (Pappaioanou et al. 2003). Another study emphasised that 
course participants later became trainers, as part of the transition from external intervention to full ownership 
of the course by a Peruvian faculty (Tomatis et al. 2011).
<<<PAGE=62>>>
www.itad.com/knowledge-and-resources/bcure 62 
 
Other contextual and intervention factors that may affect training include the initial skill levels of participants, 
the provision of practical tools, the inclusion of co-workers, and pre-existing beliefs about the importance of 
EIPM. 
The initial skills-base of participants seemed to be an important contextual factor affecting intervention 
success in some studies. A study from Fiji found that one factor constraining the achievement of EIPM skills 
was the low level of initial technical capacity and awareness of course participants – which was not 
anticipated by course managers (Waqa et al. 2013). Similar, a training course implemented in four lower-
income contexts discovered the need to build participants’ proficiency in basic quantitative skills in order for 
them to grasp the core course content. This proved time consuming and required longer-term concerted 
efforts (Pappaioanou et al. 2003). 
Some studies emphasised the importance of providing tools to support EIPM as a feature of training 
interventions, in order to help participants put knowledge into practice (Pappaioanou et al. 2003; Rowe et al. 
2010). For example, one study provided clear technical guidelines and training materials on how to ‘collect, 
calculate, interpret and use a threshold rate’ required to initiate a response to an epidemic (Pappaioanou et 
al. 2003). Tools for EIPM are further discussed in Section 3.3. 
Another US study found that one of the largest contextual barriers to implementing new skills was the fact 
that participants’ co-workers were not trained, suggesting that having a number of individuals from the same 
organisation attending a course created a ‘critical mass’ necessary for behaviour change (Jacobs et al. 2014). 
This suggests that courses aiming to promote change at an organisational level may need to consider the 
networks of participants as well as their roles within the organisation, in line with the theories of complex 
systems and multi-dimensional capacity development discussed in Section 1.3.1. 
Finally, a potentially interesting contextual factor highlighted in one study was that participants already placed 
a high importance on EIPM. Although not explicitly discussed by the authors, this may have contributed to 
course success (Jacobs et al. 2014). The same study also notes an increase in focus on EIPM by other actors, 
such as funding and accreditation agencies – possibly providing external incentives to change behaviour. This 
may explain why the control group in this study also saw mean increases in perceived importance of evidence 
use, and evidence availability (Jacobs et al. 2014). 
There was limited acknowledgement of the role of politics and power in evidence use in the studies examined. 
Rational and linear models of evidence use discussed in Section 1.2.1 appeared to explicitly or implicitly 
underpin the content of several training courses – one contained content on ‘what constitutes a policy and 
the policy cycle’ (Waqa et al. 2013) and another provided guidance on the use of evidence at different stages 
which clearly align with the policy cycle (e.g. asking an answerable question; finding the evidence to answer it; 
assessing its trustworthiness and evaluating to feed back to the process) (Pettman et al. 2013). Other courses 
were focused more on technical aspects of evidence interpretation than on the political question of how to 
use evidence in policy processes (Tomatis et al. 2011; Rolle et al. 2011). The ‘pluralism and opportunism’ 
model of EIPM was also implicitly reflected in some studies – for example Pettman et al. (2013) acknowledge 
the ‘wide range of competing information inputs required for decision making’, and describe how the course 
they report on spent ‘proportionally more time…addressing issues in applying evidence’ such as ‘working in 
the gaps where evidence is insufficient’ and ‘strategies to support individuals to work in an ‘evidence-
informed way’ in their organisations.’ However, there was little or no reference to the ‘politics and 
legitimisation’ model and associated theories, which suggest the centrality of power and politics to the 
processes of evidence use.
<<<PAGE=63>>>
www.itad.com/knowledge-and-resources/bcure 63 
 
Linked to this is the observation that studies contained limited reference to the political, cognitive, cultural and 
institutional factors promoting and constraining evidence use discussed in Section 2 – for example the 
influence of different actors and the political environment on the application of new skills in the workplace. 
This suggests some disconnect between the theories and empirical evidence discussed in Sections 1 and 2, 
and the primary intervention evidence summarised above. 
Summary: in what ways does training support EIPM, how, in what circumstances and why? 
This review examined ten primary intervention studies presenting evidence that professional training can lead 
to the outcomes of improved individual capacity for EIPM – understood as improvements in skills, knowledge 
and attitudes relating to the access, appraisal and use of evidence. Although the overall quality of studies is 
medium-high, the majority of studies are based on self-assessments of EIPM skills through pre- and post-
course surveys, and involve limited triangulation with other sources of evidence. This raises some doubts 
about the reliability of the findings given the risk of self-esteem bias. Only two studies provided more 
objective evidence of individual and organisational increase in the access, appraisal and use of evidence. 
Several studies suggest that training may lead to improvements in capacity through the mechanism of self-
efficacy – by improving participants’ confidence in their capability to perform a certain task or handle a 
particular situation. However, none of the primary studies link training approaches to any formal models of 
learning and individual skills development, which may provide other ways to conceptualise the mechanisms 
through which training leads to behaviour change. 
The studies discussed several features of training interventions and the wider context thought to contribute 
to the outcome of improved individual capacity: 
 One of the most significant intervention features suggested as important in the literature was combining 
classroom training with on-site projects, or at least ensuring the applicability of course content to 
participants’ roles, perhaps through providing tools to support EIPM. 
 The importance of supportive organisations was widely mentioned, suggesting the need to actively engage 
and ensure support when designing training courses, and to consider whether there is a locally recognised 
need for capacity development. 
 Organisational support may also help mitigate the risk of other work commitments or lack of time, 
preventing individuals from putting their new EIPM knowledge and skills into practice. Some interventions 
also successfully addressed this risk through post-training visits or mentoring. 
 Finally, some studies discussed the sustainability of capacity development interventions, emphasising the 
role of longer-term core funding or a training of trainers approach in promoting sustainability, or linking 
courses to existing institutional training programmes that can provide a long-term home for capacity 
development efforts. 
However, these contextual factors largely reflect features of organisational but not institutional contexts. 
Studies contained limited reference to political, cognitive, cultural and institutional factors promoting and 
constraining evidence use, such as those discussed in Section 2.
<<<PAGE=64>>>
www.itad.com/knowledge-and-resources/bcure 64 
 
3.2. Interpersonal change 
‘Interpersonal change’ refers to relationships and networks between individuals and groups, and how these 
influence EIPM. The studies discussed in this section are summarised in Table 8 below. 
Table 8. Summary of evidence relating to interpersonal-level interventions 
Source Field Geographical 
context 
Type of 
evidence 
Research approach and methods Quality 
(/12) 
Dobbins, 
Hanna, et al. 
2009 
Health Canada Primary 
intervention 
study 
Experimental Randomised controlled trial of 
knowledge-broker intervention 
11 
Dobbins, 
Robeson, et 
al. 2009 
Health Canada Primary 
intervention 
study 
Observational Observational findings relating to 
above experimental study, including 
reflective journals  
8 
Gabbay et al. 
2003 
Health UK Primary 
intervention 
study 
Observational Case study drawing on observation 
and interviews 
11 
Pappaioanou 
et al. 2003 
Development 
Studies 
Bolivia, 
Cameroon, 
Mexico, 
Philippines 
Primary 
intervention 
study 
Observational Case study of project implementation 6 
Peirson et al. 
2012 
Health Canada Primary 
intervention 
study 
Observational Case study: 27 semi-structured 
interviews and FGDs with 70 staff 
members; and document review 
12 
Traynor et al. 
2014 
Development 
Studies 
 Primary 
intervention 
study 
Experimental 
and 
observational  
2 interventions discussed: a 
randomised controlled trial and a 
separate qualitative case study 
10 
Armstrong et 
al. 2013 
Public 
Administration 
Canada Primary non-
intervention 
study 
Observational Case study of design process for EIPM 
intervention 
 
ICAI 2014 Development 
studies 
UK Primary non-
intervention 
study 
Observational Document review; analysis of DFID 
staff surveys; semi-structured 
interviews and FGDs with 92 
individuals 
 
Nisbett et al. 
2014 
Health Kenya, 
Ethiopia, 
India, 
Bangladesh 
Primary non-
intervention 
study 
Observational Interviews with 89 individuals  
Walter et al. 
2005 
Health Global (mainly 
developed 
countries) 
Secondary 
review 
Systematic 
review 
  
Gagliardi et 
al 2014 
Health Mainly high-
income 
countries 
Secondary 
review 
Systematic 
review 
  
Greenhalgh 
et al. 2004 
Health Global Secondary 
review 
Systematic 
review 
  
Liverani et al. 
2013 
Health Global Secondary 
review 
Systematic 
review 
  
McCormack 
et al. 2013 
Health Mainly high-
income 
countries 
Secondary 
review 
Other review 
(realist 
review) 
  
Pawson 2004 Sociology Global, 
including 
lower-income 
contexts 
Secondary 
review 
Other review 
(realist 
review) 
  
World Bank 
2015 
Development 
studies 
Global, 
including 
lower-income 
contexts 
Secondary 
review 
Other review
<<<PAGE=65>>>
www.itad.com/knowledge-and-resources/bcure 65 
 
The studies discussed in this section cover three main categories of interpersonal-level interventions: 
networks, knowledge brokers, and champions. These are discussed in turn below. 
Networks 
Networks are ‘formal or informal structures that link actors 
(individuals or organisations) who share a common interest on a 
specific issue or a general set of values’. A network might be 
virtual (e.g. a web-based portal) or physical (a group that meets 
in person), or a combination of the two (Perkin & Court 2005). 
This section considers four primary intervention studies 
containing evidence relating to networks for public sector 
decision making and/or practice (Pappaioanou et al. 2003; 
Dobbins, Robeson, et al. 2009; Gabbay et al. 2003; Peirson et al. 
2012). It also draws on insights from two secondary reviews 
(Walter et al. 2005; Perkin & Court 2005) and two non-
intervention primary studies (ICAI 2014; Armstrong et al. 2013). 
There is considerable wider literature relating to how networks 
between researchers and policy makers can result in EIPM; but 
this relates largely to evidence on ‘supply side’ factors 
promoting EIPM, which is outside the scope of this review. 
In three of the four intervention studies, networks were created as part of a multifaceted capacity 
development intervention for EIPM (Peirson et al. 2012; Dobbins, Robeson, et al. 2009; Pappaioanou et al. 
2003). Networks consisted of: formal clubs for staff with common interests to meet regularly (Peirson et al. 
2012); settings such as workshops where decision makers and technical experts could interact (Pappaioanou 
et al. 2003); and a facilitated forum to connect public health decision makers across Canada (Dobbins, 
Robeson, et al. 2009). In all three cases, the networking aspect of the intervention was a relatively minor 
component. The fourth study focuses directly on how health ‘CoPs’ in the UK used evidence to formulate 
ideas for health and social policy change (Gabbay et al. 2003). This study is interesting, as it is one of the only 
empirical studies in this review that considered in detail how interactions between people shaped the 
interpretation of knowledge, in line with more recent theories regarding the role of relationships and 
networks in shaping evidence use discussed in Section 1.2.2. 
There is limited evidence on the behaviour-change outcomes of networks, but studies imply that networks can 
facilitate behaviour change through the mechanisms of ‘social learning’ or ‘social processing’. 
Three of the four primary intervention studies emphasise the role of networks in promoting the outcome of 
knowledge sharing or exchange within or between organisations (Peirson et al. 2012; Pappaioanou et al. 2003; 
Dobbins, Robeson, et al. 2009). This outcome is also highlighted in some non-intervention primary studies 
(Armstrong et al. 2013; ICAI 2014). All three primary studies imply that networks play a role in facilitating the 
mechanism of ‘social learning’. ‘Social learning’ is a theory discussed in Section 1.3.1 – suggesting that learning 
happens through opportunities to discuss ideas with and observe the behaviour of others, resulting in 
increases in individual or collective knowledge and understanding.  For example, informants in one study 
emphasised that formal workplace ‘clubs’ provided ‘occasions to think, exchange, train and work with 
colleagues’. In another intervention, regional webinars were used to connect participants from different 
public health organisations around the country, providing opportunities for participants to discuss EIPM 
“Networks are ‘formal 
or informal structures 
that link actors 
(individuals or 
organisations) who 
share a common 
interest on a specific 
issue or a general set of 
values’.”
<<<PAGE=66>>>
www.itad.com/knowledge-and-resources/bcure 66 
 
issues, identify implications of evidence for policy and practice, and develop innovative ideas to promote EIPM 
in their organisations (Dobbins, Robeson, et al. 2009). 
One study of an intervention in four lower and middle-income contexts aiming to improve health leadership 
found that bringing together researchers and policy makers – through creating settings (e.g. workshops) where 
decision makers and technical experts could interact – led to the outcome of improved understanding and 
communication between decision makers and technical experts (Pappaioanou et al. 2003). The mechanism 
here seems to be ‘social processing,’ in which opportunities to interact led to participants’ beliefs and 
understanding shifting towards a consensus. The authors report that the approach ‘contributed toward 
decision makers understanding epidemiologic questions that were relevant to their policies or programs, and 
epidemiologists understanding the importance of framing an issue for a local policy or program in a social and 
political context’. This resonates somewhat with ideas of ‘policy networks’ and knowledge ‘co-production’ in 
the theoretical EIPM literature discussed in Section 1.2.2, which depict actors from the policy and research 
worlds as working together to interpret and ‘construct’ evidence to inform decision making. 
A literature review also emphasised the potential of networks to lead to the slightly separate outcome of 
improved trust between researchers and policy makers – for example the AFREPERN network, which has 
enabled researchers to secure confidential documents not available in the public domain (Perkin & Court 
2005). 
Studies suggest a number of contextual and intervention features that may increase use of networks – including 
external input, the support of senior management, and formal opportunities for meetings. 
Although the studies discussed in this section provide limited evidence on the mechanisms through which 
networks can result in behaviour change, there is some evidence on intervention features and contextual 
factors that make networks more or less likely to be used. For example, local government staff interviewed in 
a qualitative non-intervention study from Australia felt that knowledge sharing was enabled by networks with 
external input, e.g. from other government agencies and academics. They also emphasised the importance of 
networking opportunities being attended by senior individuals, and felt the support of senior management was 
necessary to ensure that networking drives action (Armstrong et al. 2013). Supportive management was also 
emphasised by participants in DFID’s informal Urban Virtual Network, which was set up proactively by a 
number of DFID staff in different offices and roles who were working on common issues in urban 
development. This was described as providing a ‘safe space’ to discuss a topic of mutual interest (although it is 
not clear whether or how exactly it contributed to changes in practice). Staff commented that the network 
was inhibited by an absence of senior management support, meaning that it relied on the volunteered time of 
staff members (ICAI 2014). One study from Canada emphasised the importance of providing formal 
opportunities to meet regularly for staff with common interests (Peirson et al. 2012). Another study 
emphasised strategies to enable remote participation such as teleconferences and webinars, along with a 
knowledge broker to facilitate the network (Dobbins, Robeson, et al. 2009). 
Networks can lead to evidence being interpreted by participants in ways that result in evidence non-use. 
One qualitative study from the UK demonstrates the role of networks in interpreting the meaning of evidence, 
in ways that may not always result in positive outcomes (Gabbay et al. 2003). The study examined how two 
multi-agency Communities of Practice (CoPs) in the UK’s National Health Service processed and applied 
knowledge in formulating their views. The two CoPs involved health staff, members of the public and 
individuals from the private sector coming together to work on ‘improving specific aspects of health and social 
services for older people’. This study found that the groups went beyond sharing and pooling knowledge;
<<<PAGE=67>>>
www.itad.com/knowledge-and-resources/bcure 67 
 
together they collectively ‘transformed’ the meaning of evidence, often gradually and imperceptibly over 
time. This appears to be an example of the mechanism of social processing, and again resonates strongly with 
theories of knowledge ‘co-production’ in policy networks discussed in Section 1.2.2.  In one example, a group 
of participants extracted portions of text from evidence sources based on cursory appraisal, and then shared 
what they took to be the important features with the other members – in effect transforming the evidence ‘to 
convey their own experience and knowledge’. These claims, which were not representative of the evidence 
base as a whole, then became accepted wisdom within the group. This ultimately led to the outcome of the 
group delivering non-evidence-based recommendations about interventions. The authors caution against 
drawing overly strong conclusions from these two small-scale case studies, which are focused more on 
examining the relationships and processes of knowledge translation than on the outcome. However, the 
findings do imply the need to consider the potentially powerful role of personal experience and group 
dynamics in affecting the interpretation and use of evidence within networks. 
Gabbay et al. (2003) also describe several contextual factors which appeared to result in social processing 
leading to non-use of evidence. They found that sources of evidence – such as systematic reviews and 
statistical data – were more likely to be accepted and used by the group when evidence chimed with existing 
experiences, or was communicated by a person considered an ‘expert’ or who possessed good interpersonal 
and communication skills. The former factor resonates with the theories discussed in Section 1.2.3, which 
emphasise the role of mental models and cognitive biases in shaping how individuals understand and 
interpret evidence. 
The study also found one contextual factor promoting the discussion and use of evidence by the group – the 
organisational business case that required a discussion of evidence. The study authors feel this may have 
resulted in the groups using more evidence than they would have done otherwise. This echoes evidence 
discussed in Section 2.3, suggesting that individual motivation for EIPM can be promoted by evidence being 
clearly valued within an organisation. 
Summary: in what ways do networks support EIPM, how, in what circumstances and why? 
This review discussed four medium-high quality intervention studies referring to networks established to 
promote EIPM, alongside a number of secondary reviews and non-intervention studies. These largely suggest 
that networks can help promote the outcome of knowledge sharing or exchange, but do not specifically 
measure this outcome or provide evidence on how knowledge sharing may result in behaviour change. 
Networks may also help improve understanding and communication between different groups. 
Some evidence suggests that networks may lead to change through the mechanism of social learning: 
discussing ideas with colleagues through a network provides the opportunity for people to be influenced by 
others. However, there is little detail on how exactly social learning might influence behaviour change through 
networks. Two studies also suggest that the mechanism of social processing contributed to change – 
opportunities to interact led to participants’ beliefs and understanding shifting towards a consensus. In one 
case, this mechanism seems to have helped build trust between researchers and policy makers. However, 
another study emphasises that social processing does not necessarily lead to improved use of evidence; it 
may in fact result in evidence being collectively ‘misinterpreted’ by networks, resulting in the negative 
outcome of evidence non-use. 
The evidence provides limited insights into the contextual or intervention features that may make networks 
more likely to change behaviour. However, as with training, supportive management was seen to be important
<<<PAGE=68>>>
www.itad.com/knowledge-and-resources/bcure 68 
 
to the success of networks in two studies. One study suggested that the input of external experts and senior 
individuals may also encourage participation. 
One study discusses the contextual factors that influence social processing – suggesting that evidence was 
more likely to be accepted within a network if it chimed with existing experience, was relayed by an expert or 
was communicated by someone with good interpersonal skills. In this study, an external incentive in the form 
of an organisational business case process helped steer the group towards considering more objective 
evidence. These findings resonate with theories on cognitive processes and evidence co-construction in policy 
networks, discussed in Section 1; and also with evidence from Section 2 on the importance of organisational 
incentives in promoting evidence use. 
Knowledge brokers 
Knowledge brokers (KBs) are defined in this report as 
individuals who play a formal (usually paid) role in connecting 
decision makers with research and research producers. KBs 
are increasingly employed in health organisations to ‘link 
researchers and decision makers, facilitating their interaction 
so that they are better able to understand each other’s goals 
and professional culture, influence each other’s work, forge 
new partnerships and use research-based evidence’ (Traynor 
et al. 2014). KBs may work inside a policy making organisation 
or external to it. 
There is considerable overlap between the terms ‘knowledge 
broker’ and ‘champion’ in the literature, and both are variously referred to as ‘change agents’, ‘opinion 
leaders’, ‘facilitators’ and ‘linking agents’ (McCormack et al. 2013). There is also some overlap with work-
based mentoring, which can be understood as an ‘interactive, facilitative process meant to promote learning 
and development’ (Gagliardi et al. 2014), usually involving a formal or informal relationship between staff 
members in an organisation and a ‘knowledgeable guide’ (Pawson 2004). Several reviews discuss the broad 
and diffuse nature of the evidence base on these types of interventions, which often vary drastically in 
context, design and their use of terminology – making it difficult to meaningfully synthesise evidence on 
outcomes (McCormack et al. 2013; Gagliardi et al. 2014; Walter et al. 2005). 
This section discusses two primary intervention studies examining the role of KBs in promoting EIPM (Traynor 
et al. 2014; Dobbins, Robeson, et al. 2009). Several secondary reviews (Walter et al. 2005; World Bank 2015b; 
Greenhalgh et al. 2004; Gagliardi et al. 2014; Liverani et al. 2013; McCormack et al. 2013) also provide insights 
into the mechanisms that enable knowledge brokering to lead to EIPM-related behaviour change. 
Knowledge brokers can contribute to the outcome of increased use of research evidence within organisations. 
Two interventions are discussed in the two primary intervention studies considered in this section: 
1. Both studies consider a randomised control trial (RCT) of a KB intervention in Canadian public health 
agencies (Traynor et al. 2014; Dobbins, Robeson, et al. 2009). The RCT results are reported in full in 
Dobbins, Hanna, et al. (2009). 
2. Traynor et al. (2014) also consider a case study of a separate Canadian KB intervention – the 
Partnerships for Health System Improvements (PHSI) programme. 
“Knowledge brokers are 
defined as individuals 
who play a formal 
(usually paid) role in 
connecting decision 
makers with research 
and research producers.”
<<<PAGE=69>>>
www.itad.com/knowledge-and-resources/bcure 69 
 
In both interventions, the KBs were external experts working within Canadian health departments to provide 
tailored support to health department staff, including group training, one-on-one consultation, and virtual 
support. The RCT found a statistically significant increase in evidence-informed decision making at follow-up – 
but only among organisations that had a low initial ‘culture of evidence use’ (measured through a staff 
questionnaire) at baseline. The case study also found a statistically significant increase in individual and 
organisational EIPM skills and capacities and a large and statistically significant increase in EIPM behaviours, 
although at the time of writing these results were not yet published in detail (Traynor et al. 2014). 
Knowledge brokers may contribute to change through the mechanisms of self-efficacy and ‘cheerleading’ for 
EIPM. 
In both interventions, KBs appeared to help promote behaviour change through the mechanism of self-
efficacy. This seems to have occurred through the direct transfer of expertise as the KB delivered coaching and 
training activities which resulted in increases in knowledge and skills (Dobbins, Robeson, et al. 2009). It also 
appears to have occurred indirectly, as the KB helped to informally build the confidence in staff in their ability 
to apply EIPM skills, mitigating ‘the anxiety inherent with the uncertainty of learning something new’ (Traynor 
et al. 2014). This mechanism seems to have been assisted by the intervention feature of personalised and in-
person guidance and support to staff members. 
A systematic review of mentorship as a knowledge translation strategy also found evidence of the ‘transfer of 
expertise’ mechanism in 12 studies, in which mentors providing coaching and other professional support 
resulted in the outcome of improved knowledge, skills and performance of mentees (mainly self-reported, 
although three studies measured objective increases in professional skills). This study could not isolate factors 
of the mentoring programmes that resulted in success, but did emphasise the need for resources to support 
mentoring activities, as well as clarity in mentoring goals (Gagliardi et al. 2014). 
KBs also appeared to play a role in contextualising evidence to the specific practice issues participants were 
facing, suggesting that KBs may build self-efficacy through direct provision of relevant contextualised evidence 
demanded by decision makers (relating to the ‘demand-pull’ model discussed in Section 1.2.2 above) (Traynor 
et al. 2014). Implicit in this mechanism is the need for a context in which decision makers actively demand 
evidence which the KB can supply. 
Finally, the KBs in both interventions appeared to also act as cheerleaders – a mechanism involving KBs 
stimulating and maintaining staff enthusiasm for EIPM (Traynor et al. 2014). Through recommending tools and 
resources and providing personal guidance on how to search for, identify and appraise research evidence, KBs 
helped to ‘maintain momentum’ among staff for skills development. Implicit in the study is that this 
mechanism operates in an intervention context involving multifaceted capacity development interventions, 
which incorporate other support (such as training) alongside a KB. 
The personal characteristics, strategies and experience of knowledge brokers are important in enabling them to 
lead to change – along with their position and level of support within an organisation. 
A realist review of EIPM strategies found limited evidence on how personal characteristics of ‘change agents’ 
(including KBs and champions) affect outcomes (McCormack et al. 2013). However, a number of other studies 
offer insights. Traynor et al. discussed contextual and intervention features that appear to have contributed to 
the increase in EIPM capacities among staff in two KB interventions – including the KB possessing strong 
teaching and interpersonal skills, which enabled the development of trusting and collaborative relationships, 
and expertise in both EIPM-related skills and the health field, which conferred credibility. The study also 
suggests that self-efficacy of staff members was promoted by KB’s ability to pick up knowledge quickly, and to
<<<PAGE=70>>>
www.itad.com/knowledge-and-resources/bcure 70 
 
provide objective direction (Traynor et al. 2014). A systematic review of political influences on the use of 
evidence in health policy found that policy makers were more likely to adopt solutions proposed by research 
intermediaries if the proposed solutions were compatible with the wider policy agenda of central government, 
suggesting the importance of a KB with sufficient understanding of political agendas and priorities (Liverani et 
al. 2013). 
Another study found that early one-to-one contact correlated with greater utilisation of KB services by staff 
members over the course of the intervention (Dobbins, Robeson, et al. 2009). A realist review of EIPM 
strategies in healthcare suggested the importance of KBs being accessible and organised and being culturally 
compatible with the target group – in terms of having a perceived connection, for example in age (McCormack 
et al. 2013). A systematic review also found evidence that the reputation and professional legitimacy of the 
institution supplying the KB may contribute to a KB’s success (Liverani et al. 2013). 
Both KB interventions discussed in Traynor et al. (2014) and Dobbins, Robeson, et al. (2009) – the RCT and the 
PHSI programme outlined above – seemed to benefit from the KBs being viewed as ‘objective outsiders’ 
separate from organisational politics. However, this may raise issues of sustainability (if KBs take the 
knowledge with them when they leave); and the KB in the PHSI programme felt that trusting relationships 
with staff had been developed in part because she had worked with them before (Traynor et al. 2014). The 
main difference between the RCT and PHSI interventions was time – in the latter programme, the KB spent 22 
months rather than 1 year in the organisation. This seems to have allowed sufficient time to build trusting and 
collaborative relationships, and also to conduct capacity development activities with staff (Traynor et al. 
2014). In addition, a realist review of EIPM interventions found that one of the most important predictors of 
success was for the KB to be embedded in the context – which could be achieved by individuals either inside 
or outside an organisation (McCormack et al. 2013). 
Finally, organisational support for a KB was seen as a crucial enabling contextual factor in both Canadian KB 
interventions. Occasionally EIPM work ‘was not deemed a priority’, and staff members were not given enough 
time or space in their workloads to spend time with the KB. However, the KB in the PHSI intervention helped 
create organisational support, by liaising with management to ensure staff had sufficient time to engage 
(Traynor et al. 2014). This suggests that a successful KB with the requisite skills can help influence managers 
towards recognising the value of EIPM in an organisation. This finding is supported by a realist review of EIPM 
strategies, which emphasised that KBs with good interpersonal skills, respect, positivity and responsibility are 
more likely to be able to influence managers, although this study also highlights the risks posed by KBs facing 
unrealistic expectations from managers (McCormack et al. 2013). There are also echoes of this finding in a 
primary study of nutrition champions, which found that although a champion’s ability to influence change is 
shaped by the wider environment, at the same time part of what makes a champion effective is his or her 
ability to influence this environment (Nisbett et al. 2014). 
Summary: in what ways do knowledge brokers support EIPM, how, in what circumstances and why? 
KBs are defined in this report as individuals who play a formal (usually paid) role in connecting decision 
makers with research and research producers. This section discussed findings from two medium-high quality 
intervention studies (both from Canadian knowledge-broker interventions), and several secondary reviews. 
This evidence suggests that KBs can lead to the outcomes of increased individual and organisational EIPM 
capacities, as well as an increased number of programmes and policies supported by research evidence in 
certain types of organisations.
<<<PAGE=71>>>
www.itad.com/knowledge-and-resources/bcure 71 
 
Both studies suggest that KBs help achieve these outcomes through the mechanism of cheerleading – in that 
they help stimulate and maintain staff enthusiasm for EIPM, including among managers. One study suggested 
that KBs can promote the mechanism of staff self-efficacy, through either formal training or coaching, or more 
informal support and encouragement which build staff confidence. This may involve directly supplying 
evidence demanded by decision makers – in line with the ‘demand-push’ model discussed in Section 1.2.2. 
Evidence from both primary studies as well as several secondary reviews suggests a number of features and 
qualities of KBs that may influence their effectiveness at achieving EIPM outcomes: 
 The ability of a KB to quickly pick up evidence and provide objective guidance that takes into account 
wider policy agendas, implying the need for sufficient political understanding. 
 Skills in teaching and EIPM (such as accessing, appraising and interpreting evidence), as well as some 
background in the technical field in question (e.g. health). 
 Interpersonal skills and qualities such as respect, leadership, positivity and responsibility. 
 Cultural compatibility of KBs with the target group. 
Both primary studies emphasised the importance of KBs having sufficient organisational support. Although 
successful KBs are able to build this support, a basic level of managerial buy-in appears important. Finally, one 
study suggests that early contact with staff members may promote staff use of knowledge brokering services, 
and also that KBs benefit from more time in general in order to build up trust. 
 
Champions 
In contrast to KBs (who play a formal role in translating knowledge for policy makers, and are often external to 
an organisation) champions are defined in this review as people embedded within an organisation or 
institutional context, who (formally or informally) promote EIPM practices. Two primary intervention studies 
found through the evidence search relate to the role of champions in promoting EIPM (Pappaioanou et al. 
2003; Peirson et al. 2012). Two non-intervention studies (ICAI 2014; Nisbett et al. 2014) and four secondary 
reviews (Greenhalgh et al. 2004; McCormack et al. 2013; World Bank 2015b; Walter et al. 2005) also provide 
insights into the role of champions in promoting EIPM. 
The intervention studies find evidence that champions can contribute to the outcome of increased use of 
research evidence within organisations. 
Both the intervention studies discussing champions related to multifaceted EIPM interventions, in which 
‘champions’ emerged informally. 
1. The first study examines the first two years of a ten-year EIPM strategy within a Canadian public 
health organisation. In this case, certain senior staff members (both with and without formal EIPM 
responsibilities) played a role in ‘championing’ EIPM within the organisation, and were considered 
essential to achieving the outcome of higher visible use of research evidence and EIPM processes 
(Peirson et al. 2012). 
2. Another study discusses the Data for Decision Making (DDM) programme in Bolivia, Cameroon, 
Mexico and the Philippines; an intervention based on training and mentoring and discussed in more 
detail in Section 3.1 above. Again, champions were not a formal part of the intervention, but 
‘talented, visionary and strongly motivated senior health officials who championed DDM concepts’ 
were found to play an essential role in achieving the outcome of country ownership of EIPM goals, 
objectives and activities and in ensuring improved use of evidence in health policy making 
(Pappaioanou et al. 2003).
<<<PAGE=72>>>
www.itad.com/knowledge-and-resources/bcure 72 
 
Champions (and also knowledge brokers and mentors) may influence behaviours through the mechanisms of 
social learning, ‘transformational leadership’ and ‘network facilitation’. 
A systematic review examining the ‘diffusion of innovations’ through service organisations found that 
organisational innovations can be promoted by champions acting as ‘transformational leaders’, who influence, 
persuade and build support for change among other members of the organisation (Greenhalgh et al. 2004). 
One study of a Canadian health EIPM intervention appears to demonstrate this mechanism – emphasising the 
role of a senior individual in catalysing, ‘steering and staying the course for change’ throughout the 
organisation. This individual also promoted change through securing resources, in the form of significant and 
stable funding and time for staff to dedicate to EIPM (Peirson et al. 2012). Another intervention study also 
emphasised the role of ‘a talented, visionary, and strongly motivated senior health official who championed 
[EIPM] concepts’ and who was ‘essential for country ownership of goals, objectives, activities, and project 
success’ (Pappaioanou et al. 2003). KBs may also act as transformational leaders – one study emphasised the 
role of knowledge brokers in ‘championing’ EIPM by liaising with managers and persuading them to ensure 
staff had enough time to meet with and learn from the KB (Traynor et al. 2014). 
There is also some evidence that champions may promote change through the mechanism of ‘social learning,’ 
a theory of learning discussed in Section 1.3 and in relation to networks above, which holds that people are 
more likely to change their behaviours when practices are adopted by those close to them (World Bank 
2015b). For example, a systematic review of EIPM interventions emphasised the role of champions as ‘opinion 
leaders’ who can exert influence on the beliefs and actions of their colleagues, which in one study was found 
to be a key success factor in achieving the outcome of improved learning and clinical change. However, overall 
the systematic review found mixed results on the role of opinion leaders in promoting EIPM in healthcare 
settings (Walter et al. 2005). A realist review of strategies to promote evidence-informed healthcare also 
emphasised the role of knowledge brokers in modelling EIPM behaviours that others in the organisation copy, 
which is more likely to lead to change in contexts where the KB has gained the respect of staff members by 
demonstrating leadership (McCormack et al. 2013). 
A secondary review also suggested that change can be promoted through the mechanism of ‘network 
facilitation’, in which champions develop cross-functional coalitions among different groups within the 
organisation (Greenhalgh et al. 2004). A non-intervention study of nutrition champions in Bangladesh, 
Ethiopia, India and Kenya demonstrates this mechanism – finding that the most effective champions actively 
sought to bring different groups of stakeholders together (in different ways depending on the country 
context, discussed further below) (Nisbett et al. 2014). 
The personal characteristics, strategies and experience of champions are important contextual factors enabling 
them to lead to change – along with their position within an organisation or society. 
Two intervention studies emphasise the importance of the seniority of champions; particularly in relation to 
the transformational leaders mechanism (Pappaioanou et al. 2003; Peirson et al. 2012). The individuals 
described as ‘champions’ in non-intervention studies are frequently senior members of organisations or 
institutional environments – for example DFID’s Chief Scientist established new EIPM practices within the 
Research and Evidence Division by bringing in external experiences from the health field (ICAI 2014; see also 
Nisbett et al. 2014). However, one systematic review found evidence that opinion leaders do not always need 
to have leadership roles to promote social learning – rather, it seemed important that they came from the 
appropriate level of an organisation at different stages (e.g. experts at early stages of an intervention, and 
peers during implementation) (Walter et al. 2005).
<<<PAGE=73>>>
www.itad.com/knowledge-and-resources/bcure 73 
 
Another non-intervention study of nutrition champions in Bangladesh, Ethiopia, India and Kenya found that 
champions were viewed as particularly effective in mobilising and influencing others (acting as network 
facilitators) to act on nutrition when they demonstrated ‘post-conventional’ stages of personal development; 
for example they were able to recognise assumptions and the presence of dynamic systems, and were able to 
deal with complexity and a lack of certainty (Nisbett et al. 2014). Similarly, a study of a Canadian health 
intervention emphasised the importance of champions’ ‘vision and commitment’ and unwavering support for 
EIPM, which also seemed linked to their role as transformational leaders (Peirson et al. 2012). 
The study of nutritional champions also highlighted the importance of the institutional location of champions, 
closely linked to the political and policy environment, in enabling champions to act as network facilitators. For 
example, it was only in India that members of civil society were clearly viewed as influencing change; in Kenya 
key individuals within government were seen as the most important; and in Ethiopia very few individuals were 
considered influential, potentially reflecting a more authoritarian political structure (Nisbett et al. 2014). This 
study also suggested that champions used and moulded networks in different ways to build coalitions around 
issues, depending on the context. For example, the nutrition network in Bangladesh was relatively 
fragmented, and individuals cited as being the most effective in terms of contributing to positive changes in 
nutrition policy were able to span separate domains. In Kenya, leaders contributed to ‘building a more mature 
network’, facilitating participation in it, and then leveraging it to bring about change. In India, leaders 
demonstrated an ability to cross boundaries between civil society, academia and the state (Nisbett et al. 
2014). These findings relate strongly to EIPM theories of ‘policy networks’ discussed in Section 1.2.2, which 
suggest that researchers, policy makers and other groups 
(such as members of civil society and the media) often work 
together across professional divides, bound by shared value 
systems, political interests or specific problems. 
Finally, interview respondents in one Canadian study raised 
concerns relating to the stability and continuity of champions, 
which was seen as necessary to give time for EIPM to become 
embedded throughout the organisation. One respondent said 
‘If a new Medical Officer of Health … came in and said “we’re 
not going to do this,” people wouldn’t rally up and say “you 
can’t take that from us, that’s ours and we own that.” It’s not 
there yet’ (Peirson et al. 2012). 
Summary: in what ways do champions support EIPM, how, in what circumstances and why? 
Champions are defined in this review as people embedded within an organisation or institutional context, who 
(formally or informally) promote EIPM practices. This section discusses evidence on the role of champions in 
promoting EIPM from two medium-high quality intervention studies, two primary non-intervention studies 
and four secondary reviews. In both intervention studies, ‘champions’ emerged informally (rather than as an 
official part of the intervention). These studies considered champions to be essential in achieving the outcome 
of improved use of evidence within organisations or institutional environments. 
The literature suggested three main mechanisms that enabled champions to promote increased use of 
evidence, and provided insights into the contextual factors which enabled these mechanisms. 
First, some studies suggest that champions can bring about change through the mechanism of 
transformational leadership – building support for change within an organisation, or securing new resources. 
“Champions are defined 
as people embedded 
within an organisation or 
institutional context, 
who (formally or 
informally) promote 
EIPM practices.”
<<<PAGE=74>>>
www.itad.com/knowledge-and-resources/bcure 74 
 
The personal characteristics, strategies and experience of champions appear to be important contextual 
factors in enabling them to lead to change – with various studies emphasising the importance of vision, 
commitment and dedication to EIPM, champions’ seniority, their stability and continuity within an 
organisation, and their ability to apply external learning from a different job or field within a new context. 
Two secondary reviews shed light on a second mechanism that may enable champions to lead to change; that 
of social learning, in which people modify their behaviours when they are adopted by those close to them. 
This mechanism was also found in relation to networks, and links to the ‘role modelling’ or ‘opinion leading’ 
role of champions. One study suggests that seniority is not necessarily the most important factor – instead it 
may be more important for champions to exist within ‘appropriate levels’ of an organisation at different stages 
of an intervention, with peers potentially more influential when change is underway. 
Finally, one study suggests that champions may act as network facilitators, developing coalitions between 
different groups or individuals. This study found that network facilitation is affected by the institutional 
location of champions and the wider political environment, which affect the kinds of networking strategies 
that champions can successfully employ. This evidence resonates with theories of policy networks discussed 
in Section 1.2.2 above; which suggest that evidence use in policy processes is influenced by a wide and fluid 
range of actors working both within and outside government. 
3.3. Organisational change 
Organisational change refers to change in the systems, policies and procedures, practices, culture or norms 
within an organisation. This section draws on evidence from five primary intervention studies, six non-
intervention studies and three secondary reviews – as summarised in Table 9 below. 
Table 9. Summary of evidence relating organisational-level interventions 
Source Field Geographical 
context 
Type of 
evidence 
Research approach and methods Quality 
(/12) 
Dobbins, 
Robeson, et 
al. 2009 
Health Canada Primary 
intervention 
study 
Observational Observational findings relating to 
above experimental study, including 
reflective journals  
8 
Gabbay et al. 
2003 
Health UK Primary 
intervention 
study 
Observational Case study drawing on observation 
and interviews 
11 
Nutley et al., 
2013 
Health Kenya Primary 
intervention 
study 
Observational 13 IDIs with tool users and non-users 10 
Peirson et al. 
2012 
Health Canada Primary 
intervention 
study 
Observational Case study: 27 semi-structured 
interviews and FGDs with 70 staff 
members; and document review 
12 
Yost et al., 
2014 
Health Canada Primary 
intervention 
study 
Observational Reflective diaries kept by KBs, semi-
structured interviews, document 
reviews 
12 
ICAI 2014 Development 
studies 
UK Primary non-
intervention 
study 
Observational Document review; analysis of DFID 
staff surveys; semi-structured 
interviews and FGDs with 92 
individuals 
 
Shaxson, 
2014 
Public 
Administration 
UK  Primary non-
intervention 
study 
Observational Case study of organisational 
development process 
 
Waldman, 
2014 
Development 
Studies 
Afghanistan, 
Nepal, Sierra 
Leone 
Primary non-
intervention 
study 
Observational 52 in-depth interviews and field visits
<<<PAGE=75>>>
www.itad.com/knowledge-and-resources/bcure 75 
 
Gagliardi et 
al 2014 
Health Mainly high-
income 
countries 
Secondary 
review 
Systematic 
review 
  
Walter et al. 
2005 
Health Global (mainly 
developed 
countries) 
Secondary 
review 
Systematic 
review 
  
World Bank 
2015 
Development 
studies 
Global, 
including 
lower-income 
contexts 
Secondary 
review 
Other review   
 
The findings can be categorised into evidence on EIPM ‘tools’, and 
evidence on broader EIPM ‘systems and incentives’. Organisational 
‘tools’ include checklists, guidance notes, assessment criteria and 
templates, designed to help individuals search for, assess and 
interpret evidence. Organisational ‘systems’ for EIPM are broader; 
including processes, procedures and events at an organisational 
level that promote access, appraisal and use of evidence. These 
may include strategic plans, committee meetings, performance 
measures and programme approval processes. 
Organisational tools 
Two intervention studies considered the role of ‘tools’ to assist 
with EIPM. 
1. One examined a Canadian EIPM intervention in three 
public health organisations, assessing how well checklists, guidance notes, assessment criteria and 
templates helped individuals search for, assess and interpret evidence (for example, a data extraction 
table helping users to extract relevant information from systematic reviews) (Yost et al. 2014). 
2. Another study examined how the District Health Profile (DHP tool) affected health decision making in 
Kenya (Nutley et al. 2013). The DHP tool aggregated and analysed health data from a number of 
different reporting spreadsheets, to automatically produce reports and graphs in response to 11 
priority health questions (e.g. ‘are HIV positive individuals who are eligible for treatment receiving 
treatment?’). 
EIPM tools such as guidance, templates, checklists and assessment criteria can result in improved capacity by 
facilitating behaviour change, increasing self-efficacy, and increasing the value staff place on evidence. 
The Canadian study found that tools were perceived by staff as helping to keep EIPM practices ‘on track’ by 
providing a structure and concrete process for public health officials to follow (Yost et al. 2014). The tool 
therefore appears to have resulted in change through the mechanism of facilitation – enabling or facilitating 
staff to adopt EIPM behaviours, which led to the outcome of self-reported improvements in individual capacity 
and use of evidence in day-to-day work. This mechanism is underpinned by change management theories, 
which ‘emphasise the importance of enabling strategies providing practical assistance for individuals and 
groups to change’ – for example, by providing technical, financial, organisational or emotional support (Walter 
et al. 2005). Similarly, the study of the decision support tool in district health decision making in Kenya found 
that the tool seemed to work by making users’ existing work easier and more efficient – leading to the 
outcome of improved data analysis, review and interpretation at a district level, which in turn enabled staff to 
solve problems resulting in better health services (Nutley et al. 2013). The facilitation mechanism is also 
evident in a systematic review of interventions to promote EIPM, which found that computerised support 
“Organisational ‘tools’ 
include checklists, 
guidance notes, 
assessment criteria and 
templates, designed to 
help individuals search 
for, assess and interpret 
evidence.”
<<<PAGE=76>>>
www.itad.com/knowledge-and-resources/bcure 76 
 
systems can result in the outcome of improved evidence-based health practice by removing barriers to the use 
of evidence (Walter et al. 2005). 
In the study of the Canadian intervention, tools also seemed lead to change through the mechanism of 
improving staff self-efficacy; increasing staff confidence to use EIPM processes by providing step-by-step 
guidance (Yost et al. 2014). Another interesting mechanism is discussed in this study, suggesting that tools 
increased the value staff placed on evidence by improving people’s confidence in the findings they gathered 
through tools. Similarly, in Kenya, use of data in the tool by decision makers resulted in increased demand for 
additional data – a ‘virtuous cycle’ – by flagging up areas where more data was required that was not 
currently contained in the tool. The authors suggest that that ‘the use of the DHP tool may result in a deeper 
understanding of the value of data in decision making and in turn result in improved attitudes about the 
usefulness of data in general’ (Nutley et al. 2013). 
Contextual factors enabling the success of EIPM tools include pre-existing motivation for EIPM, sufficient ICT 
literacy, and sufficient instruction and support. 
Explicit in the Kenyan study and implicit in the Canadian one is the suggestion that tools help people to do 
what they are already doing better – implying that pre-existing EIPM values and practices are present in the 
context for tools to build on. This is highlighted by one interview respondent in the study by Yost et al. (2014), 
who enthused: ‘Finally! I’m getting the tools that I need to do the work that I think is the work that I’m 
supposed to be doing!’ A respondent in the Kenya study also ‘pointed out that data has to be appreciated in 
order to embrace [the tool’s] usefulness’. Tools also require a sufficient level of ICT literacy in order to access 
and use them effectively – something that requires particular consideration in lower and middle-income 
contexts where these skills may be especially low (C. J. Uneke et al. 2011). A low level of skills in the Kenya 
study was highlighted as a constraining contextual factor affecting use of the tool, along with a lack of 
technological infrastructure (computers and printers). Capacity support was therefore found to be an 
important intervention feature enabling use of the tool (Nutley et al. 2013), also emphasised by Yost et al. 
(2014). 
The Canadian study also emphasises the importance of several intervention features to promote tool 
effectiveness, including simple and clear instructions and the accessibility of tools (e.g. they are easy to find, 
available online, quick and easy to download, and available in editable Microsoft Word and PowerPoint 
formats rather than PDFs), and the relevance and timeliness of tools to current and anticipated work. This 
particular intervention also included a KB who provided support to staff members to help them use tools 
effectively (Yost et al. 2014). 
Organisational systems and incentives 
Two primary intervention studies explicitly considered the role of organisational systems change in promoting 
EIPM within organisations. 
1. One study of a Canadian EIPM strategy in a public health organisation discussed the impact of 
incorporating EIPM into strategic plans, committee meetings and conferences (Peirson et al. 2012). 
2. The second study discusses an RCT of a KB intervention in Canada, in which the KB promoted the 
inclusion of EIPM components in performance measures, and encouraged managers to require staff 
to provide evidence to support recommendations while posing critical questions (Dobbins, Robeson, 
et al. 2009).
<<<PAGE=77>>>
www.itad.com/knowledge-and-resources/bcure 77 
 
In both these studies, changes to systems were part of a broader multifaceted capacity development 
intervention involving other strategies such as training and knowledge brokering (discussed earlier in this 
section). Neither study provides much detail on how the organisational systems components of the 
intervention specifically resulted in change. 
Studies suggest that organisational systems may result in change through the mechanism of self-efficacy, as well 
as through facilitating EIPM behaviours and reinforcing them. 
The study of the Canadian EIPM strategy stressed that systems changes helped staff become more 
comfortable and familiar with EIPM as its language ‘permeated’ throughout the organisation. One interview 
respondent claimed ‘staff are more comfortable using the terminology...It’s in their minds, in their 
conversations’ (Peirson et al. 2012). This suggests that change at an organisational level can play a role in 
promoting self-efficacy and, in doing so, lead to the outcome of improved individual capacity for EIPM, perhaps 
particularly when combined with other forms of capacity development as in this particular intervention. This 
study also suggested that systems could be used to facilitate EIPM behaviours, similarly to the tools discussed 
above. For example, interview informants talked about the role of annual reviews in making practice into a 
routine, suggesting that EIPM concepts should be added to the review process. However, it is not clear from 
the study how far the mechanism of facilitation contributed to the observed outcome of enhanced EIPM 
within the organisation. The facilitation mechanism also seems to have been in play within the UK’s 
Department of Farming and Rural Affairs – in which systems and budgetary processes were developed to help 
provide a structure for how evidence should be used and handled, helping lead to the embedding of EIPM 
principles in the organisation (Shaxson 2014). 
Peirson et al. also suggest the role of organisational systems in reinforcing EIPM behaviours; a mechanism 
involving positive reinforcers (e.g. rewards) or negative ones (e.g. audit and the risk of negative feedback) 
acting to influence behaviours and actions. The reinforcement mechanism is based on behavioural learning 
theories; the idea that behaviour can be influenced by controlling external factors (discussed in Section 1.3.1, 
and in Walter et al. 2005). For example, the study emphasised the importance of including EIPM expectations 
within performance, accountability and incentive structures, such as individual performance objectives 
(Peirson et al. 2012). Including EIPM components in performance measures was also encouraged by 
knowledge brokers in the study of the KB intervention, although the results do not suggest how this aspect of 
the KB’s work helped contribute to the ultimate outcome of improved capacity for EIPM and (in certain 
organisations) more evidence-based policies (Dobbins, Robeson, et al. 2009; Dobbins, Hanna, et al. 2009). 
Interesting evidence on the reinforcement mechanism is also discussed in the 2015 World Development 
Report, which summarises evidence suggesting that ‘non-instrumental incentives’ such as status and 
recognition can be as effective as monetary incentives in motivating people to exert effort. Two examples 
from Switzerland and Zambia suggest that the outcome of improved workplace performance resulted from 
staff being promised ‘non-instrumental’ awards for good performance, such as a personal thank you from the 
manager, or a publicly presented chart to represent sales (World Bank 2015b). 
A non-intervention study examining the use of evidence by DFID advisers suggests that the ‘business case’ 
process resulted in the outcome of greater use of evidence in the organisation (Waldman 2014). Staff were 
required to complete a ‘business case’ template, including sections for appraising evidence, in order to secure 
funding for new programmes. This appeared to work through both the facilitation and reinforcement 
mechanisms – by providing a template to guide staff through the process of appraising and applying evidence, 
and also by setting standards that a programme design must meet in order to receive approval. The study 
identified the business case as a ‘major factor causing staff to seek out relevant research to justify their 
planned programmes’ (Waldman 2014).
<<<PAGE=78>>>
www.itad.com/knowledge-and-resources/bcure 78 
 
Systems (such as ‘business cases’) to promote EIPM may also result in negative outcomes. 
Section 2.1 discussed evidence suggesting that, in contexts where evidence is valued, this can encourage its 
use as a ‘weapon’ to confer legitimacy on decisions. The primary evidence on organisational systems sheds 
more light on this barrier to evidence use and the mechanisms which potentially explain it. 
Waldman’s study of the use of evidence by DFID advisers in fragile states pointed to some unintended 
consequences of business cases – policy makers ‘recycling’ evidence from previous successful cases in order to 
improve the likelihood of approval, and inserting widely used terms and concepts in order to secure ‘brownie 
points’ with senior management (Waldman 2014). These responses appear to be negative manifestations of 
the reinforcement mechanism – organisational systems created perverse incentives for staff to ‘misuse’ 
evidence. Waldman found a large amount of ‘symbolic’ use of evidence in the business case process (a model 
from the EIPM conceptual literature in which evidence is used to support pre-existing positions, discussed 
further in Section 1.2.1). This was ‘understood as being wholly normal practice.’ This review found little 
evidence on how systems can be designed to avoid perverse incentives, although one report suggested that 
independent quality assurance of DFID business cases has helped improve the use of evidence over time (ICAI 
2014). 
As well as creating perverse incentives, organisational systems may actually hinder the facilitation mechanism 
by making it more difficult to use evidence effectively. For example, one report mentions the ‘unwieldy and 
overly bureaucratic’ nature of the business case process, which it feels presents a barrier to organisational 
learning (ICAI 2014). 
Finally, a study of CoPs in the UK discussed in Section 3.2 above (Gabbay et al. 2003) suggests that without the 
contextual factor of existing commitment to and belief in the importance of research evidence, the business 
case process did not fully change behaviour – although it did force CoPs to consider evidence more than they 
may otherwise have done. The study found ‘there always remained a tension’ between the need to construct 
a business case using evidence, and the ‘default setting in which personal experience was highly valued by the 
CoPs.’ 
Summary: in what ways do organisational tools and systems support EIPM, how, in what 
circumstances and why? 
This section draws on evidence from five primary intervention studies, six non-intervention studies and three 
secondary reviews. The evidence suggests that tools and systems can lead to the outcomes of improved 
individual capacity and use of evidence, for example by improving data analysis, review and interpretation; and 
in one case resulting in improved evidence-based health practice. 
Tools and systems appear to lead to these outcomes through two main mechanisms: facilitation, and 
reinforcement. Firstly, two studies suggest that tools and systems can work through facilitating staff to adopt 
EIPM behaviours, by providing resources and processes that enable and support people to change their 
behaviour, or make people’s jobs easier. Linked to this, two studies suggest that tools can promote self-
efficacy – for example by providing step-by-step guidance that increases an individual’s confidence in her 
ability to successfully access, appraise or apply evidence; or in a more subtle way by helping to permeate the 
language of EIPM throughout an organisation, making it an accepted part of the culture. Two studies also 
suggest that tools may increase the value staff members place on evidence, for example through deepening 
their understanding of the benefits data can bring to decision making.
<<<PAGE=79>>>
www.itad.com/knowledge-and-resources/bcure 79 
 
Secondly, two studies suggest that organisational systems may work by reinforcing EIPM behaviours – through 
positive reinforcers (rewards) or negative ones (e.g. audit and the risk of negative feedback) influencing 
individual choices and actions. However, one study suggests that using systems to reinforce behaviour may 
create perverse incentives, for example to recycle evidence, use evidence symbolically to support pre-existing 
positions, or include widely used terms to secure ‘brownie points’ with managers. This builds on findings 
discussed in Section 2.1, suggesting that organisational incentives can act as a barrier to effective EIPM. 
A variety of studies suggested a small number of contextual and intervention features that influenced the 
success of organisational tools and systems for EIPM: 
 
 Low levels of skills and limited technological infrastructure can constrain the successful use of tools, 
particularly in low-income contexts. Capacity support is important to enable their successful use. 
 The use of tools may be promoted by providing simple and clear instructions and ensuring easy 
accessibility and the relevance and timeliness of tools to current and anticipated work. 
 Systems to promote EIPM may be improved by ensuring they are not overly time consuming or 
bureaucratic – factors that can present a barrier to learning. Incorporating independent quality assurance 
into EIPM systems may also reduce the risk of symbolic use of evidence, or using evidence to secure 
‘brownie points’. 
3.4. Institutional change 
Institutional change refers to change in the wider operating environment of individuals or organisations. This 
includes change within civil society and the media, as well as broader social change (e.g. in culture, norms, 
collective beliefs, attitudes, values) and change in external influencing factors (e.g. global events, political and 
economic factors, donor influence). While several BCURE projects work with civil society, the programme 
does not involve institutional-level interventions (an example might be providing capacity development to 
CSOs or journalists, to help them advocate for EIPM). Evidence on institutional change was therefore a 
relatively minor part of this review. 
The database and snowball searches found limited evidence on capacity development interventions focused 
on the wider enabling environment (e.g. civil society, the media and the general public), with the aim of 
promoting EIPM. Most evidence considering institutional factors affecting EIPM related to features of the 
institutional environment that promoted or constrained EIPM, and is discussed in Section 2.4 above. 
However, due to time constraints this review did not consider the broad literature on empowerment and 
accountability, which is likely to contain some useful insights. For example, evidence is emerging to suggest 
that providing seed funding, capacity development and relationship brokering support to small groups of local 
actors to enable them to use evidence and conduct advocacy can result in policy influence and policy change 
(DFID 2014b). This approach clearly links to the theory of ‘policy networks’ discussed in Section 1.2.3, as it 
focuses on bringing together various actors from different spheres (including academics, government 
employees wearing a non-government ‘hat’, and activists) who are united around an issue (e.g. on state 
budget advocacy) and who have some influence, rather than drawing a divide between ‘researchers’ and 
‘policy makers’. 
3.5. Policy change and policy quality 
The BCURE Theory of Change hypothesises that a combination of changes at individual, organisational, 
network and institutional level will catalyse demand for and use of evidence among targeted stakeholders.
<<<PAGE=80>>>
www.itad.com/knowledge-and-resources/bcure 80 
 
This will result in policy change, with policy and practice being increasingly informed by evidence. This in turn 
will lead to improved quality of policies and programmes. 
This review located a number of papers with insights into the impact of interventions on policy change and 
policy quality. However, these papers largely focused on the impact of specific research findings on policy 
change. This ‘supply-side’ evidence was examined in a recent literature review (Newman 2014), which 
highlighted several collections of case studies detailing ways in which research findings have led to policy 
change and development impacts (Court & Young 2003; Carden 2009). This literature lies outside the scope of 
this review, which instead aims to examine the ‘demand side’ evidence on how and in what circumstances 
capacity development interventions for EIPM have resulted in policy change and improved policy quality. 
Five primary intervention studies discussed in Sections 3.1-3.4 present evidence of policy change and 
improvements in policy quality as a result of capacity development interventions; while one further study 
provides evidence that CoPs did not lead to positive outcomes. Most of these studies did not explicitly 
attempt to measure the extent of policy change or improvements in policy quality as a result of the 
interventions – mainly focusing on measuring improvements in capacity, or changes in behaviour (discussed 
above in Sections 3.1–3.4). The evidence on policy change and policy quality from these studies therefore 
largely consists of ad hoc examples rather than systematically measured outcomes, and so it is not clear how 
representative these examples are of overall project success. The studies also provide little insight into how 
change happened at a policy level, or the contextual and intervention conditions that helped enable change at 
this level. 
The limited evidence available from the six studies is summarised in Table 10, according to whether it relates 
to change in policy processes, policy decisions or actions, and policy outcomes – three aspects of the broad 
definition of policy adopted in this review, and discussed in Section 1.2. (see Hallsworth et al. 2011; Jones 
2009; Cloete & De Coning 2011; Dunn 2012): 
 The quality of policy processes refers to factors such as the efficiency, productivity, scheduling, 
participation and timeliness of the processes used to make decisions and take actions. 
 The quality of policy decisions and actions refers to the internal logic of the theory underpinning the 
decision or action; for example its level of compliance with current knowledge, its relevance, or its 
feasibility. 
 The quality of policy outcomes refers to what happens as a result of a policy decision or action – its 
impacts on different groups of people. 
These three ‘levels’ of policy quality have their limitations. As Section 1.2.4 discussed, defining ‘policy quality’ 
is a challenge as existing definitions are often rational in nature and based on linear conceptions of policy 
processes, which several EIPM sources examined in Section 1.2.1 reject as unrealistic. These definitions are 
therefore viewed as a starting point for understanding ‘policy quality’, which the evaluation team will aim to 
further develop and nuance as the evaluation progresses. 
Table 10. Summary of empirical findings relating to policy change and policy quality 
 Nature of policy change 
Source Policy processes Policy decisions/actions Policy outcomes 
Dobbins, Hanna, et al. 
2009 
 Statistically significant increase in 
evidence-informed decision making 
among a sub-set of health 
organisations
<<<PAGE=81>>>
www.itad.com/knowledge-and-resources/bcure 81 
 
Gabbay et al. 2003  Negative outcome: observational 
evidence that CoPs made 
recommendations that did not make 
full use of available evidence 
 
Nutley et al., 2013   Self-reported improvements 
in the targeting and planning 
of health services among tool 
users 
Jacobs et al. 2014  Examples in survey responses of 
health programmes being selected 
based on evidence 
 
Peirson et al. 2012 Reports of evidence reviews 
being used to inform health 
decision making 
  
Pappaioanou et al. 
2003 
Reports of increased use of 
evidence in health policy 
making 
 Anecdotal example of district 
health officers averting an 
epidemic using new skills 
 
Improvements in policy processes 
Two studies provide evidence that capacity development interventions led to improvements in policy 
processes. One study of a multifaceted EIPM strategy in a Canadian health organisation found evidence that 
progress was being made towards ‘becoming an evidence informed decision making organisation’ – for 
example inclusion of explicit standards and expectations around evidence use in planning processes. The 
study suggested that ‘reviews using the new methods and tools were being completed and used to inform 
decision making’, but does not provide any detail on specifically how evidence was informing policy change 
(Peirson et al. 2012). Another study of the DDM programme in Bolivia, Cameroon, Mexico and the Philippines 
(mainly involving training and mentoring) suggested that the training resulted in improved use of evidence in 
health policy making; although this change was not systematically measured (Pappaioanou et al. 2003). 
However, in both studies the increased use of evidence in policy processes is viewed as a positive end in itself. 
Neither of these studies examine how evidence has improved the quality of processes (for example, by 
making them more efficient, productive or participatory). 
Improvements in policy decisions or actions 
Three studies provide evidence relating to the quality of policy decisions and actions. One study of an EIPM 
training course in the US found that 45% of participants felt that EIPM had increased within their agency since 
completing the training. Examples provided by survey respondents included programmes being selected based 
on evidence (Jacobs et al. 2014). 
Another RCT of a knowledge-broker intervention found a statistically significant increase in evidence-informed 
decision making at follow-up, but only among organisations that had a low initial ‘culture of evidence use’ 
(measured through a staff questionnaire) at baseline. This finding was reached by combining two measures of 
EIPM (Dobbins, Hanna, et al. 2009): 
 The extent to which evidence was considered in a recent planning decision, as reported by staff 
members. 
 The number of evidence-based policies and health interventions that were being implemented pre- 
and post-intervention, out of a list of 11 interventions selected by the evaluation team based on 
systematic review evidence. 
However, implicit in these findings is the assumption that policy decisions and actions are matter-of-factly 
better when they are selected based on evidence; which a range of conceptual literature discussed in Section
<<<PAGE=82>>>
www.itad.com/knowledge-and-resources/bcure 82 
 
1 suggests may be an oversimplification given the messy, political and contested nature of evidence use in 
policy processes. 
Finally, Gabbay et al. (2003) examined the workings of two CoPs in the UK’s National Health Service, 
presenting evidence of a capacity-building intervention that did not result in more evidence-informed 
practice. This study found that ‘the CoPs did not follow the conventional tenets of an evidence-based model 
of practice, despite considerable efforts (e.g. facilitation, agenda structuring, library services) to help them to 
do so.’ Rather, as discussed in Section 3.2 above, personal experience, trust in expert opinion and persuasive 
communication were more important in getting evidence accepted by the group. This ultimately resulted in 
the groups making recommendations that did not make full use of the research evidence available to them. 
 
Improvements in policy outcomes 
 
Two studies provide evidence of improvements in policy outcomes as a result of capacity development 
interventions. However, in both cases the evidence on improved outcomes is fairly thin and anecdotal rather 
than deliberately or systematically measured. 
One study relates to the DDM capacity development project, which largely involved training for health 
decision makers. The study provides an anecdotal example of improved policy outcomes, when Cameroon 
district health officers involved in the training used their new skills to detect an impending meningitis 
epidemic (with the help of visiting DDM consultants) through the analysis of surveillance data. As a result, 
participants averted a large scale epidemic (Pappaioanou et al. 2003). 
 
In another study examining the impact of a tool for health decision making in Kenya, interview respondents 
provided examples of the tool leading to programme improvements. Health staff reported that the tool had 
enabled them to identify trends and problems, resulting in improvements in the targeting and planning of 
services. Specific examples of change included increases in the number of mothers delivering babies at health 
facilities, and increases in the number of staff and testing kits (Nutley et al. 2013). 
 
Summary: in what ways can capacity development interventions promote policy change and 
improvements in policy quality, how, in what circumstances, and why? 
This section draws on six primary intervention studies providing evidence relating to policy change and policy 
quality. However, most of these studies did not explicitly aim to measure these outcomes, and so this 
evidence is sparse and generally ad hoc rather than systematically measured. 
Two studies provide evidence that capacity development lead to improvements in the quality of policy 
processes: in that training resulted in increased use of evidence in decision making. However, both studies 
view evidence use as a positive end in itself, rather than shedding light on how evidence improved the quality 
of processes (for example, by making them more efficient, productive or participatory). 
Three studies provide evidence relating to the quality of policy decisions and actions. Two provided evidence 
that capacity development resulted in an increased number of programmes being based on evidence. 
However, implicit in these findings is the assumption that decisions and actions are inherently better when 
they are selected based on evidence; which Section 1 suggests may be an oversimplification given the messy, 
political and contested nature of evidence use in policy processes. A third study presented less positive 
results, finding that an EIPM intervention involving CoPs ultimately resulted in recommendations that did not 
make full use of the evidence available, because personal experience and group dynamics proved more 
influential than concerns over the objectivity and representativeness of evidence.
<<<PAGE=83>>>
www.itad.com/knowledge-and-resources/bcure 83 
 
Finally, two studies provide evidence of improved policy outcomes as a result of capacity development 
interventions – the averting of an epidemic following EIPM training, and improvements in the targeting and 
planning of health services as a result of using a decision support tool. Again, in both papers the evidence on 
improved policy outcomes is fairly thin and anecdotal rather than deliberately or systematically measured. 
 
3.6. Conclusions and implications for the BCURE evaluation 
This section has investigated what works to build capacity among decision makers for EIPM, for whom, in 
what circumstances, and why. Overall, the evidence on capacity development for EIPM is limited and the 
majority of papers relate to training courses narrowly focused on improving individual skills and capacity. 
Many studies do not explicitly discuss mechanisms, consider contextual factors in any great detail, or provide 
disaggregated information to look at who benefits or fails to benefit from capacity development interventions. 
Despite these limitations and the small evidence base, useful insights can be distilled from the studies 
considered in this section on how and why different interventions may have resulted in (or not resulted in) 
change, and the contextual and intervention factors that helped or hinder programme success. The 
mechanisms identified need further refinement and testing, especially in light of the very small evidence base 
behind certain findings. However, they do provide a useful starting point for the BCURE evaluation, helping to 
identify the potential ways in which BCURE activities might result in change. They may also be of interest to 
other policy makers and practitioners grappling with the challenge of building capacity for EIPM; in helping 
think about not only what types of intervention might be appropriate, but how and why they might work. 
 
The main outcomes, mechanisms, and contextual and intervention factors discussed in this section are 
summarised below. 
 
Individual-level interventions: training. Eleven primary intervention studies and one secondary review provide 
evidence suggesting that professional training can lead to self-reported improvements in individual capacity for 
EIPM, including improvements in individual skills, knowledge and attitudes relating to the access, appraisal 
and use of evidence. However, there are some reliability issues with self-reported measures, and only a few 
studies provided more objective evidence that training influenced EIPM behaviours (for example improving 
decision making or resulting in the completion of an EIPM-related task). 
 
The evidence suggests that training may lead to improvements in capacity through the mechanism of self-
efficacy, by improving participants’ beliefs (or confidence) in their capability to perform a certain task or 
handle a particular situation – although other models of learning may provide valid alternative ways to 
conceptualise the mechanisms at work within training interventions. Combining classroom training with on-
site projects, and actively engaging participants’ organisations, were two intervention features frequently 
linked to training success; especially as supportive organisations seemed to be an important contextual factor 
influencing the impact of training. The risk of other work commitments or lack of time inhibiting changes in 
behaviour may potentially be mitigated by post-training mentoring. 
 
Interpersonal-level interventions: networks. Evidence from four primary intervention studies, two non-
intervention studies and two secondary reviews suggests that networks for EIPM may promote knowledge 
sharing or exchange, although most studies do not discuss whether or how this results in behaviour change. 
Some evidence suggests that networks involve a mechanism of social learning: discussing ideas with 
colleagues providing the opportunity for people to be influenced by others. There is little detail on 
intervention or contextual factors that might make networks successful, although supportive management 
and the input of external experts or senior individuals may encourage people to participate.
<<<PAGE=84>>>
www.itad.com/knowledge-and-resources/bcure 84 
 
In providing opportunities for participants to interact, two studies suggest that networks may also result in 
individuals’ beliefs shifting towards a consensus, through the mechanism of social processing. However, one 
study finds that social processing does not necessarily lead to improved use of evidence; it may in fact result 
in evidence being collectively ‘misinterpreted’ by networks, resulting in non-evidence-based 
recommendations. In this study, evidence was more likely to be accepted and processed if it chimed with 
existing experience, was relayed by an expert, or was communicated by someone with good interpersonal 
skills. 
Interpersonal-level interventions: knowledge brokers. KBs play a formal (usually paid) role in connecting 
decision makers with research and research producers. Two primary intervention studies and several 
secondary reviews suggested that KBs can increase individual or organisational capacity and promote 
behaviour change. Both primary studies imply that KBs may influence change through the mechanism of 
cheerleading, stimulating and maintaining staff and managerial enthusiasm for EIPM. One study also suggests 
KBs may work through promoting self-efficacy either through formal training or informal encouragement. The 
literature suggests a number of skills and qualities that a good KB should possess, including the ability to 
quickly pick up evidence and provide objective guidance that takes into account wider policy agendas; skills 
and knowledge in teaching, EIPM and the technical field in question; and interpersonal skills such as respect, 
leadership, positivity and responsibility. In terms of contextual factors, organisational support was highlighted 
as crucial by both primary studies. Although successful KBs are able to build managerial support, an initial 
level of buy-in appears to be important. 
Interpersonal-level interventions: champions. Champions are people embedded within an organisation or 
institutional context, who (formally or informally) promote EIPM practices. Evidence from two primary 
intervention studies, four non-intervention studies and two secondary reviews examined the role played by 
champions in promoting EIPM. These suggest that champions can help improve use of evidence within 
organisations or institutional environments through (at least) three different mechanisms: 
1. Transformational leaders may mobilise support for change within an organisation, including through 
securing resources for EIPM. Champions’ seniority and vision, commitment, and dedication seem to 
be important here, along with their stability and continuity within an organisation. 
2. Two secondary reviews suggest that champions may also work through social learning as they ‘role 
model’ particular EIPM behaviours that others follow, or lead opinion in new directions. In this case 
the seniority of champions may not necessarily be as important – with peers potentially playing this 
role as well as leaders. 
3. Finally, one study suggests that champions may act as network facilitators, developing coalitions 
between different groups or individuals around particular issues. This study found that network 
facilitation is affected by the institutional location of champions and the wider political environment, 
which influence the kinds of networking strategies champions can successfully employ. 
Organisational interventions: tools and systems. Five primary intervention studies, three non-intervention 
studies and three secondary reviews provided evidence to suggest that tools and systems can improve 
individual capacity and use of evidence, for example by improving data analysis, review and interpretation. 
Four potential mechanisms may help explain the influence of tools and systems: 
1. Two studies suggest that they may facilitate staff to adopt EIPM behaviours, through providing 
resources and processes that enable and support them to change their behaviour, or make their jobs 
easier. These studies suggest the importance of tools being relevant and timely, and having simple 
and clear instructions. Low levels of skills and limited technological infrastructure can constrain the
<<<PAGE=85>>>
www.itad.com/knowledge-and-resources/bcure 85 
 
successful use of tools, particularly in low-income contexts – suggesting the importance of capacity 
support. 
2. Two studies found that tools may also increase the value staff place on evidence, for example 
through deepening their understanding of the benefits data can bring to decision making. 
3. Two studies suggest that systems may reinforce EIPM behaviours through positive means (rewards) 
or negative ones (e.g. audit and risk of negative feedback) – although using systems to reinforce 
behaviour may also create perverse incentives to recycle evidence or use it in a political or tactical 
way, particularly if systems are time consuming or bureaucratic. 
4. Finally, two studies imply that tools and systems may promote self-efficacy – for example by 
increasing staff confidence in their ability to successfully appraise evidence, or more subtly by helping 
to permeate the language of EIPM throughout an organisation and make it an accepted part of the 
culture. 
Evidence on policy change and improvements in policy quality: The studies discussed in this section 
predominantly discuss how far interventions improved capacity or led to behaviour change around evidence 
use. Evidence on policy change and policy quality as a result of increased evidence use is fairly thin and 
anecdotal rather than deliberately or systematically measured. In total, five primary intervention studies 
provide evidence that capacity development interventions resulted in improved policy processes, policy 
decisions and actions, and/or policy outcomes. However, these studies tend to view evidence use as a positive 
end in itself, rather than shedding light on how evidence improved the quality of processes. Similarly, implicit 
in these findings is the assumption that decisions and actions are inherently better when they are selected 
based on evidence; which Section 1 suggests may be an oversimplification given the messy, political and 
contested nature of evidence use in policy processes. 
References 
Abeysinghe, S., 2012. “Because we all know that vaccines are an 
extremely effective public health tool”: Path Dependency, 
H1N1 and the World Health Organisation. Policy Studies, 
33(5), pp.381–397.  
Armstrong, R. et al., 2013. Knowledge translation strategies to 
improve the use of evidence in public health decision 
making in local government: intervention design and 
implementation plan. Implementation science : IS, 8(1), 
p.121. 
Bandura, A., 1988. Organizational applications of social cognitive 
theory. Australian Journal of Management, 13(2), 
pp.275–302. 
Bandura, A., 1977. Self-efficacy: Toward a unifying theory of 
behavioral change. Psychological Review, 84(2), pp.191–
215. 
Baser, H. & Morgan, P., 2008. Capacity, Change and 
Performance, Maastricht. Available at: 
www.ecdpm.org/capacitystudy. 
Beck, M., Asenova, D. & Dickson, G., 2005. Public 
Administration, Science, and Risk Assessment: A Case 
Study of the U.K. Bovine Spongiform Encephalopathy 
Crisis. Public Administration Review, 65(4), pp.396–408. 
Best, A. & Holmes, B., 2010. Systems thinking, knowledge and 
action: towards better models and methods. Evidence & 
Policy: A Journal of Research, Debate and Practice, 6(2), 
pp.145–159. 
Boswell, J., 2014. “Hoisted with our own petard”: evidence and 
democratic deliberation on obesity. Policy Sciences, 47(4), 
pp.345–365.  
Broadbent, E., 2012. Politics of research-based evidence in 
African policy debates: Synthesis of case study findings. 
EBPDN and MWANANCHI, (June). 
Brookfield, S.D., 1985. Self-Directed Learning: A Conceptual and 
Methodological Exploration. In Studies in the Education of 
Adults. pp. 19–32. 
Brown, C., 2012. The “policy-preferences model”: a new 
perspective on how researchers can facilitate the take-up 
of evidence by educational policy makers, 8(4), pp.455–
472. 
Bryan, R.L., Kreuter, M.W. & Brownson, R.C., 2009. Integrating 
adult learning principles into training for public health 
practice. Health promotion practice, 10(4), pp.557–563.  
Cabinet Office, 1999. Professional Policy Making for the Twenty 
First Century, London. Available at: 
http://dera.ioe.ac.uk/6320/1/profpolicymaking.pdf. 
Cabinet Office, 2002. Supplementary Green Book Guidance: 
Optimism Bias, London. Available at: 
https://www.gov.uk/government/publications/green-
book-supplementary-guidance-optimism-bias. 
Capacity.org, People Matter: An introduction to capacity 
development. Available at:
<<<PAGE=86>>>
www.itad.com/knowledge-and-resources/bcure 86 
 
http://www.capacity.org/capacity/opencms/en/topics/int
roduction-to-cd/. 
Carden, F., 2009. Knowledge to policy: making the most of 
development research, New Delhi: SAGE Publications. 
Clar, C. et al., 2011. Systematic Review: What are the effects of 
interventions to improve the uptake of evidence from 
health research into policy in low and middle-income 
countries? Final report to DFID. Available at: 
http://r4d.dfid.gov.uk/PDF/Outputs/SystematicReviews/S
R_EvidenceIntoPolicy_Graham_May2011_MinorEditsJuly
2011.pdf. 
Cloete, F. & De Coning, C., 2011. Improving Public Policy: 
Theory, Practice and Results 3rd ed. V. Schaik, ed., 
Pretoria. 
Collins, M., 1988. Self-directed learning or an emancipatory 
practice of adult education: Re-thinking the role of the 
adult educator. In Proceedings of the 29th Annual Adult 
Education Research Conference. Calgary: Faculty of 
Continuing Education, University of Calgary. 
Court, J. & Young, J., 2003. Bridging Research and Policy: Insights 
from 50 Case Studies, ODI, London. 
Davies, P., 2013. Getting Evidence Into Policy. 3ie-LIDC Seminar 
Series, (February). Available at: 
http://www.3ieimpact.org/media/filer_public/2013/04/2
9/3ie-lidc_presentation_200213.pdf. 
Davies, R. et al., 2012. Evaluability Assessment For DFID’s 
Empowerment and Accountability and Gender Teams, 
Bristol. 
Deans, F. & Ademokun, A., 2011. Investigating capacity to use 
evidence: Time for a more objective view ? INASP, 1, 
pp.1–4. 
DFID, 2013a. BCURE Evaluation Terms of Reference (Internal 
DFID), London. 
DFID, 2013b. DFID Evidence Survey Results Report, Available at: 
http://r4d.dfid.gov.uk/Output/194821/. 
DFID, 2014a. How To Note on Assessing the Strength of 
Evidence, Available at: 
https://www.gov.uk/government/publications/how-to-
note-assessing-the-strength-of-evidence. 
DFID, 2014b. Rising to the challenge : supporting “problem 
driven iterative adaptation” and “politically smart, locally 
led” approaches through a donor-funded programme. 
The experience of the State Accountbaility and Voice 
Initiative in Nigeria, Available at: http://savi-
nigeria.org/wp-
content/uploads/2014/10/DFID_SAVI_brief_Challenge_O
nline.pdf. 
DFID, 2012. Strengthening use of evidence in policy and 
practice: Research uptake programme and 
research/evaluation call (BCURE Business Case)., Available 
at: http://devtracker.dfid.gov.uk/projects/GB-1-
203778/documents/. 
Dobbins, M., Robeson, P., et al., 2009. A description of a 
knowledge broker role implemented as part of a 
randomized controlled trial evaluating three knowledge 
translation strategies. Implementation Science, 4, p.23.  
Dobbins, M., Hanna, S.E., et al., 2009. A randomized controlled 
trial evaluating the impact of knowledge translation and 
exchange strategies. Implementation science, 4, p.61.  
Dunn, L., 2002. Theories of Learning. Learning and Teaching 
Briefing Paper Series, Oxford Centre for Staff and 
Learning Development, (Oxford Brookes University). 
Dunn, W., 2012. Public Policy Analysis, Englewood Cliffs: 
Prentice Hall. 
Eden, D. & Avirma, A., 1993. Self-efficacy training to speed 
reemployment: Helping people help themselves. Journal 
of Applied Psychology, 78, pp.352–360. 
El-Jardali, F. et al., 2014. A retrospective health policy analysis of 
the development and implementation of the voluntary 
health insurance system in Lebanon: Learning from 
failure. Social Science & Medicine, 123, pp.45–54.  
Evans, D., 2003. Hierarchy of evidence: a framework for ranking 
evidence evaluating healthcare interventions. Journal of 
clinical nursing, 12(1), pp.77–84.  
FAO, 2010. Capacity Development Learning Module 1, Available 
at: http://www.fao.org/capacitydevelopment/en/. 
Fishbein, M. & Middlestadt, S., 1994. Noncognitive effects on 
attitude formation and change: Fact or artifact? Journal 
of Consumer Psychology, 4, pp.181–202. 
Flitcroft, K. et al., 2011. Getting evidence into policy: The need 
for deliberative strategies? Social Science & Medicine, 
72(7), pp.1039–1046.  
Freire, P., 1972. Pedagogy of the Oppressed, London: Penguin. 
Gabbay, J. et al., 2003. A Case Study of Knowledge Management 
in Multiagency Consumer-Informed `Communities of 
Practice’: Implications for Evidence-Based Policy 
Development in Health and Social Services. Health:, 7(3), 
pp.283–310.  
Gagliardi, A.R. et al., 2014. Exploring mentorship as a strategy to 
build capacity for knowledge translation research and 
practice: a scoping systematic review. Implementation 
Science, 9(1), p.122.  
Greenhalgh, T. et al., 2004. Diffusion of innovations in service 
organizations: systematic review and recommendations. 
The Milbank quarterly, 82(4), pp.581–629.  
Greenhalgh, T., 2003. Transferability of principles of evidence 
based medicine to improve educational quality: 
systematic review and case study of an online course in 
primary health care. BMJ, 326(7381), pp.142–145.. 
Hallsworth, M., Parker, S. & Rutter, J., 2011. Policy Making in the 
Real World, Available at: 
http://www.instituteforgovernment.org.uk/sites/default/f
iles/publications/Policy making in the real world.pdf. 
Hallsworth, M. & Rutter, J., 2011. Making Policy Better: 
Improving Whitehall’s core business, Available at: 
http://www.instituteforgovernment.org.uk/sites/default/f
iles/publications/Making Policy Better.pdf. 
Haynes, A.S. et al., 2011. Galvanizers, Guides, Champions, and 
Shields: The Many Ways That Policymakers Use Public 
Health Researchers. The Millbank Quarterly, 89(4), 
pp.564–598.
<<<PAGE=87>>>
www.itad.com/knowledge-and-resources/bcure 87 
 
Hufen, J. a. M. & Koppenjan, J.F.M.M., 2014. How evidence 
becomes authoritative in public policy implementation. 
Lessons from three Dutch white ravens. Policy Studies, 
35(3), pp.264–281.  
Hunsmann, M., 2012. Limits to evidence-based health 
policymaking: policy hurdles to structural HIV prevention 
in Tanzania. Social Science & Medicine, 74(10), pp.1477–
1485. 
ICAI, 2014. How DFID Learns, Report 34. Independent 
Commision for Aid Impact. 
IMF, 2003. Data Quality Assessment: Generic Framework, Fifth 
Review of the Fund’s Data Standards Initiatives, IMF. 
Innvaer, S. et al., 2002. Health policy-makers’ perceptions of 
their use of evidence: a systematic review. Journal of 
health services research & policy, 7(4), pp.239–244.  
Jacobs, J. a et al., 2014. Capacity building for evidence-based 
decision making in local health departments: scaling up 
an effective training approach. Implementation science, 
9(1), p.124.  
Jones, H. et al., 2013. Knowledge, policy and power in 
international development: a practical framework for 
improving policy, ODI. 
Jones, H., 2009. Policy-making as discourse : a review of recent 
knowledge-to-policy literature, IKM-ODI. 
Jones, N., 2011. Involving legislators in evidence-informed policy 
processes: a neglected part of the democratic governance 
agenda, ODI. 
Jones, N., Datta, A. & Jones, H., 2009. Knowledge, policy and 
power: six dimensions of the knowledge-development 
policy interface, ODI: RAPID. 
Jones, N. & Pellini, A., 2009. Evidence-Informed Policy in Post 
Conflict Contexts: Nepal, Peru and Serbia, Discussion 
Paper 17, Oslo Governance Centre: UNDP. 
Jung, T. & Nutley, S.M., 2008. Evidence and policy networks: the 
UK debate about sex offender community notification. 
Evidence & Policy, 4(2), pp.187–207. 
Kaplan, A., 1999. The Developing of Capacity, Available at: 
http://institutofonte.org.br/sites/default/files/Kaplan 
A_The Developing Of Capacity.pdf. 
Kirkpatrick Partners, The New World Kirkpatrick Model. 
www.kirkpatrickpatners.com. Available at: 
http://www.kirkpatrickpartners.com/OurPhilosophy/The
NewWorldKirkpatrickModel/tabid/303/Default.aspx. 
Knowles, M.S., 1975. Self-directed learning, New York: 
Associated Press. 
Knowles, M.S., Holton, E.F. & Swanson, R.A., 2005. The Adult 
Learner: The Definitive Classic in Adult Education and 
Human Resource Development, Elsevier.  
Lasswell, H., 1977. The Politics of Prevention. In 
Psychopathology and Politics. Chicago: University of 
Chicago Press. 
Lave, J. & Wenger, E., 1991. Situated Learning. Legitimate 
peripheral participation, Cambridge: University of 
Cambridge Press. 
Lindblom, C., 1959. The Science of “Muddling Through.” Public 
Administration Review, 19(2), pp.79–88. 
Liverani, M., Hawkins, B. & Parkhurst, J.O., 2013. Political and 
institutional influences on the use of evidence in public 
health policy. A systematic review. PloS one, 8(10). 
Lyon, A.R. et al., 2011. Developing the mental health workforce: 
review and application of training approaches from 
multiple disciplines. Administration and policy in mental 
health, 38(4), pp.238–253.  
Mallett, R. et al., 2012. The benefits and challenges of using 
systematic reviews in international development 
research. Journal of Development Effectiveness, 4(3), 
pp.445–455. 
Matovu, J.K.B. et al., 2013. Strengthening health workforce 
capacity through work-based training. BMC international 
health and human rights, 13(1), p.8.  
McCormack, B. et al., 2013. A realist review of interventions and 
strategies to promote evidence-informed healthcare: a 
focus on change agency. Implementation science, 8(1), 
p.107.  
Merriam, S.B., 2001. Andragogy and Self-Directed Learning: 
Pillars of Adult Learning Theory. New Directions for Adult 
and Continuing Education, 2001(89), p.3.  
Morton, S., 2012. Exploring and Assessing Research Impact. PhD 
Thesis, University of Edinburgh. 
Murphy, P., 1999. Learners, Learning and Assessment, London: 
Paul Chapman. 
Newman, K., 2014. What is the evidence on the impact of 
research on international development?, London: 
Department for International Development (DFID). 
Available at: 
http://r4d.dfid.gov.uk/pdf/outputs/Misc_EcoDev/impact-
of-research-on-international-development.pdf. 
Newman, K., Fisher, C. & Shaxson, L., 2012. Stimulating Demand 
for Research Evidence: What Role for Capacity-building? 
IDS Bulletin, 43(5), pp.17–24.  
NICE, 2014. Developing NICE guidelines: the manual. Reviewing-
research-evidence: Guidance and guidelines. Available at: 
http://www.nice.org.uk/article/pmg20/chapter/reviewing
-research-evidence. 
Nickerson, R.S., 1998. Confirmation bias: A ubiquitous 
phenomenon in many guises. Review of General 
Psychology, 2(2), pp.175–220.  
Nisbett, N. et al., 2014. What are the Factors Enabling and 
Constraining Effective Leaders in Nutrition? A Four 
Country Study, IDS Working Paper 447. 
Nutley, S., Walter, I. & Davies, H.H.T., 2007. Using Evidence: 
How Research Can Inform Public Services, Bristol: The 
Policy Press. 
Nutley, S.M., Davies, H. & Walter, I., 2002. Evidence Based Policy 
and Practice: Cross Sector Lessons From the UK, 
Nutley, T., McNabb, S. & Salentine, S., 2013. Impact of a 
decision-support tool on decision making at the district 
level in Kenya. Health research policy and systems / 
BioMed Central, 11(1), p.34.
<<<PAGE=88>>>
www.itad.com/knowledge-and-resources/bcure 88 
 
OECD-DAC, 2006. The Challenge of Capacity Development: 
Working towards good practice, Available at: 
http://www.oecd.org/development/governance-
development/36326495.pdf. 
Oliver, K., Innvar, S., et al., 2014. A systematic review of barriers 
to and facilitators of the use of evidence by policymakers. 
BMC health services research, 14(1), p.2.  
Oliver, K., 2012. Evaluating power, influence and evidence-use in 
public health policy-making: A social network analysis. 
PhD Thesis, University of Manchester. 
Oliver, K., Lorenc, T. & Innvær, S., 2014. New directions in 
evidence-based policy research: a critical analysis of the 
literature. Health research policy and systems / BioMed 
Central, 12, p.34.  
Orton, L. et al., 2011. The use of research evidence in public 
health decision making processes: systematic review. PloS 
one, 6(7), p.e21704.  
Pappaioanou, M. et al., 2003. Strengthening capacity in 
developing countries for evidence-based public health: 
Social Science & Medicine, 57(10), pp.1925–1937.  
Parkhurst, J.O., 2014. What Constitutes “Good” Evidence for 
Public Health and Social Policy Making? From Hierarchies 
to Appropriateness. Social Epistemology Review and 
Reply Collective, 3(10), pp.34–46. 
Pawson, R., 2006a. Evidence-based Policy: A Realist Perspective, 
London: SAGE Publications. 
Pawson, R., 2004. Mentoring relationships: An explanatory 
review, ESRC UK Centre for Evidence Based Policy and 
Practice, Leeds. 
Pawson, R., 2006b. Realist Evaluation and Realist Synthesis. 
Available at: 
www.s3ri.soton.ac.uk/qmss/documents/NicosiaRaymond
Pawson.ppt. 
Pawson, R. et al., 2004. Realist synthesis: an introduction, 
University of Manchester: ESRC Research Methods 
Programme. 
Pawson, R., 2006c. Realist synthesis: new protocols for 
systematic review. In Evidence-Based Policy. Sage, pp. 1–
26. Available at: 
http://www.coris.uniroma1.it/news/files/Pawson_Realists
ynthesis_chp4.pdf. 
Pawson, R. & Tilley, N., 1997. Realistic Evaluation, London: Sage. 
Peirson, L. et al., 2012. Building capacity for evidence informed 
decision making in public health: a case study of 
organizational change. BMC public health, 12(1), p.137.  
Pellini, A. et al., 2013. Towards policy-relevant science and 
scientifically informed policy: Political economy of the use 
of knowledge and research evidence, ODI. 
Perkin, E. & Court, J., 2005. Networks and Policy Processes in 
International Development: A literature review London, 
Working Paper 252, ODI. 
Pettman, T.L. et al., 2013. Cochrane update: building capacity in 
evidence-informed decision-making to improve public 
health. Journal of public health (Oxford, England), 35(4), 
pp.624–7.  
Pollard, A. & Court, J., 2005. How Civil Society Organisations use 
Evidence to Influence Policy Processes: A literature 
review, ODI, London. 
Porter, S. & Feinstein, O., 2013. Demand for and supply of 
evaluations in selected sub-Saharan African countries, 
CLEAR-AA, Johannesburg. Available at: http://www.clear-
aa.co.za/wp-content/uploads/2013/09/201402-Demand-
and-Supply-Final-Report.pdf. 
Pritchett, L. & Sandefur, J., 2013. Context Matters for Size: Why 
External Validity Claims and Development Practice Don’t 
Mix, Working Paper 336, Center for Global Development. 
Available at: 
http://www.cgdev.org/sites/default/files/context-
matters-for-size_0.pdf. 
Ramalingam, B., 2013. Aid on the Edge of Chaos: Rethinking 
International Cooperation in a Complex World, OUP 
Oxford.  
Reed, M.S. et al., 2010. What is Social Learning ? Ecology and 
Society, 15(4). 
Ritter, A., 2009. How do drug policy makers access research 
evidence? The International journal on drug policy, 20(1), 
pp.70–5.  
Rolle, I. V et al., 2011. Leadership in strategic information (LSI) 
building skilled public health capacity in Ethiopia. BMC 
research notes, 4(1), p.292.. 
Rowe, L. a et al., 2010. Building capacity in health facility 
management: guiding principles for skills transfer in 
Liberia. Human resources for health, 8, p.5.  
Ruiz, F. & Breckon, J., 2014. The NICE Way: Lessons for social 
policy and practice from the National Institute for Health 
and Care Excellence, Available at: 
http://www.alliance4usefulevidence.org/assets/NestaAlli
ance_and_NICE_paper_v5.pdf. 
Sabatier, P. & Jenkins-Smith, H. eds., 1993. Policy Change and 
Learning: An Advocacy Coalition Approach, Westview 
Press. 
Shaxson, L., 2014. Investing in Evidence: Lessons from the UK 
Department for Environment, Food and Rural Affairs, 
Australian Aid Knowledge Sector Initiative Working Paper 
2. 
Smith, K.E. & Joyce, K.E., 2012. Capturing complex realities: 
understanding efforts to achieve evidence-based policy 
and practice in public health. Evidence & Policy: A Journal 
of Research, Debate and Practice, 8(1), pp.57–78.  
Smith, M.K., 2003. “Learning theory”: models, product and 
process. The encyclopedia of informal education. 
Available at: http://infed.org/mobi/learning-theory-
models-product-and-process/. 
Smith, M.K., 1999. The social/situational orientation to learning. 
The encyclopedia of informal education. Available at: 
http://infed.org/mobi/the-socialsituational-orientation-
to-learning/. 
Sumner, A. et al., 2011. What shapes research impact on policy? 
Understanding research uptake in sexual and 
reproductive health policy processes in resource poor 
contexts. Health research policy and systems / BioMed 
Central, 9 Suppl 1(Suppl 1), p.S3.
<<<PAGE=89>>>
www.itad.com/knowledge-and-resources/bcure 89 
 
Sumner, A. & Harpham, T., 2008. The market for “evidence” in 
policy processes: the case of child health policy in Andhra 
Pradesh, India and Viet Nam. The European Journal of 
Development Research, 20(4), pp.712–732.  
Sutcliffe, S. & Court, J., 2005. Evidence-Based Policymaking : 
What is it? How does it work? What relevance for 
developing countries?, ODI. 
Tang, K.-C. et al., 2005. Building capacity for health promotion--a 
case study from China. Health promotion international, 
20(3), pp.285–295.  
Du Toit, A., 2012. Making Sense of “Evidence”: Notes on the 
Discursive Politics of Research and Pro-Poor Policy 
Making, Working Paper 21, Institute for Poverty, Land and 
Agrarian Studies (PLAAS). Available at: 
http://www.plaas.org.za/plaas-publication/wp21dutoit . 
Tomatis, C. et al., 2011. Evidence-based medicine training in a 
resource-poor country, the importance of leveraging 
personal and. Journal of Evaluation in Clinical Practice, 17, 
pp.644–650. 
Tough, A., 1967. Learning Without a Teacher, Educational 
Research Series. Toronto: Ontario Institute for Studies in 
Education. 
Tough, A., 1971. The Adult’s Learning Projects: A Fresh 
Approach to Theory and Practice in Adult Learning, 
Toronto: Ontario Institute for Studies in Education. 
Traynor, R., Decorby, K. & Dobbins, M., 2014. Knowledge 
brokering in public health: a tale of two studies. Public 
Health, 128(6), pp.533–544.  
Trostle, J., Bronfman, M. & Langer, A., 1999. How do researchers 
influence decision-makers? Case studies of Mexican 
policies. Health policy and planning, 14(2), pp.103–14.  
Ubels, J., Acquaye-Baddoo, N. & Fowler, A. eds., 2010. Capacity 
development in practice, London: Earthscan. Available at: 
http://www.snvworld.org/sites/www.snvworld.org/files/p
ublications/capacity_development_in_practice_-
_complete_publication.pdf. 
UK Civil Service, 2010. Policy Skills Framework, UK Civil Service. 
Available at: http://www.civilservice.gov.uk/wp-
content/uploads/2011/08/Policy-Skills-Framework.pdf. 
Uneke, C.J. et al., 2011. Enhancing health policymakers’ capacity 
to use information and communication technology in 
Nigeria. Journal of Health Informatics in Developing 
Countries, pp.228–246. 
Uneke, C.J., Ezeoha, A.E. & Ndukwe, C.D., 2012a. Enhancing 
Leadership and Governance Competencies to Strengthen 
Health Systems in Nigeria: Assessment of Organizational 
Human Resources Development. Healthcare Policy, 7(3). 
Uneke, C.J., Ezeoha, A.E. & Ndukwe, C.D., 2012b. Promotion of 
evidence-informed health policymaking in Nigeria : 
Bridging the gap between researchers and policymakers. 
Global Public Health: An International Journal for 
Research, Policy and Practice, 7(7), pp.37–41. 
Uneke, C.J.J. et al., 2011. Individual and organisational capacity 
for evidence use in policy making in Nigeria: an 
exploratory study of the perceptions of Nigeria health 
policy makers. Evidence & Policy: A Journal of Research, 
Debate and Practice, 7(3), pp.251–276.  
USAID, 2012. Data Quality Assessment Checklist and 
Recommended Procedures, Available at: 
https://usaidlearninglab.org/sites/default/files/resource/f
iles/Data Quality Assessment Checklist.pdf. 
De Vibe, M., Hovland, I. & Young, J., 2002. Bridging Research and 
Policy: An Annotated Bibliography. ODI Working Paper 
174, p.76 S. Available at: 
http://www.odi.org.uk/RAPID/Publications/Documents/w
p174.pdf. 
Waldman, T., 2014. The Use of Statebuilding Research in Fragile 
Contexts: Evidence from British Policymaking in 
Afghanistan, Nepal and Sierra Leone. Journal of 
Intervention and Statebuilding, 8(2-3), pp.149–172.  
Wallace, J. et al., 2012. Making evidence more wanted: A 
systematic review of facilitators to enhance the uptake of 
evidence from systematic reviews and meta-analyses. 
International Journal of Evidence Based Healthcare, 10, 
pp.338–346.  
Walter, I., Nutley, S. & Davies, H., 2005. What works to promote 
evidence-based practice? A cross-sector review. Evidence 
& Policy: A Journal of Research, Debate and Practice, 1(3), 
pp.335–364.  
Waqa, G. et al., 2013. Knowledge brokering between 
researchers and policymakers in Fiji to develop policies to 
reduce obesity: a process evaluation. Implementation 
science, 8(1), p.74.  
Weiss, C.H., 1980. Knowledge Creep and Decision Accretion. 
Science Communication, 1(3), pp.381–404.  
Weiss, C.H., 1982. Policy Research in the Context of Diffuse 
Decision Making. Journal of Higher Education, 53(6), 
p.619.  
Weiss, C.H., 1979. The Many Meanings of Research Utilization. 
Public Administration Review, 39(9), pp.426–431.  
Wesselink, A., Colebatch, H. & Pearce, W., 2014. Evidence and 
policy: discourses, meanings and practices. Policy 
Sciences, 47(4), pp.339–344. 
World Bank, 2015a. World Bank Country and Lending Group. 
data.worldbank.org. Available at: 
http://data.worldbank.org/about/country-and-lending-
groups. 
World Bank, 2015b. World Development Report 2015: Mind, 
Society, and Behavior, Available at: 
http://go.worldbank.org/HYR6FHEK60. 
Yost, J. et al., 2014. Tools to support evidence-informed public 
health decision making. BMC public health, 14(1), p.728.
<<<PAGE=90>>>
www.itad.com/knowledge-and-resources/bcure 90 
 
Annex 1. Full search strategy and search terms 
This review used an iterative search strategy, with new searches (using new and revised search terms) conducted as 
understanding grew about particular theories, and as new theories were uncovered in the literature. A paper was 
deemed relevant if it was judged to contribute to our understanding of the BCURE Theory of Change: did it provide 
evidence to support, challenge or further articulate the outcomes, the hypothesised causal links, or the assumptions?  
 
Relevant thematic domains: We focused our search on academic fields above and beyond the international development 
literature that we believed, based on our initial reviews during the proposal stage, may contain useful information for the 
literature review. These fields included: Political Science (civil society and accountability, political processes and systems, 
institutional reform); Development Studies (politics of aid, good governance); Public Administration of Central and Local 
Governments (including institutional design and reform performance management, with a focus on selected key 
initiatives to institute new practices, such as gender mainstreaming, and use of research evidence and data; Health 
Systems (policy and management); Medical Sciences (management, education); Adult Education and Training (including 
capacity development to increase use of research evidence); and Behaviour Science and Social Marketing (decision 
making theory, diffusion of innovation, influencing behaviours). 
 
We targeted this literature through tailored search terms, searches of relevant databases, and by including experts from 
these fields in our snowball search strategy. We did not exclude evidence from other fields if it was identified through 
structured or snowball searches; but we did not explicitly look for it. 
 
Structured searches: The following search terms were used for the Boolean and smart-searches: 
 
Research question 
(Words in bold and their synonyms 
are also search terms) 
Search terms 
(There is considerable overlap between RQs, so many terms are only listed once. Searches 
were conducted using both British and American spellings) 
1. What factors can promote 
and constrain evidence-
informed policy making in 
public sector 
environments? 
Research, evaluation, data, evidence use, culture, decision making, stud*, policy 
making, evidence-based management, evidence(-)informed policy making/ EIPM, 
performance/ performance data, support, block, challenge, imped*, “public 
sector”, “civil service”, government, politician, “decision maker”, “health policy”, 
“health systems”, “healthcare systems”, “public health” “social determinants of 
health,” “health promotion”, Implementation science, results- based management, 
RBM, ‘technical assistance’ + evidence 
2. What factors lead to 
professional skills and/or 
knowledge being acquired 
by public sector workers 
through teaching and 
training? 
Learn*, teach*, train*, pedagog*, skills, edu* adult, work, workplace, “professional 
development”, “adult learning”, cognition, “self-efficacy”, Life long learning, critical 
appraisal, critical appraisal training, transformative, catalysing, organisation 
development, tipping points 
3. How and in what 
circumstances can 
capacity development 
interventions promote 
individual behaviour 
change within 
organisations? 
Staff, work*, motivat*, programme, project, initiative, application, “adult learning”, 
vocational training, mentor*, “on-the-job”, behaviour, “behaviour change”, 
productive*, sustained, change, success, confidence, capabil*, belief, attitude, 
commitment, champion, incentiv* 
4. How and in what 
circumstances can 
capacity development 
interventions promote 
organisational, network 
and institutional change? 
Culture, “culture change” leadership, management, support, change management, 
innovation, influence, reform, “performance management”, complex* system*, 
“organisations for the 21st century”, “good governance”, “networked governance”, 
engage*, interact*, relationship, system, proced*, budget*, policy, guideline, 
“quality assurance”, competency, media, press, “civil society”, “enabling 
environment”, “the public”, “general public”, “organisational transformation”, 
Learning culture, champion, leadership, network, dialogue, relationships 
5. How and in what 
circumstances can 
capacity development 
interventions increase the 
“Evidence literacy”, “decision making”, decision, “use of evaluation*”, impact,  
access, apprais*, appl*, “evidence use” “policy entrepreneur”
<<<PAGE=91>>>
www.itad.com/knowledge-and-resources/bcure 91 
 
demand for and use of 
evidence in policy making? 
6. How and in what 
circumstances can 
interventions that increase 
demand for and use of 
research evidence lead to 
improved policy quality? 
“Policy design”, implement*, program*, project, initiative, success, “value for 
money”, “policy process” “policy content”, “technical assistan*”  
 
 Boolean searches were developed based on these search terms; for example (for RQ 1):  (Evidence OR research OR 
stud* OR evaluation) AND (use OR decision OR management OR policy) AND (“public sector” OR “civil serv*” OR 
government). After first searching for these terms ‘anywhere in the document’, it was found that this generated too 
many hits to be screened in the time available for the reivew. We therefore changed our strategy to search for the 
terms only in ‘title’ or ‘abstract’ data fields. In the cases where this did not generate sufficient results, the search was 
expanded to ‘all fields’. 
 
 Particularly fruitful search strings included: 
o “professional development” + “evidence base*” + decision making 
o capacity development + “evidence base*” + decision making 
o title: capacity development + public sector 
o title: “professional development” + public sector 
o title: public sector+ learn OR learning 
o title: capacity development +evidence 
o evidence literacy, decision making, decision