<<<PAGE=1>>>
The Foundation Review The Foundation Review 
a publication of the Dorothy A. Johnson Center for Philanthropy at Grand Valley State University a publication of the Dorothy A. Johnson Center for Philanthropy at Grand Valley State University 
Volume 9 Issue 4 
12-2017 
Considerations for Measuring the Impact of Policy-Relevant Considerations for Measuring the Impact of Policy-Relevant 
Research Research 
Megan Collado 
AcademyHealth 
Lauren Gerlach 
AcademyHealth 
Caroline Ticse 
AcademyHealth 
Katherine Hempstead 
Robert Wood Johnson Foundation 
Follow this and additional works at: https://scholarworks.gvsu.edu/tfr 
 Part of the Nonprofit Administration and Management Commons, Public Administration Commons, 
Public Affairs Commons, and the Public Policy Commons 
Recommended Citation Recommended Citation 
Collado, M., Gerlach, L., Ticse, C., & Hempstead, K. (2017). Considerations for Measuring the Impact of 
Policy-Relevant Research. The Foundation Review, 9(4). https://doi.org/10.9707/1944-5660.1386 
Copyright © 2018 Dorothy A. Johnson Center for Philanthropy at Grand Valley State University. The Foundation 
Review is reproduced electronically by ScholarWorks@GVSU. https://scholarworks.gvsu.edu/tfr
<<<PAGE=2>>>
The Foundation Review  //  2017  Vol 9:4    41
Tools
Measuring the Impact of Research
Considerations for Measuring the Impact 
of Policy-Relevant Research 
Megan Collado, M.P.H., Lauren Gerlach, M.P.P., and Caroline Ticse, B.A., AcademyHealth; and 
Katherine Hempstead, Ph.D., Robert Wood Johnson Foundation 
Keywords: Impact, assessment, evaluation, measurement, framework, grantmaking, health insurance coverage, 
policy
Introduction
Philanthropy, and the research and analysis 
it supports, has an important role to play in 
informing policy and making government more 
effective. Indeed, the gold standard for many 
researchers and the funders who support them 
is the ability to produce research findings that 
inform policymaking or contribute to policy 
change. Yet all too often, foundations and other 
research funders struggle to understand whether 
and how their investments have affected pol-
icy, a challenge that is compounded by time 
lags between research output and recognized 
impact, a lack of clear standards for impact mea-
surement, and the simple fact that many factors 
beyond research influence policy decisions. 
Even the most esteemed foundations are not 
immune to this challenge. The Robert Wood 
Johnson Foundation (RWJF) is the largest 
philanthropy in the United States dedicated 
solely to health. In 2014, it announced a new 
vision to build a national “culture of health” 
— a culture in which everyone in America has 
the opportunity to lead a healthier life (RWJF, 
2017). One critical component to this vision is 
the belief that good health is promoted through 
access to high-quality health care and affordable 
health insurance coverage. Over several decades, 
the RWJF has invested in numerous programs 
and projects to identify gaps in health insur -
ance coverage and support enrollment in health 
insurance across the country. 
In 2015, the RWJF asked AcademyHealth, a lead -
ing national organization for health services 
and policy research, to conduct a pilot project 
focused on a subset of the foundation’s research 
Key Points
 • P hilanthropy, and the research and analysis 
it supports, has an important role to play in 
informing policy and making government 
more effective. Yet all too often, foundations 
and other research funders struggle to 
understand whether and how their invest-
ments have affected policy. 
 • T
his article highlights the findings of an 
18-month pilot project conducted by Acade -
myHealth to help the Robert Wood Johnson 
Foundation better understand the impact 
of a subset of the foundation’s research 
grants, across investment types, on health 
insurance coverage and health reform, and 
to help inform how the foundation may more 
systematically track and measure the impact 
of the research it funds. 
 • T
his pilot was unique in that it sought to 
formulate practical recommendations for 
how foundation staff might collect, organize, 
and interpret key measures of policy impact 
on an ongoing basis, particularly when 
working with limited time and resources. 
This article focuses on insights that may be 
of interest to other foundations seeking to 
measure the policy impact of their research 
investments.
investments on health insurance coverage and 
health reform. Specifically, the purpose of the 
pilot was twofold: (1) to help the RWJF better 
understand the impact of a subset of grants 
across investment types, and 2) to help inform 
how the foundation may more systematically 
track and measure the impact of the research it 
doi: 10.9707/1944-5660.1386
<<<PAGE=3>>>
42    The Foundation Review  //  thefoundationreview.org
Tools
Collado, Gerlach, Ticse, and Hempstead
funds. Like many other foundations, the RWJF 
conducts regular program evaluations, but this 
pilot was unique in that it sought to formulate 
practical recommendations for how foundation 
staff might collect, organize, and interpret key 
measures of policy impact on an ongoing basis, 
particularly when working with limited time 
and resources.
In this article we highlight findings from the 
18-month pilot project, with particular attention 
to insights that may be of interest to other foun -
dations. While the focus of the AcademyHealth 
pilot and this article is on the impact of 
health-focused research investments, we think 
many of the observations will be relevant to pol-
icy-oriented research investments across sectors. 
We begin with a brief discussion of research-im -
pact assessment, a growing area of work that 
seeks to use rigorous methodological approaches 
to understand the impact of research findings 
within academia and on society. We then turn 
to the AcademyHealth pilot, its context, and the 
types of research projects included. Next, we 
reflect on our findings and observations from 
the pilot project — specifically, the effectiveness 
of various impact-tracking tools and grant-mon -
itoring processes to support impact-assessment 
activities. Finally, drawing on lessons from 
the pilot project, we present considerations 
for an impact-measurement strategy that may 
be adopted by other foundations seeking to 
understand the policy impact of their research 
investments.
Assessing Research Impact
Philanthropy, whether it supports research 
and analysis or programs and services, is mis -
sion-driven. To ensure investments are aligned 
with their mission and vision, foundations have 
increasingly employed strategic or outcome-ori -
ented philanthropy, which involves clearly 
defined goals, evidenced-based approaches, and 
formal assessments of success and effective -
ness (Brest, 2012). Multiple formal evaluation 
approaches exist to measure the effectiveness 
of a foundation’s investments, including forma -
tive evaluation to assess program development 
or delivery; summative evaluation to assess 
program effectiveness; process evaluation to 
determine if the program was implemented as 
intended; outcome evaluation to assess short- or 
long-term changes in outcomes, behaviors, and 
practices as a result of the program; and impact 
evaluation to capture long-term changes, such 
as policy changes, resulting from the program 
(University of Minnesota, 2017). 
Yet, standalone evaluations are of little benefit to 
foundations unless the results of the evaluations 
are used to inform future foundation invest -
ments or program decisions. Carol H. Weiss 
(1998) describes the broader applications for eval-
uation use, including instrumental use to inform 
decision-making about investments and pro -
grams; use for conceptual purposes, which can 
provide program staff with a better understand -
ing of the program’s strengths and weaknesses; 
use for mobilization, which can affirm the need 
for specific changes to a program; and use for 
influence or enlightenment, where evaluation 
findings contribute to a larger body of evidence 
or knowledge base. 
Research-impact assessment, the focus of this 
article and the AcademyHealth pilot project, 
falls within the impact subset of evaluation. 
Foundations and other research funders may 
be motivated to evaluate the impact of their 
research investments for a number of reasons. 
Molly Morgan Jones and Jonathan Grant (2013) 
presented a framework for these motivations, 
which they termed the four “A’s”: advocacy, 
accountability, analysis, and allocation. As 
governments and other research funders grap -
ple with challenging fiscal environments and 
competing priorities, research-impact assess -
ment can serve to advocate, or “make the 
case,” for research funding and help to estab -
lish research as a priority. Related to advocacy, 
limited research funding requires researchers 
and funders to demonstrate accountability 
for investments, particularly for public dol-
lars but increasingly for private dollars as well. 
Foundations may conduct an analysis to better 
understand what investments worked and under 
what circumstances. This type of assessment 
can showcase the policy impact from research 
and can help to demonstrate the pathways from 
research investment to impact. This analysis can
<<<PAGE=4>>>
The Foundation Review  //  2017  Vol 9:4    43
Tools
Measuring the Impact of Research
ultimately inform how a foundation or govern -
ment allocates research dollars and contribute 
to research-strategy development and manage -
ment decisions.
The approach a funder takes to assessing 
research impact is closely tied to the purpose 
or goals of the particular research investment 
under consideration, whether that is advancing 
scientific knowledge on a topic, informing pub -
lic policy, or improving health outcomes. There 
are several traditional techniques for assessing 
research impact, including bibliometric or cita -
tion analysis, document reviews, interviews, and 
surveys, each with its own strengths and weak -
nesses (Jones & Grant, 2013).
Regardless of the technique used, numerous 
challenges can make research-impact assessment 
difficult. These challenges are not new and are 
well documented. A notable challenge is the time 
lag between research investment and research 
impact. Evidence suggests that it may take 17 
years, on average, to translate research findings 
into policy and practice (Slote Morris, 2011). 
Even research productivity measures, like cita -
tions or product output, can take multiple years 
to materialize. It often takes several years from 
the receipt of a research grant to publication of 
findings, and multiple years may elapse follow -
ing publication before meaningful citations are 
accrued. Further, a grantee’s reporting period 
often coincides with its grant period, and, as 
such, important impacts that may result many 
years following the conclusion of a study are not 
routinely captured. 
Another important challenge is measuring the 
attribution and contribution of research to a par -
ticular outcome. The ability to directly attribute 
an outcome to a specific research investment is 
the gold standard of research-impact assessment, 
but is incredibly difficult to achieve. Establishing 
that a research investment has contributed to 
a particular outcome is only slightly less chal-
lenging. Attribution and contribution pose a 
particular challenge for measuring the impact 
of research investments on policy and deci -
sion-making, the focus of the AcademyHealth 
pilot project, since policymaking is a complex 
process often informed by a body of evidence 
— rather than a single study — and many other 
streams of information (Penfield, Baker, Scoble, 
& Wykes, 2014).
Despite the limitations of research-productiv -
ity measures, these are some of the measures 
researchers rely on for promotion and tenure at 
their institutions and for reporting impact back 
to their funders. Given the limitations of these 
measures, many funders, largely outside of the 
U.S., have adopted frameworks and method -
ological approaches that require researchers to 
report not only research outputs but also the 
broader impact of their funded work. One prom -
inent example is the United Kingdom’s Research 
Excellence Framework (REF), which asks higher 
education institutions to submit both traditional 
measures of research output and case studies 
demonstrating the impact of their research 
As governments and other 
research funders grapple with 
challenging fiscal environments 
and competing priorities, 
research-impact assessment 
can serve to advocate, or 
“make the case,” for research 
funding and help to establish 
research as a priority. Related 
to advocacy, limited research 
funding requires researchers 
and funders to demonstrate 
accountability for investments, 
particularly for public dollars 
but increasingly for private 
dollars as well.
<<<PAGE=5>>>
44    The Foundation Review  //  thefoundationreview.org
Tools
Collado, Gerlach, Ticse, and Hempstead
beyond academia (Higher Education Funding 
Council for England, 2016).
Building on the REF among other frameworks, 
the AcademyHealth pilot sought to introduce 
both foundation staff and researchers to practi -
cal methods and tools for more systematically 
capturing data on research impact, with a par -
ticular focus on measures that indicate impact of 
research on policy and policy decision-making. 
The pilot project was not intended to be a for -
mal impact evaluation nor to take the place of 
comprehensive program evaluations. Rather, the 
goal was to develop a process for how founda -
tion staff might collect, organize, and interpret 
key measures of policy impact on an ongoing 
basis. These measures are intended to comple -
ment qualitative data collection and other evalu -
ation activities underway.
RWJF’s Investments in Health Insurance 
Coverage and the Pilot Project
For the RWJF and other health-focused foun -
dations, the passage of the Affordable Care Act 
(ACA) in 2010 created both a tremendous oppor -
tunity and an important challenge: the need to 
generate evidence with the rigor required to be 
credible and the timeliness needed to inform 
policy discussions and keep pace with the rapidly 
evolving policy landscape. In response to this 
challenge, the foundation supported a range of 
research projects intended to help policymak -
ers and other decision-makers understand and 
respond to issues around ACA implementation. 
These research investments included: 
• investigator-initiated research studies and 
policy analyses to evaluate provisions of 
the law, identify potential refinements, and 
inform implementation;
• survey research to help policymakers and 
stakeholders understand consumers’ atti -
tudes toward and experiences with insur -
ance under the ACA; and
• data set creation, analysis, and dissemina -
tion to bring new data to bear on emerging 
policy issues.
While these diverse investment types converge 
upon a shared goal — to inform policies that 
improve access to affordable health insurance 
coverage — the methods, products, audiences, 
and reach of these grantees and their activities 
vary greatly. 
The grants examined as part of the pilot included 
six projects completed prior to the start of the 
pilot. For these grants, AcademyHealth devel-
oped case studies that drew on several sources of 
data: semistructured telephone interviews with 
each of the principal investigators/project leads; 
review of relevant grant products, reports, and 
available web and/or media analytics; and inter -
views with policymakers and other end users 
of the grantees’ work. Each finished case study 
summarized the results of these data collection 
[T]he AcademyHealth 
pilot sought to introduce 
both foundation staff and 
researchers to practical 
methods and tools for more 
systematically capturing data 
on research impact, with a 
particular focus on measures 
that indicate impact of research 
on policy and policy decision-
making. [T]he goal was to 
develop a process for how 
foundation staff might collect, 
organize, and interpret key 
measures of policy impact on 
an ongoing basis.
<<<PAGE=6>>>
The Foundation Review  //  2017  Vol 9:4    45
Tools
Measuring the Impact of Research
efforts to describe how, when, and why grants 
were or were not impactful. 
While the research investment types included 
in the AcademyHealth pilot had different aims, 
methods, and intended audiences, each invest -
ment type made an important contribution to 
health policy. In different ways, findings from 
included grants helped inform policymaking 
within state and federal government and within 
health care delivery systems. They also provided 
evidence that was used by intermediary organi -
zations to inform policy discussions, including 
advocacy organizations, stakeholder groups, and 
the media. Taken together, the research pro -
duced by this portfolio of grantees was cited in at 
least 24 policy documents, including a Supreme 
Court decision, numerous amicus briefs, and sev -
eral reports to Congress; mentioned or used by 
at least 13 policymakers or end users; mentioned 
in more than 500 media stories; and viewed or 
downloaded over 30,000 times.
For five active grants included in the pilot proj -
ect, AcademyHealth tested a set of tracking tools 
to help inform a practical approach for gathering 
impact metrics while a grant is underway. These 
tools were intended to capture indicators of 
impact, including grantee mentions in traditional 
and social media; citations in policy documents, 
grantee publications, and alternative article-level 
metrics (e.g., blog posts mentioning published 
work); and peer-reviewed citations of published 
work. Eight specific tools were implemented in 
the pilot project:
1. Researchfish, an online platform for grantee 
reporting that records and attributes 
research outputs, outcomes, and impact to a 
specific grant;
2. Cision, an online media-monitoring soft -
ware that enables manual and automated 
searches;
3. Google News/ Alerts, an online search 
engine that allows for manual and auto -
mated searches of media and other online 
mentions;
4. CQ (Congressional Quarterly) Press Library, 
a database of policy documents (e.g., legisla -
tion, testimony, congressional reports) that 
allows for manual and automated searches 
of grantee citations in public and private 
policy documents;
5. PubMed, a biomedical literature database 
that allows for manual and automated 
searches for grantee publications;
6. Altmetric Bookmarklet, a free, online 
plug-in that provides alternative article-level 
metrics for select publications;
7. Google Scholar, an online, scholarly litera -
ture database that shows citation counts for 
publications via a manual search; and
For five active grants 
included in the pilot project, 
AcademyHealth tested a set of 
tracking tools to help inform 
a practical approach for 
gathering impact metrics while 
a grant is underway. These 
tools were intended to capture 
indicators of impact, including 
grantee mentions in traditional 
and social media; citations 
in policy documents, grantee 
publications, and alternative 
article-level metrics (e.g., blog 
posts mentioning published 
work); and peer-reviewed 
citations of published work.
<<<PAGE=7>>>
46    The Foundation Review  //  thefoundationreview.org
Tools
Collado, Gerlach, Ticse, and Hempstead
Tool Purpose Process Example Availability When to Use
Researchfish Research 
outputs and 
outcomes 
reporting
Grantees are notified 
quarterly to update 
their profiles with 
outputs and 
outcomes associated 
with their grant.
A grantee reported 
briefing policymakers 
on study findings.
Subscription 
fee
Implement at the 
beginning of a research 
study and maintain 
through the grant period 
and a designated post-
grant monitoring period.
*
Cision Media 
monitoring
Automatic alerts 
are set up for the 
full names of each 
principal investigator; 
staff manually  
reviews results for 
relevant impacts.
A grantee was quoted 
in an article in The  
New York Times.
Subscription 
fee
Implement at the 
beginning of a research 
study and maintain 
through the grant period 
and a designated post-
grant monitoring period.
*
Google Alerts Media/ 
online 
monitoring
Automatic alerts 
are set up for the 
full names of each 
principal investigator; 
staff manually  
reviews results for 
relevant impacts.
A grantee was quoted 
in an article in The  
New York Times.
Free Implement at the 
beginning of a research 
study and maintain 
through the grant period 
and a designated post-
grant monitoring period.
*
CQ 
(Congressional 
Quarterly)  
Press Library
Mentions 
in policy 
documents
Automatic alerts 
are set up for the 
full names of each 
principal investigator; 
staff manually  
reviews results for 
relevant impacts.
A grantee was cited 
in a report from the 
Office of the Assistant 
Secretary for Planning 
and Evaluation within 
the U.S. Department 
of Health and Human 
Services.
Subscription 
fee
Implement at the 
beginning of a research 
study and maintain 
through the grant period 
and a designated post-
grant monitoring period.
*
PubMed Publications Automatic alerts 
are set up for the 
full names of each 
principal investigator; 
staff manually  
reviews results for 
relevant impacts.
A grantee published a 
paper in Health Affairs.
Free Implement at the 
beginning of a research 
study and maintain 
through the grant period 
and a designated post-
grant monitoring period.
*
Altmetric 
Bookmarklet
Alternative, 
article-level 
metrics
Automatic alerts are 
set up for a grantee’s 
publication; staff 
records relevant 
results.
A grantee publication 
was mentioned by six 
news outlets, three 
blogs, 106 tweets, and 
two Facebook pages.
Free Implement for grantee 
publications as they are 
produced.
Google Scholar Scholarly 
literature 
database, 
citations
Staff manually 
searches using the 
title of a grantee 
publication and 
records the “cited by” 
number provided; 
automatic alerts can 
also be set up.
A grantee publication 
had eight citing 
articles.
Free Implement for publications 
as they are produced; 
search at regular intervals 
for a designated post-grant 
monitoring period.
Science-Metrix Biblio-
metrics 
and citation 
analysis
Staff contracts with  
a survey research  
firm to conduct 
citation analysis 
of identified 
publications.
A grantee publication 
was cited by 50 peer-
reviewed publications 
in journals, with a 
relative impact factor 
of 1.65.
Contract-
based
Implement at the 
conclusion of a research 
study; best if performed 
at least two years after the 
conclusion of a portfolio/
release of associated 
publications.
TABLE 1  Pilot Project Impact-Monitoring Tools
*Implementing online tracking tools at the beginning of a research study ensures that no relevant mentions of the grantee and/
or study are missed; however, the search results are likely to be most relevant and indicative of impact toward the end of the 
grant period, when the researcher has findings or has published.
<<<PAGE=8>>>
The Foundation Review  //  2017  Vol 9:4    47
Tools
Measuring the Impact of Research
IMPACT
INFLUENCE
AWARENESS
}
• Policymaker or end-user mention or use of research
• Grantee provided or /f_indings cited in testimony
• Policymaker or end-user inquiry or request for information
• Brie/f_ings with policymakers or other end-users
• Citations in policy documents
• Other examples of research impact on policy
}
• Media mentions
• Reporter inquiries
• Peer-reviewed citations
• Citations in grey literature
• Academic presentations or webinars
• Collaborations or partnerships
• Awards or recognition
• Other examples of research in/f_luence
}
• Page views/downloads
• Product output
• Social media outputs or mentions
• Other examples of awareness of research
Figure 1. A Pyramid Approach to Measuring Policy Impact 
8. Science-Metrix, an international research 
evaluation firm that performs citation anal-
ysis and other services.
Most of these tools enable real-time tracking of 
grantees and their research products, with the 
exception of the citation analysis performed by 
Science-Metrix and the citation count derived 
from Google Scholar, which are retrospective in 
nature. (See Table 1.) We have named the specific 
tools included in the pilot project to give founda -
tion staff an idea of the types of tools available to 
support grant monitoring and impact tracking, 
but this list is not exhaustive and the inclusion of 
these particular tools in the pilot project is not 
intended to be an endorsement of any one tool. 
This component of the pilot sought to deter -
mine the accuracy and feasibility of a range of 
tracking tools for concurrent grant monitoring, 
complemented by direct and regular outreach to 
active grantees to solicit any recent examples of 
impact. To the extent possible, AcademyHealth 
also applied the tracking tools to the six grants 
included in the retrospective analysis to better 
understand the tools’ effectiveness in capturing 
impact metrics from years past. 
Organizing and Interpreting Impact 
Measures: The Metrics Menu
The specific charge of the AcademyHealth pilot 
project was to develop a tool and process for 
more systematically capturing the impact of the 
RWJF’s research investments. Drawing from 
both the case study development and the test -
ing of online tracking tools, AcademyHealth 
developed a grant monitoring tool — the Metrics 
Menu — to organize different types of impact 
data according to three different strata we iden -
tified as important indicators of research impact 
(See Figure 1.) 
In the case of the AcademyHealth pilot proj -
ect, the RWJF was particularly interested in the 
impact of its research investments on health pol-
icy and health policy decision-making. As many 
foundation staff are likely aware, process and pro -
ductivity measures such as page views or product 
output are often the easiest to assess, but do not 
capture the full impact of a research investment. 
To address this limitation, we attempted to iden -
tify indicators of policy impact and classify them 
into three broad strata: awareness measures, 
influence measures, and impact measures. Taken 
FIGURE 1  A Pyramid Approach to Measuring Policy Impact
<<<PAGE=9>>>
48    The Foundation Review  //  thefoundationreview.org
Tools
Collado, Gerlach, Ticse, and Hempstead
together, they cover a range of indicators of 
research’s impact on policy, providing research 
funders and their grantees with examples of the 
types of metrics they might collect to inform 
their research-impact assessment activities. 
We defined awareness measures as those that 
capture the visibility of a product or suite of 
products from a grant. Although not policy 
impact per se, metrics like website page views or 
publication downloads help to highlight grant 
products or projects that garner above-aver -
age attention and awareness, which may signal 
potential policy impact. These measures are 
often readily accessible to foundation staff or eas -
ily obtained from the grantee. 
The influence measures move a step beyond 
awareness to capture important interactions 
between grantees and potential end users of 
their work that could result in policy impact. 
For example, grantees in the AcademyHealth 
pilot reported spending significant time talking 
with reporters, either specifically about study 
findings or about a broader policy issue relevant 
to their grant. These conversations sometimes 
led to mentions in media stories, but not always. 
In cases where a grantee was not subsequently 
cited in a story, these conversations brokered 
important relationships between researchers and 
members of the media and helped to establish 
RWJF-funded researchers as go-to resources for 
future stories.
Finally, impact measures indicate use of funded 
research in policy and policy decision-making. 
Possible indicators of impact range from citation 
of a research article or other grant product in a 
policy document (e.g., legislation, regulations, 
court decisions, testimony) to a policymaker 
contacting an expert researcher to inform ongo -
ing decision-making. In the course of the pilot 
project, we observed numerous occasions in 
which in-person interaction with a policymaker 
was an effective means of informing policy deci -
sions. When a policymaker directly reaches out 
to a researcher, this signifies he or she views the 
researcher as a trusted expert in the topic area. 
These direct and personal interactions are consid -
ered “productive interactions” and are examples 
of social impact (Spaapen & van Drooge, 2011). 
Findings 
AcademyHealth’s experience documenting 
the impact of a subset of RWJF grantees offers 
valuable insights for other foundations seeking 
a practical approach for routinely collecting 
indicators of the policy impact of their research 
Strata Metric Source
Awareness
Website page views and downloads Grantee-reported web analytics
Grantee product output Count of grantee deliverables
Influence
Media mentions Media-monitoring software (e.g., Cision); Google Alerts
Citations in peer-reviewed literature Google Scholar; citation analysis (e.g., Science-Metrix)
Impact
Citations in policy documents
CQ Press Library alerts/searches; manual review of 
citations in relevant policy documents (e.g., legislation, 
testimony); grantee-reported testimony
Policymaker request for information Grantee-reported exchange
TABLE 2  Policy Impact Metrics and Sources for Data Collection
<<<PAGE=10>>>
The Foundation Review  //  2017  Vol 9:4    49
Tools
Measuring the Impact of Research
investments. The AcademyHealth process is 
neither a large-scale program evaluation nor a 
full research-impact assessment, and, as such, 
it necessarily lacks some of the rigor and com -
prehensiveness associated with these types of 
efforts. What it does offer, however, is a way for 
foundation staff to more systematically iden -
tify, collect, and organize different types of data 
that, together, can more closely approximate a 
research investment’s actual policy impact. In 
this section, we reflect on the effectiveness of our 
impact measurement strategies, including the 
pros and cons of the methods we tested. 
Understanding the Benefits and Limitations  
of Tracking Tools
Impact-measurement tools, including those 
implemented in the AcademyHealth pilot, aim 
to capture a broad range of research outputs and 
outcomes, from publications and citations to 
mentions in the press and other policy-relevant 
sources. They also vary in terms of their ease of 
use, cost, and the “signal to noise” ratio of the 
search results. As such, each tool has distinct 
advantages and disadvantages. Implementing 
standard search strategies (e.g., using the prin -
cipal investigator’s full name) across a range of 
tools increases the consistency of the grant mon -
itoring and is more comprehensive than indi -
vidual, one-off, or irregular attempts to identify 
examples of research use and impact. However, 
the time and energy required to process search 
results depends on several factors. For example, 
the uniqueness of the principal investigator’s 
name can significantly affect the “signal to noise” 
ratio and require greater staff time to parse irrel-
evant results. Although automatic alerts address 
this issue to some extent, more staff time may be 
required to monitor prolific grantees who work 
on multiple grants, produce many products, and 
generate evidence within a defined content area, 
which can complicate attributing search results 
to specific foundation-funded grants.
AcademyHealth tested most of the tracking tools 
both retrospectively as well as in concurrent 
grant monitoring. On the whole, we found that 
using these tools to identify the impact of com -
pleted grants was more labor-intensive and poten -
tially less accurate than using the tools to help 
inform concurrent monitoring, in which search 
results can be assessed and recorded in near real 
time. Also, the pilot tested tools that require a 
subscription fee as well as those that are publicly 
available. There is a tradeoff between paid versus 
free tools, but based on our experience, in many 
cases the tradeoff is minimal. Most of the impact 
tracking that was the focus of our pilot could 
be accomplished using the publicly available 
tools, although the paid tools can provide more 
nuanced or detailed results in some instances.
Finally, regardless of the tool used, impact track -
ing and measurement is imperfect. Media stories 
and policy documents sometimes refer to bodies 
of work in general, and/or do not reference the 
author or study title by name, making it diffi -
cult for a tool or manual search to identify. Even 
detailed searches do not capture everything, and 
relevant items can be missed. Further, quantita -
tive measures alone fail to capture the full impact 
of a grant, as they cannot assess who is down -
loading and reading a brief or the quality of the 
news outlet citing a study’s findings. This under -
scores the importance of gathering qualitative 
information from grantees and from research 
[R]egardless of the tool 
used, impact tracking and 
measurement is imperfect. 
Media stories and policy 
documents sometimes refer 
to bodies of work in general, 
and/ or do not reference the 
author or study title by name, 
making it difficult for a tool or 
manual search to identify. Even 
detailed searches do not capture 
everything, and relevant items 
can be missed.
<<<PAGE=11>>>
50    The Foundation Review  //  thefoundationreview.org
Tools
Collado, Gerlach, Ticse, and Hempstead
end users, through direct outreach or interviews, 
to provide context for the impact of a research 
study and supplement the quantitative measures. 
Eliciting Information From Grantees
In the pilot project, we tested two strategies for 
gathering qualitative information from RWJF 
grantees: regular and direct outreach to active 
grantees and semistructured interviews with 
grantees whose projects had concluded. Both 
strategies are effective for eliciting detailed, nar -
rative information from grantees to enhance the 
quantitative measures described above. Direct 
grantee outreach in real time has the primary 
advantage of prompting grantees to provide 
examples of research impact as those examples 
occur. For example, in our experience, many 
salient examples of policy impact may be infor -
mal or unplanned, including a telephone call 
or hallway conversation between a researcher 
and a policymaker or journalist. These import -
ant examples of impact cannot be captured by 
web-based tracking tools and stand to be lost 
in the absence of regular communication with 
the grantee. It is important to note, however, 
that given the time lag between the conduct of 
a research study and the study’s impact, active 
grantees may not have significant information to 
share during their study period. 
Conversely, retrospective qualitative analysis, 
such as the interviews we conducted with past 
grantees and the users of their work, yields 
significantly more detailed results, but at a sig -
nificant cost to staff time. For example, our 
interviews revealed that several grantees gave 
presentations at conferences that helped them 
connect with eventual end users of their work, 
information we would not have gained had we 
asked grantees to simply report the number of 
presentations given. However, the process of 
eliciting this information from grantees and con -
firming it with the research users they identified 
required time and other resources from project 
staff that may not be available to foundations and 
other funders.
Making Sense of Impact Metrics
The Metrics Menu developed through the 
AcademyHealth pilot is intended to be a tool 
used by researchers and foundation staff to orga -
nize impact metrics captured from web-based 
tracking tools and/or qualitative data collection. 
It organizes these metrics into awareness, influ -
ence, and impact measures to help researchers 
and their funders track the myriad ways research 
findings may reach a policymaker, some of 
which are more direct than others. (See Table 
2.) While we view the Metrics Menu as a useful 
tool for helping researchers and their funders 
organize and interpret impact data, we recog -
nize that simply listing counts across different 
metrics types does not provide a full picture of 
whether, why, and how a research grant had 
impact. Rather, the Metrics Menu is most valu -
able when paired with a narrative account that 
provides additional qualitative information and 
helps corroborate and contextualize the data 
captured in the menu. More broadly, we recog -
nize that even this detailed, two-step approach 
cannot conclusively determine whether or not a 
researcher or research study has had an impact 
on policy. However, we believe this process still 
has value as a practical approach for uncovering 
and explaining examples of impact that research 
funders may not capture otherwise.
[M]any salient examples of 
policy impact may be informal 
or unplanned, including a 
telephone call or hallway 
conversation between a 
researcher and a policymaker 
or journalist. These important 
examples of impact cannot 
be captured by web-based 
tracking tools and stand to be 
lost in the absence of regular 
communication with the grantee.
<<<PAGE=12>>>
The Foundation Review  //  2017  Vol 9:4    51
Tools
Measuring the Impact of Research
Discussion
Drawing from our reflections on the effective -
ness of the pilot project, this section lays out 
several key considerations for foundations in 
developing and implementing an impact mea -
surement strategy. In particular, we recommend 
foundations consider the following key questions 
as they develop or refine their own measure -
ment strategies. 
What: Defining Outcomes of Interest
Foundations and other research funders may be 
interested in many different types of research 
impact, such as advancing knowledge, inform -
ing policy, or making a broader contribution to 
society. For any funder interested in assessing 
research impact, an important first step is iden -
tifying the type of impact of greatest interest 
and the types of metrics that can approximate 
that impact.
Our primary outcome of interest in the pilot 
project was the impact of research investments 
on policymaking, and, as such, we developed 
three strata of measures that may indicate pol-
icy impact. Other foundations may also want to 
consider stratifying the information they col-
lect from grantees and other sources to provide 
a more accurate picture of the contribution of 
a particular study. For example, we found that 
grants with a documented impact on policy 
(e.g., grant products cited in policy documents 
like court decisions, legislation, regulations, or 
testimony) often achieved considerable visibil-
ity (as measured by page views and downloads). 
Foundations seeking to determine which prod -
ucts or projects generated the greatest aware -
ness could consider asking grantees to submit 
grant-related products and associated web 
analytics on a regular basis. A regular review 
of these web analytics might suggest particular 
products or projects to monitor more closely for 
policy impact.
When: Timing for Impact Monitoring
The pilot project also suggested important con -
siderations for the timing of impact monitoring. 
Many of the RWJF grantees noted there is often 
a lag between the conclusion of a research study 
and the public release of study findings. Further, 
the conclusion of a study and/or release of study 
findings may not coincide with a “policy win -
dow” — a time when findings are relevant to cur -
rent policy discussions (Kingdon, 1993). Certain 
types of projects may have a longer lag time than 
others: For example, researchers who rely on 
traditional dissemination vehicles, like peer-re -
viewed publications, often experience longer 
timelines, as it may take many months or even 
years to have a paper reviewed, accepted, and 
published. The time lag between release of study 
findings and their application to policy decisions 
suggests foundations may want to follow up with 
a grantee for a period of multiple years after the 
grant concludes. Real-time monitoring of an 
active grantee is important to ensure the grantee 
adheres to the project schedule, but foundations 
interested in gaining a more comprehensive view 
of the policy impact of their investments should 
consider monitoring projects beyond the conclu -
sion of the formal grant period.
How: Choosing an Impact-Monitoring 
Approach
A broad range of tools exist to support grant 
monitoring and impact tracking, but as has been 
stated, these tools should be paired with qual-
itative data. Foundation staff could consider a 
range of options to couple quantitative metrics 
with narrative information. In monitoring active 
grants, the AcademyHealth pilot coupled use 
of the tracking tools with regular and direct 
outreach to grantees. For concluded projects, 
AcademyHealth staff conducted semistructured 
telephone interviews with grantees and end users 
For any funder interested in 
assessing research impact, 
an important first step is 
identifying the type of impact 
of greatest interest and the 
types of metrics that can 
approximate that impact.
<<<PAGE=13>>>
52    The Foundation Review  //  thefoundationreview.org
Tools
Collado, Gerlach, Ticse, and Hempstead
of their work. The purpose of the telephone 
interviews and the direct grantee outreach was 
to capture examples of policy impact that the 
tools would miss, such as conversations with 
policymakers or journalists. Regardless of the 
specific tools or processes implemented, founda -
tions should incorporate both quantitative and 
qualitative data collection into their impact-mea -
surement strategy. 
Who: Engaging Dedicated Grant Monitors to 
Systematically Track Grantees
Given the complexity of research-impact track -
ing, the resources required, and the level of effort 
involved, RWJF grantees in the AcademyHealth 
pilot project indicated they would need resources 
and support to perform this level of tracking 
and reporting. Given this feedback, we recom -
mend identifying a designated grant monitor 
to conduct impact tracking. Depending on the 
size of the portfolio, this could be the grant’s 
project officer or manager within the founda -
tion. Alternatively, if a foundation wishes to 
assess a larger portfolio or multiple portfolios, 
a foundation could engage an external organi -
zation to monitor the projects during the grant 
period and for a period following the conclusion 
of a research study.
Identifying a designated grant monitor or mon -
itoring organization that is responsible for 
research-impact tracking has several advantages. 
First, it enables consistent measurement across 
a portfolio of projects. The monitor can ensure 
that the same alerts and strategies are applied to 
each grantee so that the data are collected sys -
tematically and reported consistently. Second, 
a designated monitor reduces the burden and 
reporting requirements for grantees. That said, 
grantees will still need to work closely with the 
monitor to report examples of grant impact that 
cannot otherwise be captured by tracking tools 
or systematic searching. 
Conclusion
Systematically measuring the impact of research 
on policy is a long-standing challenge for many 
organizations, and this pilot confirms there is 
no silver bullet. However, the AcademyHealth 
pilot project for the RWJF proved useful in sev -
eral respects. Chiefly, the pilot succeeded in its 
goal of helping the foundation better understand 
the impact of different types of research invest -
ments, particularly for less traditional research 
investments whose findings did not end up in the 
peer-reviewed literature. The project also pro -
vided useful insights into the RWJF’s target audi -
ences. Like many organizations, the foundation 
has a range of audiences for its work, some big 
and some small, with varying levels of influence 
that may not correspond to size — for example, 
certain policy audiences may be small in number 
but highly influential. In the case of the RWJF 
pilot, conversations with research users about 
how and why a project was impactful also turned 
up important insights about where key audiences 
go for information and why they view that infor -
mation as trustworthy or useful.
Importantly, the AcademyHealth pilot project 
also provides useful information for other foun -
dations as they consider practical ways to collect, 
organize, and interpret key measures of policy 
impact on an ongoing basis, keeping in mind 
[O]nline tracking tools 
present an opportunity to 
more systematically capture 
examples of research impact, 
and they can sometimes 
provide important evidence of 
the visibility, influence, and 
impact of funded research. 
That said, these measures 
must be paired with qualitative 
data to better understand not 
only the impact of research 
investments, but the impact 
pathway as well.
<<<PAGE=14>>>
The Foundation Review  //  2017  Vol 9:4    53
Tools
Measuring the Impact of Research
that this process does not take the place of large-
scale program evaluations. Among our lessons 
learned, online tracking tools present an oppor -
tunity to more systematically capture examples 
of research impact, and they can sometimes pro -
vide important evidence of the visibility, influ -
ence, and impact of funded research. That said, 
these measures must be paired with qualitative 
data to better understand not only the impact of 
research investments, but the impact pathway as 
well. Another key takeaway is that the grantee 
is often the best source of information about the 
impact of his or her work. By enlisting the assis -
tance of a designated grant monitor, or perhaps 
an external monitoring organization, founda -
tions can partner with grantees to collect key 
indicators of impact both while a grant is under -
way and after the project concludes. It is our 
hope that the lessons learned in this pilot project 
prove useful for other foundations seeking to 
support impactful research and systematically 
assess their success in this regard.
References
Brest, P. (2012, Spring). A decade of outcome-oriented 
philanthropy. Stanford Social Innovation Review.  
Available online at ht t ps://ssi r.org/a r t ic les/ent r y/a _ 
decade_of_outcome_oriented_philanthropy
Higher Education Funding Council for England. 
(2016). REF impact. London: Author. Available online 
at http:/ /www.hefce.ac.uk/rsrch/REFimpact/ 
Jones, M. & Grant, J. (2013). Making the grade: Method -
ologies for assessing and evidencing research impact. 
In A. Dean, M. Wykes, & H. Stevens (Eds.), 7 essays on 
impact (pp. 25–43). Exeter, UK: University of Exeter.
Kingdon, J. W. (1993). How do issues get on public policy 
agendas? In W. J. Wilson (Ed.), American Sociological 
Association Presidential Series: Vol. 8. Sociology and the 
public policy agenda (pp. 40–50). London: Sage.
Penfield, T., Baker, M. J., Scoble, R., & Wykes, M. C. 
(2014). Assessment, evaluations, and definitions of 
research impact: A review. Research Evaluation, 23 (1), 
21–23. 
Robert Wood Johnson Foundation. (2017). Building 
a culture of health. Princeton, NJ: Author. Available 
online at https:/ /www.cultureofhealth.org/ 
Slote Morris, Z., Wooding, S., & Grant, J. (2011). The 
answer is 17 years, what is the question: Understand -
ing time lags in medical research. Journal of the Royal 
Society of Medicine, 104 (12), 510–20.
Spaapen, J., & van Drooge, L. (2011). Introducing “pro -
ductive interactions” in social impact assessment. 
Research Evaluation, 20 (3), 211–18.
University of Minnesota. (2017).  Different types of evalu -
ation.  Available online at https:/ /cyfar.org/different- 
types-evaluation  
Weiss, C. H. (1998). Have we learned anything new about 
the use of evaluation? American Journal of Evaluation, 
19(1), 21–33. 
Megan Collado, M.P.H.,  is a director at AcademyHealth. 
Correspondence concerning this article should be ad -
dressed to Megan Collado, AcademyHealth, 1666 K Street, 
NW, Suite 1100, Washington, DC 20006 (email: megan.
collado@academyhealth.org ). 
Lauren Gerlach, M.P.P.,  is a senior manager at 
AcademyHealth. 
Caroline Ticse, B.A., was a research associate at 
AcademyHealth at the time of this study. She is now a 
pricing analyst at The Buonopane Group.
Katherine Hempstead, Ph.D., is a senior adviser at the 
Robert Wood Johnson Foundation.