<<<PAGE=1>>>
Participants’ Perceptions of the effectiveness of CLEAR -
AA’s Development Evaluation Training Programme in 
Africa 
 
 
 
 
 
Mokgophana Ramasobana, 
 Student No: 541533 
Wits School of Governance 
 
 
 
A research report submitted to the Faculty of Commerce, Law, and Management, the 
University of the Witwatersrand in partial (50%) fulfilment of the requirements for the degree 
of Master of Management (in the field of Public Sector Monitoring and Evaluation). 
 
 
27 February 2020
<<<PAGE=2>>>
i 
 
Keywords: evaluation capacity building, evaluation capacity development, training 
effectiveness,  
Abstract 
In the past few decades, monitoring and evaluation (M&E) capacity-building programmes and 
training budgets aimed at addressing the rising demand for M&E skills have been increasing. 
Over the same period, extensive research focusing on the broader evaluation capacity 
development (ECD) spectrum has been commissioned. However, insufficient re search 
assessing the effectiveness of M&E capacity -building programmes has been conducted; 
therefore, their contribution towards building skills and knowledge is unknown. In this study, 
qualitative research aimed at assessing the effectiveness of the Devel opment Evaluation 
Training Programme in Africa (DETPA), delivered by the Centre for Learning on Evaluation 
and Results – Anglophone Africa (CLEAR -AA), was used as a case study to begin to 
understand how Kirkpatrick’s (1959) training effectiveness model could be applied to 
understand the DETPA training programme implemented in Africa. This is informed by 
DETPA’s popularity in the region and plans to scale up and implement the programme on an 
annual basis.  Therefore, conducting this assessment contributes towards the improvement of 
the programme. Using Kirkpatrick’s model, semi-structured interviews were conducted aimed 
at assessing participants’ perceptions on whether or not the DETPA programme has contributed 
towards building their individual capacities (skills and knowledge), influenced their individual 
behavioural change as well as organisational behavioural change.  The interviews were also 
aimed at ascertaining their perceptions regarding the gaps of programmes such as the DETPA. 
This study focused on the participants of the 2017 programme, which also marked the launch 
of the DETPA. For the purpose of research rigour  interviews with different categories of 
respondents were conducted as follows: ten (10) DETPA 2017 participants, one (1) DETPA 
facilitator, two (2) DETPA moderators and four (4) line managers of participants. The findings 
are not generalizable, as the purpose of the study was not to conduct a quantitative analysis of 
the perceptions of participants, but to better understand how individual participants personally 
experienced the potential effects of the DETPA on their individual and organisational 
capacities.  Based on the participant’s perceptions, the programme has been perceived to have 
the following effects: generally, the findings indicate that the overall programme was perceived 
by participants to have contributed positively to their M&E capacity development. It was 
specifically perceived to have contributed towards i mproving their skills and knowledge as 
well as to some extent to have contributed towards sustaining the transfer of skills. In addition,
<<<PAGE=3>>>
ii 
 
there was mixed feedback on whether the programme has contributed towards improving 
participants’ organisational performance. In terms of the perceived gaps in the programme, it 
was significant that participants proposed that CLEAR -AA should consider integrating the 
Made in Africa Evaluation (MAE) philosophy throughout the entire DETPA, as it is currently 
only included as a single module. This elevates the role of local or contextual approaches in 
understanding the effectiveness of training programmes delivered in the African diaspora. In 
conclusion, this study recommends that further empirical research should be conducted to 
better understand the mechanisms by which training influences skills and knowledge 
acquisition as well as organisational effectiveness  in M&E, as well as to allow for the 
generalisation of these findings.   
 
 
Keywords: Evaluation capacity development, Evaluation capacity building, Training 
effectiveness, Skills and Knowledge acquisition and transfer and Organisational performance
<<<PAGE=4>>>
iii 
 
Declaration 
I declare that this thesis/dissertation titled ‘ Participants Perceptions of the effectiveness of 
CLEAR-AA’s Development Evaluation Training Programme in Africa’ is my own, independent 
work. I have documented and referenced all sources used and cited. This submission is in 
partial fulfilment of the requirements of the degree of Master of Management (Public sector 
monitoring and evaluation) at the University of the Witwatersrand, Johannesburg. This report 
has not been submitted for any other degree or examination or any other institution. 
 
 
 
 
 
 
 
 
 
 
__________________________________ 
Mokgophana Ramasobana 
Johannesburg, 27 February 2020
<<<PAGE=5>>>
iv 
 
TABLE OF CONTENTS  
ACRONYMS AND ABBREVIATIONS .................................................................................................. viii 
ACKNOWLEDGEMENTS ........................................................................................................................ ix 
CHAPTER 1: INTRODUCTION TO THE RESEARCH ......................................................................10 
1.1 Introduction ...........................................................................................................................................10 
1.2 Problem Statement ...............................................................................................................................12 
(i) Statement of the problem ...............................................................................................................12 
(ii) The context ..................................................................................................................................13 
(iv) The rationale .....................................................................................................................................14 
(v) Purpose statement .............................................................................................................................15 
1.3 Research Questions ...........................................................................................................................15 
Primary research question .....................................................................................................................15 
Sub-Questions .........................................................................................................................................15 
1.4 Structure of the research report ......................................................................................................15 
CHAPTER 2: LITERATURE REVIEW ..................................................................................................17 
2.1 Introduction ...........................................................................................................................................17 
2.2 Background of the Development Evaluation Training Programme in Africa ................................17 
2.3 Contextualising Capacity and Skills Development Discourses in Africa .........................................19 
2.3.1 The emergence of New Public Management in Africa ...............................................................19 
2.3.2 Interlinking New Public Management initiatives and Human Resource Development ..........21 
2.3.3 Overview of Human Resource Development theory ...................................................................23 
2.3.4 Applying HRD in an African context ...........................................................................................27 
2.4 Understanding the concept of organisational performance or change ........................................29 
Implementing the concept of organisational performance or change ................................................30 
Factors that advance or inhibit organisational change .......................................................................32 
2.5 Conceptualising Evaluation Capacity Building and Evaluation Capacity Development ...............38 
2.5.1 Linking the role of training programmes to organisational change ..........................................40 
2.5.2 The professionalisation of the evaluation field and related debates around relevant skills ....41 
2.5.3 The advantages and disadvantages of the professionalisation of M&E ....................................43
<<<PAGE=6>>>
v 
 
2.5.4 Debates on the evaluation of M&E training programmes .........................................................47 
2.6 Common Approaches to Measuring Training Effectiveness ............................................................50 
2.7 Models for Evaluating the Effectiveness of Training Programmes ..................................................52 
2.8 The application of Kirkpatrick’s model in previous studies .............................................................56 
Level 2: Learning ................................................................................................................................62 
Level 3: Transfer .................................................................................................................................62 
Level 4: Organisation .........................................................................................................................62 
CHAPTER 3: METHODOLOGY .............................................................................................................63 
3.1 Introduction ...........................................................................................................................................63 
3.2 Research paradigm and strategy .....................................................................................................63 
3.3 Research design .................................................................................................................................64 
3.4 Data and data gathering ...................................................................................................................65 
3.5 Sampling ............................................................................................................................................66 
3.6 Data analysis ......................................................................................................................................66 
3.7 Validity and reliability ......................................................................................................................67 
3.8 Limitations and delimitations ..........................................................................................................67 
3.9 Ethics ..................................................................................................................................................68 
3.10 Informed consent ............................................................................................................................69 
3.11 Confidentiality and harm to participants .....................................................................................69 
CHAPTER 4:  DESCRIPTION OF RESULTS .......................................................................................70 
4.1 Introduction .......................................................................................................................................70 
4.2 Participants’ overall reaction to the DETPA programme ................................................................71 
Primary research question .....................................................................................................................71 
4.2.2 Key theme: Relevance of the DETPA programme .....................................................................75 
4.2.3 Key theme: Insightful and good use of participant’s time .........................................................76 
4.2.4 Key theme: Improvement in the skills and knowledge acquisition ...........................................77 
4.3 Learning attained during the DETPA programme ...........................................................................78 
4.4 Improved and sustained transfer of skills and knowledge ................................................................81 
4.5 Organisation improvement ..................................................................................................................82
<<<PAGE=7>>>
vi 
 
4.6 Proposed areas of improvements .........................................................................................................83 
4.7 Limitations .............................................................................................................................................86 
4.8 Conclusion .............................................................................................................................................87 
CHAPTER 5 ANALYSIS AND DISCUSSION OF RESULTS ..............................................................87 
5.1 Introduction .......................................................................................................................................87 
Framing the context in which the analysis of the study is conducted .................................................88 
5.2 Level 1 - Overall reactions to the DETPA programme .................................................................89 
5.2.1 The coordination of the DETPA programme ..............................................................................90 
5.2.2 The curriculum content and the context in which it was implemented ....................................90 
5.2.3 Facilitator’s role and their facilitation style ................................................................................94 
5.2.4 Skills, Knowledge and Trainee attitudes/attributes ....................................................................95 
5.3 Level 2 - Learning .............................................................................................................................96 
5.3.1 Skills and knowledge acquisition ..................................................................................................96 
5.3.2 Increased trainee’s evaluation technical, and cross-cultural competencies ..............................97 
5.4 Level 3- Improved and sustained transfer of skills and knowledge ...........................................100 
5.5 Level 4 – Improved organisational performance .........................................................................103 
5.6 Areas of improvement ....................................................................................................................107 
5.7 Conclusion .......................................................................................................................................110 
CHAPTER 6 CONCLUSION AND RECOMMENDATIONS .............................................................112 
6.1 Summary of the research report....................................................................................................112 
6.2 Recommendations and future research ........................................................................................114 
REFERENCES..........................................................................................................................................116 
APPENDICES .......................................................................................................................................126 
Appendices 1.1: Project information sheet .........................................................................................126 
Part I: Project Information Statement................................................................................................126 
Aims of the Research ............................................................................................................................126 
The Primary research aim: ..................................................................................................................126 
Significance of the Research Project ...................................................................................................126 
Research Plan and Method ..................................................................................................................127
<<<PAGE=8>>>
vii 
 
Part II: Information to ensure you of the research ethics considerations of this study ..................128 
Appendices 1.2: Informed consent form .............................................................................................129 
Appendices 1.3: Interview Guide.........................................................................................................130
<<<PAGE=9>>>
viii 
 
ACRONYMS AND ABBREVIATIONS  
 
ACBF   The African Capacity Building Foundation 
AUC   African Union Commission 
CDSF   Capacity Development Strategic Framework 
CLEAR-AA   The Centre for Learning on Evaluation and Results Anglophone Africa 
ECB   Evaluation Capacity Building 
ECD   Evaluation Capacity Development 
EBDM   Evidence Based Decision Making 
DETPA   Development Evaluation Training Programme in Africa 
HEIs   Higher Education Institutions 
HRD   Human Resource Development 
HIV/AIDS  Human Immunodeficiency Virus I- Acquired Immune Deficiency Syndrome 
MAE   Made in Africa Evaluation 
M&E   Monitoring and Evaluation 
NEPAD New Partnership for Africa’s Development (The African Union Development 
Agency) 
NPM   New Public Management 
RCT   Randomised Control Trial 
ROI   Return on Investment 
TOC   Theory of Change 
TOR   Terms of Reference 
IPDET   The International Program for Development Evaluation Training 
IKS   Indigenous Knowledge Systems
<<<PAGE=10>>>
ix 
 
 
ACKNOWLEDGEMENTS  
 
This study was conducted as part of my fulfilment of my Master’s  degree, and I am therefore 
indebted to the olive branch extended to me by both the CLEAR-AA management, colleagues 
and associates. Your support has left an indelible mark on my academic path. Furthermore, I 
would like to extend my gratitude to the DETPA participants, line managers, facilitators and 
moderators who voluntarily and devotedly participated in the interview process. This is 
followed by my appreciation of the guidance and inputs provided by my supervisor , Ms 
Candice Morkel of CLEAR -AA, towards the conceptualisation of the research proposal until 
the documentation of this report . Significantly, thanks to Matlale Ramasobana (my spouse) 
who cajoled me into returning to the computer screen. When I was frustrated with the research 
process and concepts, she engaged me with he r own thoughts and listened to unending 
renditions of varying stylistic editions, reading versions and tiresome text. She consistently 
offered me cheerful smiles and an over-supply of tea, food and moral support to mitigate some 
of my frustrations. Her support ensured that I could deliver this document; without her support, 
none of this would have been feasible. I also wish to extend my special gratitude to my family, 
especially my parents, Phuti and Madinti Ramasobana. This includes our son, Tshenolo Legong 
and daughter Lua Sbukwana , who throughout the study understood my long absence  and 
limited time outs. Most importantly, to God be the glory.
<<<PAGE=11>>>
10 
 
CHAPTER 1: INTRODUCTION TO THE RESEARCH  
1.1 Introduction 
In recent years, the demand for monitoring and evaluation ( M&E) capacity building 
interventions have increased. This has led to the mushrooming of M&E short -course training 
programmes (Basheka & Byamugisha, 2015; Wao, Onyango, Kisio, Njatha, & Onyango, 
2017). However, the effectiveness of these capacity- building initiatives has  yet to be 
sufficiently evaluated (Wao, et al., 2017). Regardless of the global economic crisis impacting 
all sectors, but particularly constraining public sector finances , as argued by Sharma (2016), 
Russ-eft (2002) and Punia and Kant (2013), the delivery of short course training progra mmes 
is continually increasing (Wotela, 2017). As an illustration, a number of traditional universities, 
or Hi gher Education Institution (HEIs), have developed M&E short course training 
programmes which are very  popular, particularly with public sector officials (ibid.). A web 
search shows that t hese include the University of Cape Town (South Africa), Stellenbosch 
University (South Africa), the University of Witwatersrand (South Africa), the Ghana Institute 
of Management and Public Administration, Uganda Technology and Management University, 
as well as the University of Nairobi (Kenya) . Short course programmes such as these are 
designed to meet the rapidly growing M&E skills demand (Abrahams, 2015; Basheka & 
Byamugisha, 2015; Wao et al., 2017). However, there is little empirical evidence attesting that 
these programmes are contributing to improving evaluation practice (Preskill, 2008; Tarsilla, 
2014). This is corroborated by Surgery (2010), who acknowledges that the popular assumption 
that M&E training budgets leads to improved productivity remains untested. 
Despite the increase in  M&E capacity building programmes and training budgets aimed at 
addressing the rising demand for M&E skills, participants’ perceptions on the effectiveness of 
short-course training programmes towards improving their skills and knowledge, and 
perceived individual and organisational behavioural change s has been insufficiently 
documented (Morkel, & Ramasobana, 2017; Wao et al., 2017). In the limited cases where 
assessments were conducted in Africa, Wao et al. (2017) argue that the curriculum of the 
training programmes offered is densely theoretical without practical components. In other 
words, the training is  not tailored to respond to the participant’s skills and knowledge needs. 
Others suggested that M&E courses are observed to be irrelevant and not useful (Rambau, 
2005; Tonhäuser & Büker, 2016).
<<<PAGE=12>>>
11 
 
This is corroborated by a survey conducted by Morkel and Ramasobana (2017) intended to 
review the extent to which organisations in Africa assess the outcome of their capacity building 
or training programmes. According to their findings, only eight out of the thirteen organisations 
that were surveyed occasionally assess the outcome of training on knowledge acquisition. 
Therefore, their contribution towards improving the participant’s skills and knowledge is 
unknown, which is a common refrain in other researc h of a similar nature (Preskill, Hallie, & 
Boyle, 2008; Tarsilla, 2014).  This raises a question around the prevailing problem of capacity 
building initiatives and their contribution towards improving the participant’s skills and 
knowledge as envisaged by the training providers. 
In this study, the DETPA programme w as used to examine participants’ perceptions of the 
effectiveness of short-course training programmes.  DETPA is an annual flagship short course 
training programme offered by CLEAR -AA, which was launched in 2017.  The programme 
aims to build the capacity of M&E practitioners and scholars in order to strengthen evaluation 
practice across the continent (CLEAR -AA, 2018). The impetus for the two -week-long 
programmes is a response to the rapid ly growing M&E demand for skilled evaluators to 
commission and implement evaluation interventions in Africa (ibid.). The initiative to 
capacitate evaluators is informed by the assertion that the majority of evaluators originate from 
the West  (Cloete, 2017; Tirivanhu, Robertson, Waller &  Chirau, 2017) and therefore , this 
programme aims to contribute to addressing the skills gap on the African continent. The 
programme is categorised into two tracks, known as Fundamentals and Advanced. The 
“Fundamentals of M&E” track is designed for new entrants to the M&E sector whilst the 
“Advanced Track” targets intermediate or advanced M&E practitioners. Over the past two 
years, the programme has attracted M&E practitioners and government officials responsible 
for developing and maintaining their national evaluation systems (NES), or those who have the 
authority to influence policy, planning, and evaluation cycles (CLEAR -AA, 2018). Thus, the 
DETPA programme has been envisaged to conduct training aimed at providing skills to 
practitioners in various sectors and government officials to optimally execute their duties.  
However, there has been insufficient research undertaken aimed at determining whether or not 
the DETPA programme has contributed to building the capacity of M&E practitioners, 
government officials and scholars in order to strengthen evaluation practice as per its 
objectives. Therefore, whether or not the programme has assisted the target market to execute 
their duties remain largely unknown.  Hence, the current study is  useful in contributing to 
building a knowledge base around whether or not the programme has contributed to improving
<<<PAGE=13>>>
12 
 
participant’s capacities in M&E . However, it should be noted that it  is a perception study; 
therefore, more research would be needed to make further contributions to knowledge on this 
topic. 
In the past 50 years, Kirkpatrick’s (1959) training effectiveness model has been used as a 
benchmark for the  assessment of training effectiveness (Alliger & Janak, 1989; Brown, & 
Reed, 2002; Chang, 2010; CLEAR -AA, 2013; Sharma, 2016). This model measures the 
effectiveness of training programmes on four levels: (a) the participants’ reactions to the 
programme – Level 1; (b) an assessment of the content, or what the participants learned - Level 
2; (c) the participants’ performances on the job -  Level 3; and (d) the impact of the training 
upon the organi sation - Level 4 (Alliger & Janak, 1989; Jasson & Govender, 2016; Sharma, 
2016). Owing to its simplicity and applicability to measure the effectiveness of short -course 
training programmes, Kirkpatrick’s (1959) training model, which is described in detail in this 
proposal, was applied as the conceptual framework for this study.  
The primary research question that this study addres sed is: what are participant’s perceptions 
regarding the DETPA 2017  programme as delivered by CLEAR -AA, and specifically has  it 
been effective across the four components of Kirkpatrick’s (1959) model ? This study only 
focused on the 2017 cohort because the programme was piloted in 2017 and this research 
commenced prior to  the d elivery of the 2018 programme. Lastly, the study examine d 
participants’ perceptions regarding the gaps of the DETPA  and how these gaps  can be 
improved.  
The following section presents the research problems that the study address es. This entails 
discussing the statement of the problem, the context, the knowledge gap and the rationale for 
the study.  
1.2 Problem Statement 
(i) Statement of the problem 
Various evaluation capacity building ( ECB) initiatives by numerous higher education 
institutions (HEIs) in Africa have been designed to meet the M&E skills gaps (Abrahams, 2015; 
Basheka & Byamugisha, 2015; Wao et al., 2017) . One such case, which is the focus of this 
study, is the CLEAR -AA DETPA programme.  Over the past tw o years, the programme has 
trained 78 participants from 18 countries in Africa who represent  37 private and public 
organisations (CLEAR-AA DETPA Internal Report, 2018) . Despite the significant interest
<<<PAGE=14>>>
13 
 
generated in the DETPA programme, which is evident in the number of participants from 
various African countries, private and public organisations represented and the enrolment rate, 
there is little empirical evidence for the reasons for the popularity of the programme. In 
addition, the effectiveness of the programme in terms of building the capacity of its participants 
– even at the level of perceptions -  is yet to be examined. Therefore, the contribution of the 
DETPA programme towards building participants ’ individual capacities (skills and 
knowledge), their individual behavioural change s and organisational behavioural change s is 
unknown (CLEAR, 2018) . This study address ed this gap by assessing  the participants’ 
perceptions of the effectiveness of the DETPA programme using Kirkpatrick’s (1959) 
evaluation model as a framework. Furthermore, the study envisaged contributing to the
 general 
ECB community and CLEAR-AA, in particular, with insights into improving the 
conceptualisation, impl ementation and assessment of its training programme s. However, it 
should be noted that this perception study is not intended to generalise, rather it is intended to 
obtain a deeper understanding of individual’s perspectives and experiences of the training 
programme and its contribution to developing their individual and organisational M&E 
capacities. The in-depth understanding of individual experiences of how programmes such as 
the DETPA contribute to ECB could pave the way for further  empirical research geared to 
develop further understanding on this topic , and provide evidence to support future 
improvements in DETPA and similar CLEAR-AA programmes. 
(ii) The context 
According to Tarsilla (2017), short course training discourses have been dominated by western 
scholars. In Africa, the rising demand for M&E skills gave rise to the development of numerous 
training interventions, including the design of M&E short course training programmes by HEIs 
to address the M&E skills gap ( Abrahams, 2015; Basheka & Byamugisha, 2015; Wao et al., 
2017). However, not enough research has been conducted on the effectiveness of these training 
programmes in mitigating the skills gap or  on improving organisational performance. Some 
scholars such as Alliger and Janak ( 1989), Jasson and Govender ( 2016) and Sharma (2016) 
have contributed to the discourse on assessing training effectiveness in general in the fields of 
hospitality and human resources. Their work contributes to our understanding of  the 
development of short course training programmes and the extent to which their effectiveness 
has been measured. However, more research focusing on assessing M&E training programmes 
in particular still needs to be undertaken (Morkel & Ramasobana, 2017; Tirivanhu et.al., 2017; 
Wao et al., 2017). This study, therefore, contributes to building an understanding of the way in
<<<PAGE=15>>>
14 
 
which participants perceive how short course ECB programmes in Africa, such as the DETPA, 
affects M&E skills and knowledge acquisition, as well as individual and organisational 
performance. 
(iii)  The knowledge gap 
Various research studies acknowledge that the effectiveness of short course training 
programmes is not documented (Morkel & Ramasobana, 2017; Wao et al., 2017) . Thus, the 
contribution of the DETPA programme towards improving participants’ skills and knowledge 
and the performance of individuals and organisations in M&E is unknown. On the other hand, 
CLEAR-AA plans to continue to implement the programme on an annual basis. Therefore, the 
current study is useful in contributing to building a knowledge base around whether or not the 
programme has contributed to improving participant’s M&E capacities. In addition, this study 
contributes towards deepening scholars’ under standing on how individuals perceive the 
effectiveness of short course training programmes in M&E, which may contribute to 
understanding how to better design and implement such programmes with the target audience’s 
needs in mind.      
 (iv) The rationale 
Kirkpatrick’s (1959) model indicates that assessing the effectiveness of training programme s 
globally remains an important exercise. This is a relevant point of departure for the assessment 
of training programmes, such as the DETPA delivered in the African region. The effectiveness  
of the DETPA programme in relation to (a) participants’ reactions to the programme – Level 
1, (b) an assessment of the content, or what the participants learned - Level 2, (c) the 
participants’ performances on the job -  Level 3, and (d) the impact of the training upon the 
organisation - Level 4, is unknown. To this end, the return on investments, such as time and 
financial resources , provided by CLEAR -AA as well as organisations that sponsored the 
enrolment of their staff to the DETPA programme has not been accounted for. Therefore, this 
study contributes towards understanding whether there is a perception that short course training 
programmes, such as the DETPA, contribute to improving the M&E skills and knowledge gaps 
on all four levels of participants in Africa. The findings from the study provide  the general 
ECB community and CLEAR -AA, in particular , with insights in to improving the 
conceptualisation, implementation and assessment of its training programme interventions , 
based on the perceptions of some participants of the DETPA 2017 programme , and also 
provides the basis for further research in this area.
<<<PAGE=16>>>
15 
 
(v) Purpose statement 
The primary purpose of this research study was  to assess partic ipants’ perceptions of the 
effectiveness of the CLEAR -AA DETPA 2017  programme across all four levels of 
Kirkpatrick’s model of evaluating training effectiveness : reaction, learning, transfer and 
organisational impact. 
1.3 Research Questions 
This study has one primary research question and five sub-questions. 
Primary research question 
What are participants ’ perceptions regarding the DETPA 2017 programme in terms of its 
effectiveness regarding Kirkpatrick’s (1959) four levels of training effectiveness:  reaction, 
learning, transfer and organisational impact?  
Sub-Questions 
(i) What are the  participants’ overall perceptions and reactions to the  DETPA 2017   
programme? 
(ii) What are participants’  perceptions on whether or not they have acquired new 
knowledge, skills and learning from participating in the delivery of the DETPA  
2017 programme?  
(iii) What are the participants ’ perceptions regarding their job- related performance in 
M&E after their participation in the DETPA 2017? 
(iv) What are participants’ perceptions regarding the impact of the training on their 
organisations, specifically in terms of M&E practice? 
(v) What are participants ’ perceptions regarding the gaps of programmes such as the 
DETPA 2017 in building skills, knowledge, individual performance and 
organisational improvement and how they can be enhanced?  
 
1.4 Structure of the research report 
This report is structured in six main parts; following this introduction chapter is chapter two,  
which presents the literature reviewed and the theoretical framework applicable to this research 
study. This entails a review of the previous studies conducted by  various researchers with the 
aim of understanding similar problems. This also includes various theoretical frameworks used, 
in order to justify the theoretical framework underpinning this study. Chapter three discusses 
the research design and methodology that this study is based on.  Chapter four a nd five
<<<PAGE=17>>>
16 
 
respectively present the findings and analysis, and Chapter six concludes by providing a 
summary of the discussions, recommendations and prospective or future research areas.
<<<PAGE=18>>>
17 
 
CHAPTER 2: LITERATURE REVIEW  
2.1 Introduction  
This chapter presents the literature review that underpins this  study on measuring the 
effectiveness of training programmes. The chapter begins by providing a background of the 
Development Evaluation Training Programme in Africa (DETPA) as a case and unit of analysis 
for this study.  This chapter  foregrounds capacity-building initiatives within the New Public 
Management (NPM) paradigm, which is linked to the upsurge in results -based management 
approaches in the public sector since the 1980s. This chapter further provides a distinction 
between the concepts of evaluation capacity development (ECD) and evaluation capacity 
building (ECB ) as  part of providing an introductory background to the concept of M&E 
training. Furthermore, the chapter presents selected theories on measuring the effectiveness of 
training across the globe, including the African continent. Since it is well known that training 
does not occur in a vacuum and  is anticipated to contribute towards organisational 
performance, a broad discussion focusing on organisational development and organisational 
change concepts is presented. In addition, characteristics or determinants of assessing the 
evaluation of training programmes from the reviewed literature are identified, including their 
limitations. Lastly, the chapter provides the rationale for using Kirkpatrick’s (1959) training 
effectiveness model as an appropriate  conceptual framework for this study. This section also 
identifies themes and concepts that empower the researcher to answer the research questions 
and interpret the findings. 
2.2 Background of t he Development Evaluation Training Programme i n Africa  
As discussed previously, the Development Evaluation Training Programme in Africa (DETPA) 
is an annual flagship programme launched in 2017 and delivered by CLEAR -AA. This 
programme aims to contribute towards building localised M&E practitioners and scholars with 
apt skills as well as enhancing the evaluation field in the region. This serves as a response to 
the urgency to champion initiatives to increase the number of local practitioners and evaluation 
approaches and methodologies (Chilisa & Tsheko, 2014; Cloete, 2016). Put differently, these 
initiatives are envisaged to incorporate the nuances of the evaluation needs in Africa as well as 
explore contextually relevant approaches to evaluation. Hence, the “Made in Africa 
Evaluation” (MAE) agenda has been identified as an appropriate approach  which underpins 
DETPA’s ideological lens. This approach purports that context, culture, history, and beliefs are
<<<PAGE=19>>>
18 
 
critical to the way we shape evaluation in response to the diversity and complexity of 
development in Africa (CLEAR-AA, 2018).  
Over the past two years, the programme has attracted M&E  practitioners responsible for 
building their national evaluation systems, or those who are in positions of influence in terms 
of policy, planning, and evaluation (CLEAR -AA, 2018). The majority of these practitioners 
have received tremendous support in terms of financial and time allocations  from their 
respective employers or organisations. To some extent, one  could infer that the DETPA 
programme has received some level of legitimacy in the eyes of these  individuals and their 
respective employers. On the  other hand, it also forces the DETPA organisers to  ensure that 
the delivery of this programme meets the expectations of both individual participants as well 
as the organisations that have invested resources towards supporting their employees in the 
programme.  Kirkpatrick’s (1959) model, which has been extensively used to understand how 
training influences individual and organisational level s of capacity building, is used as the 
conceptual framework of this study. This will be explained further in  section 2.6 which 
discusses the conceptual framework used in this study.  
According to the CLEAR -AA (2018) report, the content of the programme was  divided into 
two parts. Firstly, the f undamental track was designed to cover elementary M&E concepts. 
This was designed as an awareness-building track, which densely described fundamental M&E 
concepts. It is envisioned that at the completion of this track, participants will be expected to 
champion the importance of integration of M&E in the programmes, projects and policymaking 
cycle. Secondly, the advanced track focuse s on deepening participant’s understandings of 
M&E concepts in line with their existing praxis’s.  At the completion of this track, the 
programme envisages that participants will be proponents of Afrocentric approaches  to and 
methods of evaluation. This includes the linkages between M&E concepts and their contextual 
environments as well as their practices. 
The DETPA programme forms an integral part of  CLEAR-AA’s broader theory of change 
around supporting M&E related capacity  building initiatives anticipated to  strengthen 
evaluation practice on the continent (CLEAR-AA, 2018). At a conceptual level, CLEAR-AA, 
aspires to contribute towards strengthening the capacities of African institutions to interrogate 
and use evidence for decision making. This is done via promoting homegrown or African 
theories and expertise which are purported to have higher probabilities of enhancing the M&E 
skills and knowledge sets of individual practitioners and their respective organisations (ibid.).
<<<PAGE=20>>>
19 
 
However, this study acknowledges there are  some limitations associated with the DETPA 
programme. For instance, there is no selection criteria because the programme uses open 
enrolment (CLEAR-AA, 2018). However, it could be argued that the programme still uses the 
traditional training methods of teaching and learning. This traditional approach include s the 
use of face-to-face interactions, in a classroom setting.  
 
2.3 Contextualising Capacity and Skills Development Discourses in Africa  
Contextualising ECD in this section of the chapter is categorised into five main parts. Part One 
discusses the emergence of New Public Management (NPM) and  interlinks the NPM and the 
growth of the Human Resources Development (HRD) approach in building public sector 
capacities. To do so, a summary background on NPM which contributed to the emergence and 
progression of the M&E sector in Africa is provided. This is followed by an in-depth discussion 
on HRD and how it applies in the African context. Part Two introduces the concepts of 
evaluation capacity building and evaluation capacity development. In addition, it discusses the 
growing demand for M&E capacity building programmes. Part Three presents common 
approaches to measuring training effectiveness, and provides a segue to  Part Four, which 
presents the components of Kirkpatrick’s (1959) model of Evaluating Training Effectiveness. 
Part Five summarises  themes emerging from the reviewed literature, and concludes the 
framework within which the findings of this study are discussed.  
 
2.3.1 The emergence of New Public Management in Africa 
  
Africa’s challenge of underdevelopment drove International Finance Institutions ( IFI) and 
others to focus their attention on capacity building initiatives (Itika, 2011). As far back as the 
1960s or the post-independence era, African states were classified to be in a dire financial 
situation and therefore needed some form of assistance from the Global North (Dzimbiri, 
2008). This is often referred to as financial reforms. A big part of these reforms entailed issuing 
financial aid as bail outs to various governments in the region. Thus, a continent-wide financial 
loan regime was imple mented with the expectation that these loans will be paid back with a 
reasonable amount of interest . This implies that a huge amount of financial resources with 
conditions were injected into Africa’s public purse (Chipkin & Lipietz, 2012). In tandem, these
<<<PAGE=21>>>
20 
 
IFI’s (and others) made strides to ensure that building governments abilities to account for their 
financial investments (loans) was part of the terms and conditions of these reforms (ibid.).  
 
Fast forward to the  1980s, additional but different  reforms focusing on the institutions , 
otherwise known as  public re forms as opposed to financial reforms , were ushered in by 
multilateral and bilateral agencies. This is in synchronisation with the advent and popularity of 
the New Public Management (NPM) wave (Chipkin & Lipietz, 2012; Dzimbiri, 2008) . This 
meant that most of the governments in the region  executed NPM as an attempt to uphold the 
principles of accountability, governance and transparency. The NPM regime was envisaged to 
improve the e fficiency and service delivery headed by public institutions (Cameron, 2009). 
Therefore, public institutions needed to apply and embed the cited efficiencies which elevated 
a need to employ public officials with skills to optimally manage or provide oversight to public 
institutions (Dzimbiri, 2008) . As opposed to the 1960s era which instilled financial 
accountability, the NPM era steered accountability for non-financial resources, via promoting 
the efficiencies of public institutions (ibid.).  
Both 1960s and 1980s  were complemented by capacity building (CB) strategies focusing on 
achieving different capacity outcomes. For example, in the case of the 1960s , the majority of 
the training initiatives focused on enhancing public officials’ skills to account for the finances 
allocated by the IMF (Chipkin & Lipietz, 2012; Dzimbiri, 2008). On the other hand, the 1980s 
witnessed the mushrooming of capacity building initiatives converging on bolstering public 
official’s capacities to uphold the mandates of public institutions to effectively deliver services 
on behalf of the citizens (Pestoff, 2016). Put differently, capacity development initiatives in the 
1980s were designed to build and hone  the capacities of public officials to  implement and 
sustain the NPM wave. In summary, the majority of capacity development (CD) initiatives 
were initially focused on accounting for the huge loans  from the IMF (and others) and on 
providing skills to new administrations. Thus, the 1960s to 1980s CD was mainly focused 
around financial management, accounting and governance (Chipkin & Lipietz, 2012).  
Considering the generous amount of investments aimed at financial and institutional reforms, 
multilateral and bilateral institutions, including development experts, agitated for the urgency 
to measure what works and does not work in Africa’s development (Tarsilla, 2014). As such, 
the 1990s brought about the  importance of grappling with whether or not  these reforms were 
improving the lives of Africa and her citizens. Evidence produced by organisations such as the
<<<PAGE=22>>>
21 
 
African Capacity Building Foundation (ACBF) founded in 1991 pointed to a non-achievement 
of development outcomes (Leautier & Mutahakana, 2012). This instigated a shift from focusing 
on the financial and institutional reforms by  asking fundamental questions  around whose 
development and for whom (ACBF, 2016). Thus, evaluation as a systematic approach to delve 
into documenting lessons on what works and does not work in the programmes or interventions 
implemented in Africa gained traction (Tarsilla, 2014).  
Understanding what works and does not work in development does not occur in a vacuum. 
This calls for competent  public official s with suitable skills to carry out the task of 
commissioning evaluations (Podems, 2014). This illuminates the rapid growth of evaluation 
capacity development ( ECD) interventions including M&E training programmes  geared 
towards building and enhancing the skills of the technocrats  (Morkel & Ramasobana, 2017) .   
Human resource practitioners employed in various government institutions were bestowed with 
the mandate to adapt and champion  the implementation of human resource development 
(HRD) policies as part of  their job ’s responsibilities (Itika, 2011). Therefore, the emergence 
of NPM wave, which coincided with the growing popularity of HRD approaches in the public 
sector, are significant milestones in understanding the capacity development initiatives in 
Africa, including evaluation capacity development (ECD). The following section therefore 
presents a discussion on the interlinkages between NPM and HRD.  
2.3.2 Interlinking New P ublic M anagement initiatives and Human Resource 
Development 
 
Boyle and Harris (2009), Yeung (2009)  and Pestoff (2016) concede that the wave of New 
Public Management (NPM) thinking has swept over the public sector all over the world, and 
particularly in Africa, over the past few decades. Broadly, accountability, transparency and 
democracy are the three principles underpinning the NPM concept (Cameron, 2009). Sabbi and 
Ayeko-kümmeth (2015) posit that the NPM is aimed at redressing the deficiencies of public 
institutions by decentralising the institutional arrangement s of governments. At the practical 
level, the public sector either uses both or one of the two approaches, namely,  either the 
process-oriented or the results-based approach which are aimed at increasing the public sector’s 
efficiencies (ibid.).  Tirivanhu et al. (2017,  p. 230) adds to this discourse by succintly stating 
that NPM is primarily conceptualised as a way  to improve government efficiencies in the 
delivery of services as well as to uphold accountability and transparency principles.
<<<PAGE=23>>>
22 
 
The birth and rise of HRD  was ascribed to a scholar named Nadler   (2008, p. 347). McLagan 
(1989) as cited by Beich, (2008, p. 27) defined HRD as an inclusive approach that assimilates 
three concepts which are (i) training and development, (ii) organisational development and (iii) 
career development. The author further assert ed that training and development focuses on 
boosting individual abilities using a mi x of methods , such as off -site training, on the job 
training, mentoring etc. O rganisational development emphasises the bilateral relationship 
between individuals a nd groups and their conceivable systematic contribution to effecting 
change in an organisation. Lastly, career development  focuses on building a pathway that 
recognises an individual employee’s efficacy which are then utilised towards furthering the 
individual’s career path which culminates in improving the proficiencies of their respective 
organisations. 
Mehlape (2017)  illustrates how NPM and HRD was pragmatically used to redres s the 
deficiencies of public institutions and human personnel . They explain that HRD practitioners 
practising in the region were required to  appreciate that bureaucratic deficiencies must be  
addressed in parallel with building a competent workforce to sustain the public institutions.  
The author further add ed that underscoring these approaches is the assumption that 
strengthening human capital or personnel w ould contribute to Africa’s development  (ibid.). 
Both public sector officials and private practitioners have explored new HRD approaches  to  
upskill government employees , including the development of human resource policies 
(Mehlape, 2017, p. 108)  in Africa. This has compelled managers and practitioners practicing 
in Africa to borrow HRD best practices, which  were predominantly effected in the private 
sector (Itika, 2011). 
Tirivanhu et al.  (2017) contribute further by stating that the adoption of NPM by African 
governments introduced a new approach to policy making processes and government 
performance. In other words, as previously alluded by Itika (2011) results -based management 
or outcome -based principles , which were traditionally utilised in the private sector , were 
adapted and administered to maximise the functionality of the  public institutions in terms of 
service delivery. Delivering services with a sense of speed and to the satisfaction of the citizens 
implied that government officials had to make decision s rapidly but grounded in  scientific 
evidence (Dzimbiri, 2008) . This, in turn, elevated the role of evaluation as a gateway to 
improving accountability and transparency via the collation and utilisation of evidence by local 
evaluators (Basheka & Byamugisha, 2015) . This further recognises that  these evaluations
<<<PAGE=24>>>
23 
 
continue to be  conducted and commissioned by non- Africans, which contributed to the 
sentiment that Africans lack evaluation skills (Tarsilla, 2014).  
As a response, various capacity -building initiatives aimed at capacitating local policymakers 
and evaluators with competencies to navigate the M&E profession are underway (Tirivanhu et 
al., 2017). Primarily these capacity building initiatives including training, referred to as ECD 
by Tarsilla (2014) , are broadly aimed at entrenching evaluation as a useful tool towards 
improving decision making processes within the public sector. The focus of this study is not to 
comprehensively document the sequencing of all the factors that have contribute d to the 
booming of ECD. However, this study acknowledges that other discourses such as SDG’s and 
evidence-based decision making (EBDM) also contributed to the rise of ECD. In principle, 
there are many other factors that have directly influenced the emergence of the ECD field. The 
limitations of time and space does not permit the researcher to delve into those other factors. 
Although the delivery of M&E related training ha s been commended, Tirivanhu et al. (2017) 
argue that the delivery of these train ing programmes should not only be demand driven but  
they also  propose that it should use an outcome -based approach. In other words, these 
programmes should tally with the overarching principles of improving the efficiencies of 
service delivery in the continent. Therefore, they should be tailored to capacitate African 
practitioners and scholars to use information from locally commissioned evaluations to 
improve policies and programmes  on the continent. Consequently, Tirivanhu et al.  (2017) 
recommend that training providers of evaluations such as universities and think tanks should 
strive to  deliver relevant training interventions  designed to cater for non- homogenous 
evaluation skill sets.  
Mbava and Dahler-Larsen (2019) raise a dissimilar but significant argument concerning part 
of the training programme design. They propose that training programmes delivered in Africa 
should adapt and embrace the trends of the 21st century. In practical terms, the scholars urge 
the training providers to fully  integrate the use of technology when designing the training 
content of the training phase of these programmes. Therefore, the availability of internet is an 
opportune pathway to instil uniqueness to training interventions implemented in Africa.   
 
 
2.3.3 Overview of Human Resource Development theory
<<<PAGE=25>>>
24 
 
HRD forms part of the  conceptual framing for understanding the effect of the DETPA 
programme. This section presents the HRD concept as a way to provide the  contextual 
framework of this study.  
The HRD concept was introduced in the 1970s and has since gained popularity amongst 
researchers such as Aliaga (2005), Weinberger (1998) and Kumpikaite and Sakalas (2011) who 
credit Nadler (1970) as a pioneer who conceptualised the HRD theory. This author introduced 
a broader and more inclusive definition of the HRD concept than that from  the western 
countries which acknowledged the growth of the HRD practice in other developing continents 
(Kumpikaite & Sakalas, 2011; Weinberger, 1998).  
Nadler and Nadler (1991), as cited by Kumpikaite and Sakalas (2011. p. 41), argue that HRD 
is comprised of two objectives: training and organisational development. Firstly, these authors 
argue that training is primarily about the availability of resources and time invested to sharpen 
employee’s skills sets. Organisational development focuses on the training initiatives pursued 
by the entire organisation, aimed at skilling employees as well as designing potential career 
paths to be followed by employees. Out of the two concepts, training seems to be the applicable 
to this study.  
HRD is a multifaceted field of study, which has been studied using varied lenses. Scholars such 
as Brown, Lafond and Macintyre (2001), Kumpikaite and Sakalas (2011) and Weinberger 
(1998) use numerous theories or lenses such as economic,  psychological, philosophical and 
systems theory to understand the concept of HRD.  Various scholars reinforce the fact that  
there are different theories and approaches to understanding the concept of HRD. For instance, 
Nadler (1970), as cited by Weinberger (1998, p. 77), uses psychological theory to contend that 
HRD is a sequence of activities which are planned and implemented within a specified period, 
aimed at facilitating behavioural change via adult learning approaches. This was followed by 
Craig (1976), who combined both the psychological and philosophical theory to posit that HRD 
is preoccupied with improving human performance through creating opportunities for 
continuous learning (ibid.). These authors identify behavioural change, adult learning 
approaches and human performance as determinants of understanding the HRD concept.   
 
This means that HRD can be used  for multiple purposes such as to understand the economic 
effects of interventions popularly known as ROI. In some cases, HRD has been used by human 
resource practitioners to carry out research focusing on the human relations within various
<<<PAGE=26>>>
25 
 
workspaces. That said, it can be concluded that there are diverse definitions of HRD  and 
therefore it is not homogenous (Weinberger, 1998). 
Although there is a lack of consensus on the definition of HRD, there seems to be an 
acknowledgement of the fact that countries in the Global North were responsible for the genesis 
of this concept (Kumpikaite & Sakalas, 2011; Weinberger, 1998) . This might explain the 
popularity and dominance of western scholars in the review  of the HRD literature relevant to  
this study. 
  
Nadler and Wiggs (1986) combined economic and psychological theory to argue that HRD i s 
a comprehensive learning framework that is used to improve organisational performance. This 
definition suggests that a combination of theory-based and experiential - or practice -based 
learning contributes to organisational performance. In other words, Nadler and Wiggs (1986), 
as cited by Weinberger (1998, p. 77), recommend that organisations should consider supporting 
their employees  (via funding and authorising time off work)  to participate  in training 
programmes designed to  facilitate theoretical knowledge as  well as to enable them to apply 
their newly acquired skills and knowledge in their workplaces . This, in turn, has the potential 
to contribute to an improvement in the performance of the overall organisation. 
A book titled “ Fundamentals of human resource management ” (Itika, 2011) introduced two 
HRD sub -theories: institutional th eory and organisational theory. This book elucidates that 
organisational norms, values, attitudes and employee perceptions  are the institutional theory 
elements that determine the success or failure of an organisation (Itika, 2011, p. 5). In addition, 
organisation and employee confidence in learning are organisational theory elements that 
determine the performance of an organisation (ibid, p.  5). In other words, both elements of the 
institutional and organisational theories contribute to improving employee’s job performances 
and organisational results.  
Weinberger (1998, p. 77) posits that physiological and economic theories are some of the most 
prevalent theories in studying the HRD field. Most importantly, out of the two  theories, the 
economic theory seems to be  the most preferred theory  (Nadler & Wiggs, 1986). In other 
words, the majority of the HRD studies  have focused on attributing  whether or not  training, 
education and development initiatives have contributed towards improving the economic 
growth in various countries (Kumpikaite & Sakalas, 2011). This implies that these studies used 
economic lenses to establish return on investment s (ROI) or value for money after the
<<<PAGE=27>>>
26 
 
implementation o f skills development interventions  (training, education and development).  
Unlike studies such as Weinberger ’s (1998), which focused on economic growth, this study 
focuses on building capacity for evidence-based decision making (EBDM).   
Despite the fact that the above cited authors Craig and Weinberger; Nadler and Wiggs (1986) 
researched HRD from t he perspective of the private sector, their insight s remain relevant to 
this study. Their studies provided an account of  how behavioural changes and the acquisition 
of skills and experience impact organisational change. Their account coincides with the current 
research because it undertakes to understand similar concepts, particularly in relation to the 
public sector. This trend is not limited to this  study, other public sector studies have also 
borrowed best practices form the private sector. In fact, the NPM theory ha s enabled the  
infiltration of private sector thinking into the public sector . This further reinforces the  
immediate relevance of the cited HRD work to the current study. This study focuses on EBDM 
in the public sector, and departs from orthodox writings on HRD. 
Sydhagen and Cunningham (2007) highlight the limitations of evaluating skills development 
initiatives from an  economic lens only. These scholars argue  that most of the research  
conducted intensely evaluated the HRD concept and its contribution to economic growth at the 
exclusion of human resources /personnel. Aliaga (2005) substantiated this  in a study that 
analysed the links between economics and HRD. In t hat study, economic growth and the use 
of technology were cited as key determinants of training; however, insignificant attention was 
paid to the role of employees in determining the training content. Studies by Aliaga (2005) and 
Weinberger (1998)  emphasise capacity building f or the mainstream economy;  this study 
focuses on building capacity for the delivery of public policy and development.  
 As can be seen,  HRD is a complex concept with different definitions  and contexts . As 
mentioned earlier, these  definitions are grounded i n wide-ranging theories (Mehlape, 2017).  
Therefore, applying HRD is underpinned by contextual factors and has been applied from 
different lenses such as an economic on as used by Aliaga (2005) and Weinberger (1998). Most 
of the l iterature reviewed  emanates from western countries and  outlines the dominance of 
economic theory in the HRD concept. However, the above review also gave examples of how 
the public sector adopted NPM followed by the prioritis ation and the integration of capac ity-
building interventions such as M&E training programme s. In summary, applying capacity 
building interventions should appreciate contextual  differentiation between the pursuit of 
private and the public sector objectives.
<<<PAGE=28>>>
27 
 
.  
2.3.4 Applying HRD in an African context 
 
Sydhagen and Cunningham (2007)  provide an  historical account of public policy and 
development. It has been noted that wars and calamities, which ravaged this region, are some 
of the root causes of Africa’s underdevelopment  (Itika, 2011). As an illustration, Itika (2011) 
continues to argue that these conditions gave rise to the dominance of western paradigms, 
unemployment and economic fragilities. The  effects of this unpleasant history culminated in 
the weakening of institutions and lack of skills, which led to a perpetuation of socio-economic 
imbalances post-Africa's independence (Sydhagen & Cunningham, 2007) . Consequently, the 
public sector, like the private sector, has experienced challenges in skills and knowledge 
deficiencies (Itika, 2011).  
In the previous sections, Kumpikaite and Sakalas (2011) and Weinberger (1998) situated the 
genesis of the HRD field and its popularity within western countries. Thus,  the dominance of 
northern theories, concepts and frameworks in understanding African governments and 
organisations is not surprising.  This implies that definitions and concepts, which are meant to 
inform skills development interventions aimed at c apacitating African governments, 
organisations and practitioners , are conceived from  Eurocentric as opposed to Afrocentric 
paradigms (Itika, 2011). It follows that these definitions and concepts are collated and dispersed 
into various resear ch outputs such as textbooks . These include the conceptualisation of the 
formal and informal HRD curriculum  content. Following th is, it could be argued that these 
textbooks and HRD curricula, which have been conceptualised from a Eurocentric paradigm,  
might be perceived to be contextually unfit to address the underdevelop ed regimes on the 
Africa continent.  
Sydhagen and Cunningham (2007) rationalise that the aforementioned challenges contribute to 
the perception that the continent is faced with skills gaps and mismatches. In turn, the skills 
gaps and mismatches impede the prospects of Africa’s move to fully cultivate opportunities 
available at a global scale. This might, therefore, help to clarify Africa’s minimal participation 
within the global economic scenarios. Consequently, the alluded lack of skills and mismatches 
could somehow be regarded as some of the root causes of the high unemployment rate and 
insignificant economic growths across the region (Sydhagen & Cunningham, 2007).
<<<PAGE=29>>>
28 
 
According to Itika (2011), t he challenges of the high unemployment rate and inconsistent 
economic growth are exacerbated by factors such as migration of skilled personnel to greener  
pastures outside the continent. Furthermore, the author emphasises that the role of HIV/AIDS, 
as well as high levels of poverty, cannot be disregarded. This is intensified by the fact that  
young people constitute the majority of the human capital in Africa (ibid.). Consequently, this 
elevates the importance of continuous training programmes to capacitate human personnel  
(both young and old) with the skills to advance Africa’s development. Thus, an understanding 
of the HRD concept in the region should appreciate and acknowledge these cited challenges. 
As a response to the cited challenges, in the past  few decades , African governments, 
organisations and practitioners have waged war again st the underdevelopment witnessed by 
the continent (Itika, 2011). Huma n resources and processes are highlighted as some  of the 
components responsible for the success of attaining the objectives enshrined in Agenda 2063 
“The AFRICA We Want ” framework  (The World Bank Report, 2005) . This initiative is  in 
synchronisation with the role of the new public management (NPM) wave witnessed in Africa 
and illustrates the links with the HRD concept. Both these approaches are aimed at capacitating 
African governments, organisations, and practitioners with skills fitted to address Africa's cited 
challenges responsible for her underdevelopment.  
Thus, there are attempts to explore localised or indigenous  approaches and methodologies 
aimed at ensuring that the skills development interventions in the region address the contextual 
needs of governments, organisations and practitioners. These approaches have been popularly 
known as indigenous knowledge systems (IKS) by scholars such as  Chilisa (2017) as well as 
Keane, Khupe and Muza (2016). At the core of IKS is the acknowledgement that Africa has 
distinct ways of knowing and her ways of knowing could prospectively contribute to the 
conceptualisation and implementation of skills development interventions. It is important to 
acknowledge the growing trend voiced by Africa n practitioners and scholars as an attempt to 
localise or contextualise evaluation. This includes initiatives geared to ensur ing that the 
evaluation capacity building implemented in Africa is adapted to cater to the skills needs of the 
practitioner. In some circles, this is referred to as Made in Africa Evaluation by scholars such 
as Chilisa and Tsheko (2014) and Cloete (2016).  
Organisational change or performance is at the core of NPM and HRD initiatives implemented 
in the region. Put differently, at the crux of these concepts are the attempts to ensure that 
organisations in conjunction with their stakeholders such as employees and shareholders 
perform to their maximum potential in an effective and efficient manner.  Therefore, it is
<<<PAGE=30>>>
29 
 
incumbent upon organisations operating in Africa to explore cap acity-building initiatives 
geared to capacitate their employees with M&E skills and knowledge sets that are sensitive to 
the African context . These M&E skills and knowledge  sets are expected to enable African 
practitioners employed by organisations to implement results -based management approaches 
geared towards improving Africa’s development. The next section presents the discussion on 
organisational performance or change.   
2.4 Understanding the concept of organisational performance or change  
This section unpacks organisational performance or change as a key concept related to this 
study. This will be followed by  a presentation of factors that advance or inhibit change in an 
organisation which will provide a foundation on how change in organisations is measured or 
evaluated. Subsequently, the ECB and ECD concepts are introduced, as relevant and suited to 
these capacity-building initiatives, and are explained in detail below. 
 
As a prelude, Gravenhorst, Werkman and Boonstra (2003)  contribute that achieving 
organisational development and performance or change is complex and difficult to 
comprehend. Both the concept of organisation development (OD) and organisation change 
(OC) are defined contrarily, and used interchangeably. This study is an attempt to appreciate 
that one of  the objectives  guiding  the work of CLEAR -AA is to strengthen the  abilities of 
African institutions to conduct, commission and use the evidence from evaluations (CLEAR -
AA, 2018). This is influenced by the broader NPM principles pursued by a sizeable amount of 
public institutions located in the region (Basheka & Byamugisha, 2015). Therefore, as part of 
its work, CLEAR-AA delivers capacity building interventions such as the DETPA programme 
targeting individuals and organisations in order to contribute to the building of competent 
individuals and organisations (CLEAR-AA, 2018).   
 
 
Odor (2014) defines OD as a comprehensive approach targeted at improving the capabilities 
of individuals and teams employed in an organisation. Furthermore, an OD approach entails 
both the management team , including shareholders, and the employees of a company jointly 
collaborate towards a common objective including the increasing of  organisational 
effectiveness such as production or profits. Nyasha (2011, p. 23) contributes that OD is about 
human personnel and their organisations as well as the human factor in the organisation and
<<<PAGE=31>>>
30 
 
how both these entities function. This definition acknowledges that human beings constitute 
organisations. This definition strives to put the value on the human facet to the concept of an 
organisation. 
 Gravenhorst, Werkman and Boonstra (2003)  argue that  the primary purpose of an  OD 
approach is to certify an  organisation is fully-fledged and able to learn, supplemented by the 
opportunity to apply the lessons learnt. From the application of the lessons learnt, a new culture 
of behaving and learning is subsequently introduced and embedded with time. Consequently, 
embedded learning is expected to permeate and influence how the entire organisation functions 
(Gravenhorst et al., 2003). In this argument, a learning attitude espoused by all stakeholders in 
an organisation has been elevated as one of the primary cornerstones of motivating OD.  
In concurring with the above narrative, Nyasha (2011)  adds that the OD process entails the 
development of a timeous strategic plan outlining the broad organisational response geared to 
address the inevitable urgency for change. This strategy should be comprehensive enough to 
align various aspects of an organisation with a rapidly changing environment. As an 
illustration, Nyasha (2011) argues that thought-out training interventions to enhance elements 
such as  beliefs, characters,  values and the overarching organisational structure should be 
pursued by organisations. Admittedly, these training initiatives should be adaptable to changes 
propelled by technology, competition and the fluidity of change itself.  
On a different note, Matyesha (2011)  singles out  the role of management  and their 
competencies to drive the OD agenda in an organisation. The author argues that aside from the 
commitment by all stakeholders to developing a  new organisational culture or behaviour , as 
advanced by Gravenhorst et al. (2003), the leadership espoused by the organisational 
management is an intrinsic dependent variable. In other words, the role of management  
predetermines the success or failure of OD as an organisational scenery (Matyesha, 2011).  
.  
Implementing the concept of organisational performance or change 
 
Organisations operating in the 21 st century are propelled to adapt to the ever changing world 
of doing business caused by various factors, such as the use of new technologies, competition, 
or an increasing demand for the delivery of services (Haddadi & Yaghoobi, 2014). To a large 
extent, the above-cited factors determine the success or failure of  these organisations. Many 
organisations are concerned about how their weaknesses impede them from achieving their
<<<PAGE=32>>>
31 
 
outlined objectives enshrined in their strategic plans (Odor, 2014). These kinds of organisations 
devote their time and resources to migrate the cited weaknesses into their strengths. This is 
referred to as turning the threats (negative factors) that are capable of causing organisational 
unsustainability into opportunities (positive factors) dedicated to leading organisational 
sustainability. This pathway is popularly known as organisational change (OC). According to 
Methode et al. (2019) , the OC process encompasses a set of activities that are steered by an 
organisation with the sole objective of instituting positive changes on how things are done in a 
particular organisation. This elevates the role of evaluation in assisting organisations to 
systematically locate their weaknesses which are to be turned into strengths. This is because if 
appropriately used, evaluations  reveal scientific information that could possibly be used to 
inform and drive change in organisations (Podems, 2014). 
The quest for change in organisations is propelled by the acknowledgement that organisations 
that are non- responsive to change are at the risk of be coming extinct or alternatively being 
outsmarted by their competitors (Haddadi & Yaghoobi, 2014; Matyesha, 2011; Nyasha, 2011). 
Implicitly, this change is propelled by a certain degree of an appreciation of the need for change 
and the extent to which the respective organisation embraces the urgency for change. Important 
to note is that there are internal or external factors that instigate this need for change in 
organisations (Haddadi & Yaghoobi, 2014) . Consequently, these factors enhance or impede 
the extent to which organisational change occurs.  
Over and above the inevitability of change amongst organisations in the 21st century, the 
precision with which  change is introduced is i narguably pertinent. Odor (2014) posits  that 
change could be either be invoked in an incremental or disruptive manner. Either way, the way 
change is introduced has a direct bearing on  the adaptability or non- adaptability of 
organisations. This is because the implementation of change in an organisation has multifaceted 
implications on both the structural and operational underpinnings in an organisation (ibid.) . 
Consequently, the structural and operational pillars in an organisation have intrinsic effects on 
the conceptualisation of the strategic directive of the organisation, the implementation plan as 
well as implications on the duties and responsibilities of employees   (Methode et al., 2019) . 
This also includes the vertical relationships between employees and their managers as well as 
horizontal relationships between employees post the introduction of change in an organisation 
(ibid.). 
The overwhelming majority of the OC related research has densely used an economic and 
profit-making lens to understand the concept of OC. Scholars such as Carton (2004) , Martin,
<<<PAGE=33>>>
32 
 
Kolomitro and Lam (2014) , Odor (2014)  and Ogbu (2017)  have dedicated their academic 
careers to widening the OC debate. Significantly, this discourse was investigated from different 
lens and paradigms but dominantly from the financial lens. As an illustration, Carton (2004) 
and Odor (2014) probed organisational performance from a financial or economic lens. In other 
words, these authors explored the OC concept from the lens of the shareholders  which are 
primarily premised on a continued profit-making enthusiasm as well as a consistent pursuit for 
economic growth.  
Even though the literature articulates that financial constructs have dominated the field of OC, 
Carton (2004) shows that it is possible to assess OC through a non-financial lens. Conducting 
a non-financial assessment entails operationalising the strategic objectives of an organisation 
via the development of performance indicators that are measured on a continuous basis 
(Haddadi & Yaghoobi, 2014). Carton (2004, p. 74) refers to this integration as the balanced 
scorecard. The debate on OC of measuring non -financial assessments is identical with the 
M&E principles, which is a field that has witnessed a rapid growth in the previous few decades.  
Abrahams (2015) supports this argu ment by stating that organisations in the public sector 
prioritised evaluation as a pathway to redress their poor performance and improve government 
efficiencies.  Simply put, organisations in the public sector prioritised measuring their 
performances by institutionalising M&E as a management tool.  
Factors that advance or inhibit organisational change  
Odor's (2014) contribution is somewhat insightful in light of the current study. Odor (2014)  
proposes a two-step cycle outlining how change occurs in organisations. In the first phase, the 
author explains that the concept of OC  is largely dependent on appreciating the pillars that 
constitute the functionality or structural apparatus of an organisation. The next phase after 
understanding these pillars is to devise management approaches to ensure that suitable plans 
geared to attain OC are put in place. Inherently, this implies that the extent to which the two 
phases are grasped predetermine the level of OC in an organisation. This is because, based on 
the level of understanding of the two phases  (functionality of an organisation and inept 
management approaches), either fitted or  unfitted management strategies will be instituted . 
This underlines the importance of appreciating the proposed sequential two- step phases as 
presented by Odor (2014).  
Underpinning the organisational change and development concept is the acknowledgement that 
change in an organisation is multidimensional and occurs at different levels (Matyesha, 2011).
<<<PAGE=34>>>
33 
 
However, the majority of the studies have not  comprehensively paid attention to factors that 
advance or deter organisational change. 
 Carton (2004) isolates the role of  competent management or leadership  (among others, the 
logic behind the need for change) as the key ingredients that should be driving the process for 
change. In adding to the debate, Ogbu (2017) asserts that beyond the  competencies of 
management, there is also an expectation that these management teams will put in place a well-
thought processes that utilise s resources (employees and financial) to increase opportunities 
for  achieving  organisational efficiencies. Similarly, it is incumbent upon these managers to 
diagnose organisational gaps (such as skills  and knowledge needs of employees) that are 
capable of preventing the organisation coping with operating in the 21st century (Falola et al., 
2014). Therefore, the role of competent management  seems to be one of the important pillars 
contributing to the attainment of organisational efficiencies in delivering services.  
Appreciating the nuances of the new skills and knowledge debates can potentially predetermine 
the survival or death of an organisation. Falola et al.  (2014) argue that the human resource 
practitioners employed in the organisation should make note of these important realities. This 
will enable them to design training programmes that are tailored to reignite new, state -of-the-
art, futuristic principles which will steer organisational performance and the  organisation’s 
survival in a competitive society. This should be done in collaboration with employees. This 
narrative implies that ignoring the importance of skilled and knowledgeable employees could 
result in organisational suffocation.  
The reviewed literature tends to agree with this phenomenon. As a case in point, Werkman et 
al. (1999) contribute to this debate by asserting that the acquisition of new skills and knowledge 
by organisations is capable of unearthing the competitive edge of individual organisations. 
Ogbu (2017) corroborates this argument by raising the point that organisations ’ competitive 
edges are intrinsically linked to the role of staff or employees. This is helpful because it moves 
the focus beyond the individual organisation and amplif ies the significance of employees in 
driving organisational change.  This also resonates with the argument presented earlier by Odor 
(2014) regarding the factors which interplay with organisational change.  
This study argues that the discussed factors that advance or inhibit change in organisations are 
also applicable to the field of M&E. As noted, employee skills and knowledge are critical to 
the survival of organisations. Thus, this calls for M&E programmes to strive to ensure that they
<<<PAGE=35>>>
34 
 
build the competencies of employees employed in the organisations which are in pursuit of 
improving their performance. 
Having outlined the factors that advance or inhibit OC, the next section discuss es the 
assessment of OC.  
 
Assessing organisational performance or change 
From the onset, assessing OC remains a multifaceted area of work. Its complexities are twofold. 
Firstly, measuring OC is an internal process , which is mainly conducted by senior managers 
for a myriad of competing reasons coupled with contending expectations from different 
stakeholders within various organisations  (Haddadi & Yaghoobi, 2014) . Lastly, it often uses 
self-assessment tools for individual organisations, which gives rise to the concept of bias  and 
lacks uniformity (Carton, 2004). Potentially, this breeds an opportunity for tension between the 
management and other stakeholders. The lack of uniformity in terms of measuring frameworks 
and standards exacerbate the challenges of measuring OC. This implies that the findings of the 
assessments cannot be generalisable because they are conducted per individual organisation 
(Carton, 2004). 
 
Carton (2004) is of the view that in recent decades, very few studies envisioned to understand 
the definition of OC and how it is measured have been commissioned. This phenomenon 
includes studies such as Gravenhorst et al.'s (2003) a study which paid attention to marginal 
factors that advance or inhibit change in a various organisations located in the Netherlands. In 
some circles, this could be classified as a silo or micro way of understanding OC. Although it 
is appropriate to note that the study was linear in its approach, it is equally important to note 
that the study also provides insights in terms of the contributing factors of OC. Significantly, 
the concept of change management was eliminated as one of the key concepts.  
A different study conducted by Odor (2014)  based in an African country, specifically Benin, 
advanced the dialogue  by offering a broader definition. In tandem, the author also gave an 
account of the multiplicity of internal and external change drivers that are often capable of 
influencing change in organisational settings. Notably, the study further pays special attention 
to the factors beyond training that are responsible for causing the change in an organisational 
environment.
<<<PAGE=36>>>
35 
 
 
Gravenhorst et al. (2003) , in addition, stated that the majority of the studies  are yet to 
interrogate the links between the multiplicity of dependent and independent variables. This was 
substantiated by a study by Falola, Osibanjo and Ojo (2014)  which looked at employee 
performance in the Nigerian banking sector used a piecemeal approach. The study only focused 
on elements such as how change is anticipated to occur as opposed to investigating the how, 
when and what part of the change. Therefore, the science behind the change processes is often 
lacking. This reinforces the argument by Carton (2004) that there is a dearth of studies which 
pay attention to understanding the interconnectedness of multiple factors and their effects on 
organisations.  
Gravenhorst et al. (2003) further laments that a sizeable number of experts in this field have a 
tendency to defer interrogating the compounded complexities associated with the 
organisational change concept by opting to partly unpack some isolated parts of the 
complexities. This singular a pproach, as opposed to a comprehensive approach towards 
understanding organisational change, often identifies similar constructs such as the effects of 
institutional arrangements, ineffective communication or th e lack of t eamwork as k ey 
constraints to organisational change (Nyasha, 2011). The narrow trend of identifying similar 
trends is not sur prising because  conducting multi -pronged research on OC is generally 
convoluted and require advanced levels of different skill sets.   
The study by Gravenhorst et al. (2003) is insight ful because it does not only critique and stop 
at identifying the limitations of interrogating nominal characteristics that advance or inhibit 
OC, it equally initiates some of the techniques of mitigating the limitations. The ir study 
proposed an all -inclusive approach to  interrogating and apprehending the impediments and 
enablers of change in organisations. It contributes by emphasising a pressing need to invest in 
understanding how organisational change comes about , as well what processes should be in 
place for this change to occur. This is about understanding the change cycles and the processes 
that give rise to the opportunities to achieve the envisaged changes in an organisation.  
Despite the critics, there are some scholars who enable an understanding of this phenomenon 
in a comprehensive manner. This is informed by the acknowledgement that although there are 
limited studies that have attempted to understand measuring OC in a comprehensive manner. 
For instance, Martin et al. ( 2014) endeavoured to explain  the links between how training is 
designed and delivered, particularly as an interface between change amongst employees and
<<<PAGE=37>>>
36 
 
organisations. On the other hand, Ogbu (2017, p. 64) goes beyond training as an intervention 
and pays attention to the role of trained employees and their potential contribution to  leading 
organisational change. Put differently, the author was interested in attributing the centrality of 
skilled and knowledgeable employees and their capabilities as change agents in driving 
organisational improvement or change 
Carton (2004) proposes a comprehensive  model of how organisational change comes  about 
which is  depicted in Figure  1 below. This model identifies three major domains, namely 
operational performance, financial performance and stakeholder performance which 
collaboratively contribute towards the realisation of the broader organisational performance. 
Since the current study is not interrogating financial performance, operational performance and 
stakeholder performance are the two applicable domains for this study. Speaking from the 
private sector perspective but adaptable to the public sector, Carton (2004) states that t he 
characteristics of the operational performance construct include interventions which focus on 
improving employees skills and knowledge, the attitudes of the employees towards their work 
as well as the role of management in supporting new learnings. On the other hand, stakeholder 
performance consists of the leadership role led by senior ma nagement in identifying the need 
for and implementing new cha nges in the organisation, the support espoused by the 
shareholders to the management and the agility of the organisation towards building their 
competitive edge.
<<<PAGE=38>>>
37 
 
 
Figure 1: Dimensions of measuring OC adapted from Carton 2004 
Organisational Performance
Stakeholder 
Performance
Financial 
Performance
Operational 
Performance
<<<PAGE=39>>>
38 
 
In the case of the evaluation praxis, operational and stakeholder performance are two of the 
applicable dimensions which have been applied.  Harer and Cole (2005) argue that evaluating 
the performance of an organisation is one of the building blocks to unlocking the  possibilities 
of improving organisational performance including public institutions . The authors further 
argue that this process should appreciate the inputs of various stakeholders who are jointly 
invested either in the delivery of services or, in some cases, are the beneficiaries of the delivered 
services. This implies that the commitment to use evaluation as a management tool to measure 
performance should also appreciate the stakeholder nuances and perceive such nuances as part 
of the evaluations. This could potentially contribute towards improving developmental 
outcomes.  
Matyesha (2011) raises a different point by mentioning that technical aspects  ( such as 
frameworks and indicators ) affect the abilities of the management  team to drive change in 
organisations. This author further argues that the role of leadership plays a fundamental role in 
the success or failure of the implementation of the change in an organisation. As noted in the 
previous section, Carton (2004) highlighted the role of competent management as one of the 
key ingredients in  advancing organisational change . Methode et al. (2019)  argue that 
management are vested with the leadership responsibilities to steer the introduction of new 
changes in organisations. In addition, management motiva tes their employees to optimally  
perform their duties is a prerequisite towards achieving the broader organisational success 
(Matyesha, 2011). In the case of evaluation, a competent management team which recognises 
the role of employees during the commissioning of evaluations can determine the success or 
failure to collate and use evidence gathered for decision making and improving developmental 
results (Mbava & Dahler-Larsen, 2019) 
 
2.5 Conceptualisin g Evaluation Capacity Building and Evaluation Capacity 
Development  
The literature reviewed in this study on capacity building emanates from  a social praxis, 
specifically the fields of human resources development, organisational training, and 
psychology. The terms Evaluation Capacity Building ( ECB) and Evaluation Capacity  
Development (ECD) are two key concepts in the foundation of this study (Tarsilla, 2014). Both 
concepts have similarities in their aims and objectives, however, they are guided by different 
approaches. Ross and Hopson (2006) and Tarsilla (2014)  define ECB as a hybrid of
<<<PAGE=40>>>
39 
 
uncoordinated activities such as evaluation training, mentoring and coaching activities 
implemented in one specific and often narrow setting. In addition, Wao et al. (2017) argue that 
ECB aims to ensure that individuals are capacitated to conduct evaluations and use the findings 
emerging from the evaluations.  
By definition, M&E training seeks to develop and improve the performance, skills, 
productivity, effectiveness, and growth of individuals (Sharma, 2016) . Alawneh (2008) goes 
further to define training as a short -term intervention aimed at equipping individuals with 
sufficient skills and knowledge. The definitions  by both Sharma (2016) and Alawneh (2008)  
define training in a narrow sense, by focusing only on an individual.  In some circles, training 
is referred to as capacity building. Lusthaus, Adrien and Perstinger (1999)  provide a broader 
definition that includes beneficiaries beyond an individual. They describe capacity building as 
a process or activity that improves the ability of an individual, organisation, or institution to 
execute their mandates effectively. In other words, it is designed  to improve both individual 
and organisational performance . Therefore, it remains important to recognise that capacity 
building is designed to improve the skills and knowledge of participants , which then leads to 
improving their job and, ideally, the organisation’s performance. 
In contrast, ECD is a long- term strategy aimed at improving both the individual ’s skills, 
knowledge and attitudes as well as their organisation’s (Mbabaali, 2015; Tarsilla, 2014). This 
means that ECD is a comprehensive approach that moves beyond piecemeal interventions such 
as short course training programmes. This implies that ECB focuses on the acquisition of 
evaluation skills by individuals through training and workshop interve ntions, whilst ECD 
incorporates other external factors including the working environment of the trained individual 
(Tarsilla, 2017) . Both concepts have a plethora of meanings and have often been used 
interchangeably (Carter, 2010; Mbabaali, 2015; Tarsilla, 2014) . Since this study is primarily 
concerned with a training programme that targets individuals, the term ECB will be used when 
referring to training programmes, whilst the term ECD will be used when referring to 
interventions with a scope beyond the training of individuals. 
The ECD concept has been dominated by northern scholars (Tarsilla, 2017). In the past decade, 
several strategic initiatives that illustrate the Global South’s continuous commitment to 
integrating the ECD concept in their development have been witnessed.  As an illustration,  the  
2010 AU/NEPAD meeting attended by African Heads of States agreed to endorse the Capacity 
Development Strategic Framework (CDSF) as  a capacity-building framework (Ndashimye, 
2015). This was followed by the scoping of the capacity needs assessment research study aimed
<<<PAGE=41>>>
40 
 
at identifying capacities required to carry out the AU Agenda 2063 vision (Tarsilla, 2017; The 
African Capacity Building Foundation, 2016) . These commitments illustrate the significance 
of capacity development initiatives, and training in particular, focusing on various sectors 
including the M&E sector. 
Wao et al. (2017)  argue that most of the M&E training in Africa is purported to be aimed at 
improving weak M&E systems, increas ing the number of individuals with M&E skills and 
knowledge, and contribut ing to building the capacities of organisations to use M&E as a 
management tool. A myriad of training programmes have been developed for both entry-level 
and experienced practitioners via a two-pronged approach: the formal and the informal. The 
formal approach entails undergraduate and postgraduate programmes designed by traditional 
universities and private training providers, whereas professional organisations and other 
training providers make informal training platforms available to their association members. 
These include workshops, webinars and conferences (Wao et al., 2017).  
The descriptions of both concepts illustrate that the majority of training interventions are 
envisaged to improve both individuals and their respective organisations. The next section 
discusses common approaches to measuring the effectiveness of training towards improving  
2.5.1 Linking the role of training programmes to organisational change 
 
Understanding training and its role in organisations require s a comprehension of how change 
occurs in an organisational setting. This is because training is merely one of the approaches 
which contribute s towards attaining organisational performance. This further explains why 
organisations experiment with a myriad of training interventions accompanied by a sizeable 
amount of financial resources. This is done with the hope that this training will increase t he 
opportunities to achieve the goal of acquiring new skills and knowledge which is pursued by 
different organisations (Martin et al., 2014).  
Underwriting these training initiatives is the anticipat ion that these different  initiatives will 
contribute towards improving an organisation’s overall performances. This implies that there 
is a need to first appreciate how organisational performance or change works. Then, this should 
be followed by appropriate strategies. A regime of these strategies, such as training initiatives, 
should be seen as prospective avenues geared towards increasing the opportunities to achieve 
organisational performance or change.
<<<PAGE=42>>>
41 
 
2.5.2 The professionalisation of the evaluation field and related debate s 
around relevant skills 
 
Research has shown that there is n o equivocal answer to the question of whether or not the 
M&E field could be classified as a profession. Other views held by internationally acclaimed 
experts such as Michael Patton and Ernest House contend that evaluation is a profession, whilst 
others such as Rossi contend that it has not reached the  maturity that qualifies it to be 
categorised as a profession (Mouton, 2014). In contributing to these discussions, Levin (2017) 
poses the question of whether or not an evaluation is a profession. Despite the fact that the 
M&E p ractised in South Africa seem s to have ticked all the professionalisation boxes as 
depicted in Figure 2, the question raised by Levin (2017) remains relevant and its recommended 
that it is explored. Some of the areas to be covered include the fact that there is an established 
body of knowledge (via the developed curricula and delivery of courses), skills and knowledge 
sets, which are outlined as operating standards as ethical considerations required to operate in 
the field.  
 
 
 
 
 
 
 
Phase 1
•Practitioners 
begin to do, on 
a full time 
basis, that 
which needs to 
be done.
Phase 2
•Programmes 
to teach 
practitioners 
how to 
perform the 
task(s) are 
developed & 
delivered
Phase 3
• Professional 
associations 
are 
established
Phase 4
•Advocacy for 
the 
institutionalisa
tion and who 
is eligible and 
to practice are 
spelled out
Phase 5
• The 
emergence 
of code of 
ethics
<<<PAGE=43>>>
42 
 
 
 
 
The above depiction was adapted  from Morell and Flaherty (1978)  who presents five phases 
towards the professionalisation of a profession such as the M&E profession. Phase 1  entails 
the commitment by the practitioners to practice evaluation on a fulltime basis. This is a phase 
where practitioners are beginning to focus their attention towards establishing some building 
blocks for the profession. Phase 2 includes ensuring that training programmes geared towards 
building and enhancing the skill sets of the practitioners are being delivered by either the public 
or private training providers. This phase focuses on building capacities for new entrants so that 
they can acquire skills needed to play a meaningful role in the practice.  During phase 3 t he 
professional body gets launched formally. This is a phase where the professional body is 
established and promoted followed by activities such as electing the leadership and conferences 
are developed. Phase 4 focuses on advocating for the profession to be recognised legally 
through legislation in the county in which the profession is being practiced. Phase 5 is a phase 
where the guiding frameworks underpinning the profession are explicitly established. This 
includes setting up guidelines such as a code of ethics, professional qualifications and operating 
standards etc.  
Linked to the professionalisation debate is the overarching princ iple that the evaluation field 
has been defined differently. In the case of the Northern countries, it is  thought of as century-
old, argues Mouton (2014), whilst in the Global South, it is considered a nascent field of study 
(Levin 2017; Morkel & Ramasobana, 2017). Therefore, this could affirm that the geographical 
location and the context in which M&E is practised is an important feature in understanding 
the type of skills and knowledge sets required for the establishment of evaluation as a 
profession. Importa nt to note is that regardless of which side of the professionalisation 
discourse coin (either for or against professionalisation), there is a growing consensus that the 
quality of evaluations meant for use towards the improvement of policy and programme 
planning should be maintained (Galport & Hazzam, 2017; Podems, 2014).  
Figure 2: Five phases towards professionalisation adapted from Morell and Flaherty (1978)
<<<PAGE=44>>>
43 
 
 
2.5.3 The advantages and disadvantages of the professionalisation of M&E 
In this section, the advantages  and disadvantages  of professionalisation are presented.  
Subsequently, the debate on competencies is discussed, followed by a summary of key themes 
relevant to the concept of professionalisation. 
Various scholars have outlined the advantages of professionalising the field of M&E. For 
instance, Levin (2004) and Przybylska (2016) posit that professional M&E will lead to building 
a cohort of evaluation professionals in the field as well as contribute towards increasing the 
reputation of the profession. In turn, this helps to set the standards and principles guiding both 
new entrants and experienced evaluators. The practising standards and principles might include 
elements such as the prescription of the minimum qualification requirements (degree versus 
experience), charting professional accreditation, outlining the professional ethical 
considerations and the development of the competency list (Werkman et al., 1999). Morell and 
Flaherty (1978)  corroborate this argument by stating that beyond the reputation and the 
developed standards, professionals will further explore avenues of operationalising the rules of 
engagements by developing competencies. 
Morell and Flaherty (1978)  also outline three key professionalisation results which are: 
evaluative juri sdiction, the professional role and the professional community. Evaluative 
jurisdiction refers to regulatory frameworks within an area of work and the configuration of 
professional groups working within a particular professional field. This is intended to establish 
the relationship between the classification, tasks and the job of an evaluator. The professional 
role is defined as the expected conduct which should be espoused by an evaluator practising in 
the field. This entails ide ntifying the skills and competencies of practising as an eval uator as 
prescribed by the job market. The professional community consists of a cohort of individuals 
or group of evaluators who share similarities in terms of conduct and use of terminology in the 
field. This include s ways of working such as frameworks, tools of the trade, professional 
theories etc., embraced by the evaluation community. Overall, Levin (2017) underlines the 
importance of professionalis ation purposed to legitimise the M&E profession whilst Morell 
and Flaherty (1978) corroborate s this argument by highlighting the evaluator ’s competencies 
and their contribution to the professionalisation discourse.  
Linked to the previous discussion of context  led by scholars , such as Castro, Fragapane and 
Rinaldi (2016), Levin (2017) and Podems (2014), further emphasises the role of context. Their 
argument is that  context is helpful in tailor -making evaluation competencies in line with the
<<<PAGE=45>>>
44 
 
evaluation practice (skills and knowledge) needs. For instance, in New Zealand, competencies 
are designed to cater to the entire evaluation ecosystem: evaluators, evaluation commissioners, 
employers, facilitators and higher education institutions. The process for developing and 
implementing these competencies has involved the majority of the stakeholders in the New 
Zealand evaluation ecosystem. Whilst in South Africa, competencies have been government-
led and mainly developed to guide public officials in commissioning evaluations. Furthermore, 
they are anticipated to contribute to building government M&E capacities and the development 
of job descriptions. Besides being country- led and being applied in different contexts as 
illustrated by the above examples, some of the multilateral development  partners such as 
UNAIDS have designed competencies purposed for staff recruitment and the development of 
an overarching organisational evaluation capacity development strategy (Podems, 2014).  
Beyond the disparities on whether or not to professionalise , sufficient literature from both the 
Global North and Global South attest that developing evaluation competencies is a prerequisite 
towards professionalising evaluation. For example, Galport and Azzam (2016) , Ghere, King, 
Stevahn and Minnema (2006)  and Podems, (2014)  define evaluation competencies as 
characteristics, skills, and the knowledge that an evaluator should uphold and espouse in order 
to effectively practise in the evaluation sector. In concurring with this sentiment,  Ghere et al. 
(2006, p. 109)  note that evaluation competencies include  skills, knowledge and 
attitudes/attributes (SKAs) which are essential to practice and be competent in the field. These 
competencies are often used to set the tone of the evaluation elements such as the evaluator 
education, training and professional programmes, and qualifications as well as mapping out the 
professional paths that need to be followed. 
Galport and Azzam (2017) f urther outline that training providers respond by using the 
prescribed competencies to tailor -make training programmes that are fitted to meet the 
expectations of different evaluation communities comprising of practitioner’s and employer’s 
competencies and their ro le in the evaluation profession. Subsequently, competency -based 
curricula aimed at improving the skills, knowledge and abilities of the participants will be 
developed and delivered by these training providers. Furthermore, this curriculum is designed 
to ensure that the commissioners of the evaluation can develop directi onal terms of reference 
(TOR) prior to the commencement of the evaluation. The cited TOR should spell out the roles 
and responsibilities of both the commissioner of the evaluation and the assigned evaluator 
(Galport & Azzam, 2017).
<<<PAGE=46>>>
45 
 
Having outlined the prospects of the competency guidelines and the responsiveness of the 
training providers in developing tailored training programmes, a discord between the 
evaluation commissioners and employers (who employ the trained participants) continues. 
Employers have noted that prospective employees or current employees which have been 
sponsored to undergo training continue to lack interpersonal skills, report writing skills, project 
and team management skills, and evaluation theory (Galport & Azza m, 2017. p. 81). Galport 
and Azzam (2017) further argue that these skills have received less attention during the delivery 
of the majority of the postgraduate programmes that the participants have been enrolled for. 
These discords signify the importance of ensuring that training initiatives, which are responsive 
and tailored to the evolving skills and knowledge, continue to be enhanced. 
The majority of the literature reviewed , such as Galport and Azzam (2017) , Levin (2017), 
Morell and Flaherty (1978) and Podems (2014) , presents how a selected number of 
competencies are applied in different contexts. As an illustration, Levin (2017) uses  South 
Africa as  a case study to highlight how competencies contribute towards the debate on 
professionalisation. On the other hand, Galport and Azzam ( 2017) and Podems (2014) 
specifically cite d characteristics such as skills  and knowledge as some of the important 
evaluator competencies.  
Figure 3 presents Ghere et al. 's (2006) comprehensive framework of understanding evaluator 
competencies including their contribution to the entire evaluation ecosystem. In doing so, it 
presents the six domains of competencies namely: professional practice, systematic enquiry, 
situational analysis, project management, reflective practice and interpersonal competence. 
Each of the six categories recommends the type of skills and knowledge (both personal 
characteristics and technical expertise) required to optimally execute M&E projects.
<<<PAGE=47>>>
46 
 
Domain Description of competency per domain  
Professional practice Competencies focus on the professional norms and values that are foundational fo        
ethics. 
Systematic inquiry Competencies focus on the technical aspects of evaluations, such as design, mea      
sharing results. 
Situational analysis Competencies focus on analysing and attending to the contextual and political       
determining evaluability, addressing conflicts, and attending to issues of evaluati   
Project management Competencies focus on the nuts and bolts of moving an evaluation from the in      
negotiating contracts, budgeting, identifying and  coordinating needed resources         
manner. 
Reflective practice Competencies focus on understanding one’s practice and level of evaluation ex        
for professional growth. 
Interpersonal competence Competencies focus on the people skills needed to conduct a program evaluatio        
negotiation, and cross-cultural skills. 
Figure 3: Taxonomy of key evaluator competencies adapted from Stevahn, King, Ghere and Minnema (2005). 
A US -based study aimed at categorising the most important competencies, which was  
conducted by Galport, and Azzam (2017. p. 86), further corroborates the description of the six 
domains. They posited that the elements per domain are as follows. The element of professional 
practice is defined as the evaluator’s commitment to act ethically and to strive to uphold 
integrity, respect and honesty in conducting evaluations. Systematic inquiry has to do with the 
skills necessary to collect, analyse and interpret the data as well the skills and abilities necessary 
to conduct meta-evaluations. The situational analysis focuses on the skills and abilities to reply 
to a request for proposals including accommodating the needs of the evaluation users. The 
project management domain pays attention to evaluator’s  skills and abilities to document 
formal procedures, reporting and agreements as well as the ability to transfer skills on how to 
conduct evaluations to new evaluators or evaluation users. Reflective practice is the evaluator's 
self-awareness in terms of t heir personal skills, knowledge and characteristics. Interpersonal 
competence focuses on the use of listening or communication skills as well as skills used to 
resolve conflicts, which might arise in the process of implementing the evaluation project. 
 
At the conclusion of their study, Galport and Azzam (2017) recorded that only three of the six 
cited domains were elevated as highly important. Furthermore, there were six competencies for 
the three domains, which were documented. The three domains are systematic enquiry, project 
management and interpersonal competence. Firstly, the use of quantitative methods,
<<<PAGE=48>>>
47 
 
assessment of the reliability and validity of data were cited as the three competencies in terms 
of the systematic enquiry domain. Secondly, the  skills to develop an appropriate evaluation 
budget and the use of relevant technology were cited as the two most important competencies 
in relation to the project management domain. Lastly, cross -cultural competencies were cited 
as the most important comp etencies with regard to interpersonal competence. Galport and 
Azzam’s (2017) study further recommended that future training initiatives should pay attention 
to these competencies.
 
On the contrary, some scholars have cautioned about the over-reliance of scripted competencies 
which are envisaged to be applied in different contexts. Some of the criticism provided by 
scholars such as Podems (2014) relates to the fact the evaluation is a n evolving field. This 
means that competencies which were developed in the previous few decades  might not be 
aligned with the current practice of evaluation. This raises the timeliness and the relevance of 
current competencies anticipated to be used in the field of evaluation (ibid.).  The most 
intriguing issue is the acknowledgement that there is less research linking the development of 
the competencies to  the improvement of the quality of evaluation. Consequently, there is 
minimal evidence that attests that compiling a taxonomy of competencies leads  to improving 
evaluation practise.  The above section illustrated the usefulness of competencies and their role 
in improving individual and organisational performance.  Training has often been expected to 
contribute towards enhancing the cited competencies. However, the question of whether these 
trainings are tailored to respond to the skills needs of the participants still needs to be answered.   
2.5.4 Debates on the evaluation of M&E training programmes 
Similar to other sectors such as the education and entrepreneurial sectors, evaluation sector is 
positively responding to the call of evaluating development programme  initiatives (Aluko & 
Shonubi, 2014). This is because a substantive number of practitioners in the public and private 
officials have previously enrolled in a myr iad of M&E -related training programmes. These 
programmes are designed  to skill, reskill or upskill their M&E expertise (Basheka & 
Byamugisha, 2015). In turn, these practitioners are expected to use their M&E expertise to 
evaluate the effectiveness of programmes and policies towards improving the development of 
their respective countries (Ramasobana & Kone, 2019) . As part of the urge ncy to evaluate 
training programmes , the evaluation sector has  limited the number of  delivered training 
programmes (Wao et al., 2017) . Ulum (2015)  defines evaluation as a methodical  process 
conducted periodically with the objective of establishing the value, contribution, and adequacy
<<<PAGE=49>>>
48 
 
of training interventions . This is done  via the us age of best practices such as well-defined 
frameworks.  
Nickols (2011, p. 4)  refers to this process of prioritising measuring interventions as moving 
evaluations from the back end to the front end. The author boldly recommends that an 
evaluation assessment for each training programme should be developed from the onset as 
opposed to being treated as an afterthought. The debate on the evaluation of the training 
programme is not necessarily new and has been ongoing since the 1990s (Lingham et al., 2006). 
Evaluating training programmes is a necessary process towards collating evidence for learning 
purposes. In support of this argument, Ulum (2015) asserts that when evaluations are prudently 
and timeously conducted, insightful evidence that leads to the improvement of training 
interventions is realised. Therefore, it is highly unlikely that one can argue against the fact that 
evaluating training outcomes leads to  remarkable learning opportunities. This is largely 
because these training programmes in general were envisioned to capacitate local practitioners 
with M&E skills and knowledge towards improving themselves as individuals and together 
with their respective organisations (Nickols, 2011).  This is to say the argument feeds into the 
current and urgent evaluation debate with a particular focus on these pricey training initiatives 
which have been implemented across the region (Wotela, 2017). 
It is also undeniable that the  debate on the need to evaluate training programmes has been  
reasonably documented (Wao et al., 2017). Nevertheless, it is equally true that this area is yet 
to receive the  attention it deserves  (ibid.). One could agree with Aluko and Shonubi (2014)  
who appreciate  that some level of measuring what works and does not work in training 
interventions has been taking place. Nevertheless, it is also known that efforts to measure 
training outcomes have encountered various types of challenges. In brief, insufficient budget  
allocation is, to some extent, reinforcing the minimal effort put towards measuring training and 
evaluating M&E training programmes as elucidated by Lingham et al. (2006) and corroborated 
by Falola et al . (2014). This  widens the gap  between training budgets and budgets for 
evaluating training programmes.  
Lingham et al. (2006) caution that these mismatches are not only limited to disparities in budget 
allocations. The y argue that in some cases w here measurements of M&E training are 
conducted, questions around what, how and when to measure arise. In other words, measuring 
what works and does not work in training is inherently complex. These complexities are 
premised on the knowledge that  measuring entails  the availability of technical skills,
<<<PAGE=50>>>
49 
 
knowledge, competencies and pers onal characteristics of an eval uator (Ulum, 2015) . In 
concurring with this assertion, Lingham et al. (2006) emphasise that measuring whether or not 
skills and knowledge acqui sition occurred or, whether learning or the transfer of learning or 
retention of the lessons learnt from training, are some of the elements that bring complexities.   
Therefore, the debate around evaluating training programmes cannot be limited to budget 
constraints only, because is also dependent on the evaluator skills (Ulum, 2015).  
The disparity between the training budgets and measurement of the actual training 
interventions are not isolated from the universal M&E training debates. For example, scholars 
such as Basheka and Byamugisha (2015), and Blaser Mapitsa and Chirau (2019) have argued 
that at a macro level there are misalignments between training providers (suppliers of training) 
and those who require training ( demand for training) in the broader M&E discourse. In some 
cases, such as in the South African M&E ecosystem, the re is a high demand for skilled 
evaluators rather than  capacitated evaluators who have undergone training from a higher 
education institution  (Tirivanhu et al ., 2018). This further emphasises the misalignment  
between the M&E skills and knowledge needs of practitioners versus the training programmes 
delivered by the higher education institutions. This could support the argument of misalignment 
at a micro-level between the training budgets and training measurements made earlier by Falola 
et al. (2014).
<<<PAGE=51>>>
50 
 
Despite the above discussed disparities between the training budgets and training 
measurements, and the complexities of assessing training interventions , there is a renewed 
awareness on the urgency to eval uate training programmes delivered in Africa  (Wao et al., 
2017).   
2.6 Common Approaches t o Measuring Training Effectiveness  
The discourse on assessing training programmes is not new and not limited to a particular 
sector. There are a number of scholars, amongst others, Chang (2010), Noe and Schmitt (1986), 
Noe (1986) , Preskill and Boyle (2008) and Sharma (2016) , who have contributed to the 
literature on the effectiveness of training in the M&E and wider sectors as well. From amongst 
these, one particular model has been successfully used by a number of scholars, including 
Chang (2010) and Sharma (2016) , namely Kirkpatrick’s (1959) model. Since 1959, 
Kirkpatrick’s training effectiveness has been widely used Alliger and Janak (1989), Browning 
(1970) and Punia and  Kant (2013). This model, explained in detail in section 2.8 below, argues 
that reaction, learning, behaviour, and results a re characteristics useful in  assessing the 
effectiveness of training. In a study that tested an exploratory model focusing on participant’s 
attitudes and attributes Noe and  Schmitt (1986) found that participants ’ attitudes, values, and 
expectations are significant individual characteristics in achieving training outcomes. Foxon’s 
(1989. p. 89) study, which conducted a systematic and comprehensive literature review for the 
period 1970- 1986 (16 years) in the American and Australian journals , aimed at collating 
themes related to training and development. In their study, trainee reactions, participants’ non-
absenteeism at work  and the grading of individual  facilitator’s facilitation skills and their 
teaching techniques were recorded as key characteristics.
<<<PAGE=52>>>
51 
 
Almost a decade later, Axtell, Maitlis and  Yearta (1997) presented course characteristics, 
characteristics of the trainee, and an enabling environment as characteristics that determine 
training effectiveness . Brown and Reed, (2002) and Sharma (2016) are proponents of 
Kirkpatrick’s (1959) model, where reaction, learning, behaviour and results  are used  as 
characteristics useful to assess training effectiveness. Sharma (2016) applied this framework 
in a study aimed at assessing return on investment (ROI). None of these cited studies, which 
represent three decades of research, were conducted in an African context . Thus, this study 
attempts to contribute to the scholarly knowledge of how to measure training effectiveness in 
an African context. However, it is important to note that the current study is not generalisable; 
therefore, this  raises a need to broaden the research agenda on ECB and ECD in order to 
contribute to building a body of knowledge around understanding the effectiveness of training 
interventions on the continent, particularly in the growing field of M&E. 
Due to the substantive financial and time resources invested by organisations in training 
programmes, there have been some efforts to understand the impact of training initiatives (Noe, 
1986). In other words, organisations are interested in ascertaining return on investment or value 
for money on their training investments (Thackeray, 2016). For this reason, many studies which 
assessed the outcomes of training versus financial investments have been undertaken (Chang, 
2010; Sujatha, Lakshmi, Agyeman, & Kumar, 2014).  
Furthermore, Chang (2010. p. 14) presents three reasons to measure ROI in relation to training 
investments. Firstly, the author argues t hat it is measured in order to justify the existence  of 
and the budgets allocated to the training department. This enables the training department to 
illustrate how training contributes to the orga nisation’s objectives and goals. Secondly, it 
provides information on whether to continue or discontinue training program mes. Thirdly, it 
empowers decision-makers with information on how to improve future training program mes. 
Finally, measuring or evaluating training programmes seeks to ensure that the training 
empowers employees wit h skills and knowledge that enables them to improve their job 
performance.
<<<PAGE=53>>>
52 
 
However, as mentioned earlier, many other studies have probed training effectiveness beyond 
ROI or value for money.  In other words, these studies evaluated training programmes in order 
to ascertain whether the e mployee and organisational  performance were improved post the 
training interventions. For example,  Alliger and Janak (1989. p. 342), examined training 
evaluation criteria, focusing on the skills and knowledge of individuals. To put it differe ntly, 
the study focused on factors that influenced how training interventions contribute to expediting 
skills and knowledge  of trainees  (Alawneh, 2008; Sharma, 2016) . Brown and Reed (2002)  
contributed to the training evaluation discourse by conducting a study that investigated the 
effect of training interventions in improving organisational performance. The study focused on 
the usefulness of training beyond the classroom. Using a pre and post -test assessment, Brown 
and Reed (2002) found that a significant number of participants indicated that the training 
intervention was  useful beyond the classroom  and ha d led to the improvement of their 
organisational perfo rmance. This approach continues to be helpful in ascertain ing the 
participants’ level of skills and knowledge acquired before and after the training interventions.  
As shown above, training evaluation studies have been commissioned to investigate the impact 
of training. These studies focused on a range of things. On one hand, some assessed value for 
money, whilst others evaluated whether training contributes towards improving individual or 
organisational performance . The following section discusses the determinants of training 
effectiveness. 
2.7 Models for Evaluating the Effectiveness of Training Programmes  
Kirkpatrick’s (1959) training effectiveness model has been used to assess the effectiveness of 
training programmes (Alliger & Janak, 1989; Punia & Kant, 2013) . This model explained in 
detail in section 2.8 below has four characteristics:  reaction, learning, transfer, and 
organisation. Fundamentally, this model measures the effectiveness of training programmes on 
four levels: (a) the participants’ reactions to the programme, (b) an assessment of the content, 
or what the participants learned, (c) the participants’ performances on the job, and (d) the 
impact of the training upon the organisation (Chang, 2010, pp. 2, 22).
<<<PAGE=54>>>
53 
 
In the past 60 years, Kirkpatrick’s (1959) model has  been adapted and applied to  different 
training assessment objectives  by researchers. As an illustration, Phillips (1997) , as cited by 
Giangreco, Carugati, and Sebastiano, (2010), Subramanian, Sinha and Gupta, (2012) and Punia 
and Kant, (2013) assessed ROI. Abdulwahed and Nagy ( 2009); Chang ( 2010) applied the 
model to assess the level of skills and knowledge acquisitions, learning, job performance and 
organisational improvement post the training intervention. In this case, this affirms that the 
model is adaptable to various setting s and contexts. In the ROI study, the model  was adapted 
and commissioned to investigate whether or not an organisation ha s achieved its bottom line 
in terms of its financial revenue targets. In a different setting, the model was pragmatic in the 
systematic interrogation of  the extent to which the participants of a training programme 
consider whether learning occurred.  
 
Apart from the fact that the model has been used to assess ROI and different outcomes of 
training programmes, it is also important to mention that the model has been holistically applied 
and fitted to different contexts and settings.  
 
In support of the contextual fitness, Aluko and Shonubi (2014, p. 7) made use of Kirkpatrick's 
(1959) model to assess whether programme outcomes, training institution’s objectives and 
participants expectations and the country or regional evaluation capacity building roadmap are 
aligned and can, therefore, be succin ctly eval uated. This underscores that the success of 
training programmes is advertently reliant of a multiple of strategic factors beyond the actual 
training itself.
<<<PAGE=55>>>
54 
 
Nickols (2011)  illustrates the usefulness of Kirkpa trick's (1959) model beyond its cyclical 
nature. Nickols (2011) alludes that this model can also be used retrospectively for planning or 
project management purposes prior  to the commencement of a project or programme. As 
opposed to starting from the traditional reactions to results, one could explore starting from the 
opposite direction.  In other words, one could ask questions about what results they anticipate 
the project to achieve, followed by the type of behavioural change, the kind of learning and the 
reaction foreseen to be solicited from the project or programme (Nickols, 2011).   
In a different study conducted in Australia by Paull, Whitsed and Girardi (2016), the model 
was adapted to achieve simplicity and resear ch rigour in one of the evaluations. In this 
evaluation, the model conveniently enabled the researcher s to interrogate the effects of a 
curriculum offered in a mixed- race group of high school students. This included the s tudent 
teachers and done without interrupting the academic programme of the school. The model was 
adapted and retrofitted to jointly conduct observations and written responses from both the 
students and their teachers. Once more, this proves that the model can be adapted and used to 
collect data from different cohorts of respondents involved in a training programme.  
The historical and continuous usage of Kirkpatrick’s training effectiveness model by various 
scholars illustrates that the model is adaptable, simplistic but comprehensive enough to assess 
training effectiveness. Furthermore, this explains the dominance of the model in the literature 
reviewed. However, the dominance of Kirkpatrick’s (1959) model has received some criticism 
from some circles (Axtell et al., 1997; Turab & Casimir , 2015; Sharma, 2016; Thackeray, 
2016). Chang (2010) cautions that the majority of these assessments of training programmes 
focus on the first two levels (reactions and learning) of Kirkpatrick’s (1959) model, at the 
expense of the last two (transfer and organisational impact) levels , because measuring the 
employee’s performance on the job and the organisational impact has proved to be complex.  
However, Kirkpatrick (1959) contends that the model should be applied in a systematic and 
cyclical manner in order to realise its appropriateness. Of importance is that these researchers 
(either proponents or critics of the model)  paid attention to various elements of Kirkpatrick’ s 
(1959) model and identified different characteristics to measure training effectiveness. As 
mentioned earlier, the adaptability, simplicity but comprehensiveness of Kirkpatrick’s model 
explains the dominance and its usage by the majority of capacity -building scholars. This 
rationalises the applicability of this model in this study.
<<<PAGE=56>>>
55 
 
Various authors argue for different elements of training effectiveness. On the whole , these 
determinants are categorised into three: course characteristics, participants’ characteristics and 
working environment (Axtell et al., 1997; Turab & Casimir, 2015).  Turab and Casimir (2015) 
further debate that the primary element which determines the success of the transferability of 
training is its relevance and usefulness as perceived by the participants. By definition, transfer 
of training refers to the degree to which trainees effectively apply the knowledge, skills and 
attitudes gained in a training context to the job (Baldwin & Ford, 1988; Tracey, Tannenbaum 
& Kavanagh, 1995) . This is corroborated by Axtell et al. (1997), who posit that in the short 
term, participants’ perceptions of the course is the immediate variable that determines the level 
of transfer. Most importantly, the implementation of training informs the extent to which 
learning and transfer of training happen (Turab & Casimir, 2015; Machin, 2002; Tonhäuser & 
Büker, 2016). 
In terms of the course characteristics, pedagogical approaches applied in the delivery of the 
course and the usage of practical case studies applicable and relevant to participants’ workplace 
environments are two key variables significant towards the embeddedness of transfer of 
training within organisations (Tonhäuser & Büker, 2016, p. 139). For example, a responsive 
oriented course curriculum content customised to the working environment of participants 
plays a pivotal role in igniting the transfer of training by participants (ibid.). 
In relation to curriculum content and instructional design, Punia and Kant (2013)  provide 
practical examples. Firstly, they propose that a needs analysis should be conducted, to establish 
a participant’s skills gaps. This includes an assessment of what works and what needs to be 
improved by organisations and individuals. Therefore, it is recommended that tailor -made 
training programmes are designed as a response to the needs of organisations and individuals, 
in tandem with their context. This will enable practitioners and training providers to apply 
context-specific learning approaches that address the training needs of participants. Simply put, 
Punia and Kant (2013)  emphasise that the course content and how it is implemented remain 
some of the key elements to determine training effectiveness.
<<<PAGE=57>>>
56 
 
Besides the course characteristics and the delivery of the programme, participants ’ 
characteristics or human agency influence training effectiveness. Porter and Lawler’s (1968) 
performance model, as cited by Noe and Schmitt (1986, p. 498)  and Sánchez-Romero and 
Prskawetz (2019), mentions that personal abilities, participants’ motivation and attitude as well 
as their perception of their working environment, are individual characteristics that influence 
the effectiveness of training. 
A study conducted in Sweden by Turab and Casimir  (2015) established that individual 
evaluation knowledge, skills, attitudes, and motivation, use of evaluation findings, shared 
evaluation beliefs and commitment, evaluation frameworks and processes, and resources 
dedicated to evaluation are leading themes. According to them, these themes are the pillars of 
conducting and using the evaluation findings within organisations. This elevates the 
significance of human agency in the acquisition and use of skills and knowledge emanating 
from training interventions (Turab & Casimir, 2015).  
This is corroborated by Bansal and Thakur (2013)  who reiterate that individual attitudes such 
as motivation, personality traits, motivation to learn and self-belief to apply the skills learnt are 
some of the key determinants of  transfer of training. Their study, using quantitative research 
methods, specifically structural path modelling, evaluate d the perceptions of the respondents 
on trainee attitudes.  
2.8 The application of Kirkpatrick’s  model in previous studies   
In a study that used Kirkpatrick’s (1959) framework to test and explore model participants’ 
attitudes and attributes,  Noe and Schmitt (1986)  argue that positive reactions, learning and 
change in behaviour and improvement in individual and organisational performance are the 
intended outcomes of training programmes. However, the participant’s attitudes, values and 
expectations are significant in dividual characteristics which are useful to understanding why 
training programmes are perceived differently by participants.  
Rambau (2005) goes beyond course and participant characteristics by proposing that the 
training providers should appreciate the contextual dynamics and the needs of the organisations 
and training participants. This is followed by the role of senior management at the 
organisational level that has effects on the effectiveness of training interventions (Bansal & 
Thakur, 2013).  
Punia and Kant (2013) further suggest that the use of practical examples familiar with 
participants practices and working environments such as case studies and applied teaching in
<<<PAGE=58>>>
57 
 
training classrooms, enhance the transfer of training as well as influence the sustainability of 
skills and knowledge acquired. Therefore, it is recommended that training programme 
designers should integrate the use of case studies in their curriculum. Further, they propose that 
the curriculum should be designed in a non- cyclical but systematic manner. Moreover , it is 
suggested that the course curriculum content should assimilate participants ’ working 
environments. In addition, both the pedagogical approaches and the course curriculum content 
form an integral part in the success or lack thereof of the transfer of training (Axtell et al., 1997; 
Machin, 2002; Rambau, 2005). Therefore, training providers should consider  developing 
practical content and approaches in line with the participants’ workplace realities.   
Furthermore, Punia and Kant (2013, p. 5)  promote innovative approaches to teaching and 
learning, such as the use of technology. T he authors purport that online training uses suitable 
platforms which have the potential to redress the barriers of the traditional training 
environments which are often unbearable.  This introduces ground-breaking learning platforms 
and unique learning approaches . Some of the se learning platforms include self -paced 
instruction, mentoring, computer -assisted and web -enabled programs which depart from the 
traditional classroom and face-to-face teaching environment.  
Having presented numerous characteristics that influence the effectiveness of training,  Punia 
and Kant (2013. p 115) concluded that motivation, attitude and emotional intelligence are the 
three main participant characteristics.  
Zaciewski (2001) noted the individual’s attitudes and ability to learn  as well as  the working 
environment as key determinants  of training effectiveness . The author adds that motivation, 
self-efficacy and organi sational perceptions  are significant. For example , a working 
environment has a negative or positive influence on the effectiveness of training. Further, it is 
highly unlikely that trainees will apply new skills and knowledge acquired from a training 
programme if their line managers are not supportive.  
Foxon (1989, p. 89) conducted a systematic desktop review of evaluation literature in Training 
and Development Journals between the period 1970 and 1986. Trainee reactions, participant’s 
non-absenteeism at work, and the role of facilitators are the key findings which emerged which 
determine the effectiveness of training programmes (ibid.). Important to note is that the author 
speculates that quantitative research methods focusing on the quantification of findings and the 
use of Kirkpatrick’s model dominated the literature. This could imply that evaluation 
researchers from the North prefer to apply a positivist paradigm in assessing the effectiveness
<<<PAGE=59>>>
58 
 
of training. In addition, this might be perceived to be a research gap as well  as enhance the 
urgency to assess training effectiveness from a qualitative approach, particularly in the African 
continent. It also reemphasises the use of Kirkpatrick’s  (1959) model  as the preferred and 
dominant model.   
Almost a decade after the study by Foxon (1989), Axtell, Maitlis and Yearta (1997), argue that 
minimal research on the application of knowledge, skills and attitudes learnt during the training 
programme by participants once they are back to their working environment has been 
conducted. At the end of their study, Axtell et al. (1997)  highlighted three determinants  of 
training effectiveness : course  characteristics, characteristics of the trainee, and an enabling 
environment. These three determinants aim to assess the participant’s perceptions of the 
transfer of training.  
Bansal and Thakur (2013) and Machin (2002) elevate the importance of assessing the transfer 
of training because only ten per cent of training skills have been transferable to the workplaces. 
Therefore, evaluating the transfer of training remains important and should not be assessed as 
narrowly as  has previously been done (Foxon, 1989) . Machin (2002) further commends 
Kozlowski and Salas ’ (1997) model, which applies multi-pronged strategies in assessing the 
transfer of training before, during and after training. This model assesses the transfer of training 
at an individual, team, and organisational level (ibid.).  
Given these points, there is a need to document the three determinants highlighted by Axtel et 
al. (1997). The course characteristics aim to assess the relevance or usefulness of the course as 
well as the pedagogical approaches used in delivering the course . In other words, it evaluates 
whether the training intervention has led to improving participant’s job performance. On the 
other hand, the characteristic of the participants assesses participant’s self -belief and 
motivation to apply the new set of skills to improve their job performance. Lastly, the working 
environment aims to assess the management support and participant’s independence towards 
exploring the application of the newly acquired skills (Axtell et al., 1997). 
Axtell et al. (1997); Brown and Reed (2002); Sharma (2016) are proponents of Kirkpatrick’s 
(1959) model: reaction, learning, behaviour and results. Their  studies acknowledged that 
training programmes have played a pivotal role in  the sustainability and development of 
organisations.  
Brown and Reed (2002) emphasise that organisations have identified training as a mechanism 
to capacitate internal staff to use evidence emerging from evaluation towards improving their
<<<PAGE=60>>>
59 
 
organisational perfo rmance. Although the authors are proponents of Kirkpatrick’s  (1959) 
framework, they argue that the framework has limitations in terms of understanding the transfer 
of training. In th eir study, Brown and Reed (2002, p. 2)  presented various determinants for 
transfer of training. These include t rainee readiness; trainee motivation; opportunities for 
practice and feedback during the course; lack of similarity between the training setting and the 
job setting; lack of opportunities to apply the training on- the-job; the internal organi sational 
environment, especially boss, peers, organisational policies; and the external environment.   
 
In a different study, Sharma (2016)  applied the framework in  a study that was  aimed at 
assessing return on investment (ROI)  within organisations. Further, the author argue d that 
training providers (suppliers)  and public and private organisations (demanders)  have a 
consensus on the urgency to evaluate the effectiveness and efficiency of the training 
programmes. This urgency is informed by a need to draw lessons from the training 
interventions provided. The primary purpose of these evaluations is to create a feedback loop 
or a ‘self-correcting training system’ ( Rackham, Honey & Colbert  1971; as cited by Sharma 
2016). A well-controlled training programme is one where the weaknesses and failures of the 
programme are identified and corrected by means of negative feedback, and strengths and 
successes are identified and improved on to plan the next phase of the programme. The 
Hamblin (1974) model has the following characteristics: reaction, learning , job behaviour, 
organisation and ultimate value (Sharma, 2016, p. 201)  was an adaptation of  Kirkpatrick’s 
(1959) model and introduced a Level 5 to the framework.  
There has been criticism of how Kirkpatrick’s (1959) model has been applied. It is argued that 
attempts to assess training programmes are linear, not comprehensive and tend to measure the 
effectiveness of training skills haphazardly. As is the norm in most training programmes, post-
evaluation forms are administered immediately after the delivery of the training programme 
(Axtell et al., 1997) . This is corroborated by Foxon (1989)  who argues that budget and  time 
constraints, as well as poor technical skills, compel training providers to conduct assessments 
immediately after the training with the objective of reassuring themselves that the training was 
satisfactory. This affirms Abdulwahed and Nagy's (2009) argument that traditional classroom 
assessment methods (instructional design, tests and examinations) are the core elements which 
are assessed at the expense of skills and knowledge post the delivery of the training.  
According to Alliger and Janak (1989); Sharma (2016), in some cases, Kirkpatrick’s model has 
been perceived to be hierarchal and focuses on organisational performance at the expense of
<<<PAGE=61>>>
60 
 
participants’ reactions. They further argue that the model assumes that the four levels are 
chronological and follows a sequential order. Fundamentally, causal relationships are yet to be 
established. On the contrary, Twitchell, Holton and Trott (2000), as cited by Sharma (2016), 
argue that the model pays attention to the behavioural changes of the participants who have 
received training. In addition, the models enable practitioners to deepen their understanding of 
the training programmes in a simplistic and contextually applicable manner. 
Various scholars have moved beyond assessing skills and knowledge acquisition and focus on 
the transfer of training. Transfer of training is defined as the degree to which trainees effectively 
apply the knowledge, skills and attitudes gained in a training context to the job (Tracey, 
Tannenbaum, & Kavanagh, 1995). For example, Kolb’s (1984) four-stage learning cycle theory 
broadens the skills transfer discourse by acknowledging that  an individual’s learning 
experiences are informed by myriad characteristics which shape how they construct knowledge 
(Kolb 1984). This model  was used by Abdulwahed and Nagy (2009); Healey and Jenkins 
(2000) and Vince (1998)  who conceded that working environment experiences, previous 
training experiences, perceptions of the instructional design of the training, training provider, 
facilitators and an individual’s preferred learning style are some of the characteristics useful in 
assessing the transfer of training.  
On the other hand, Axtell et al. (1997) and Turab and Casimir  (2015) propose that course 
characteristics, participants ’ characteristics and an  enabling environment are three of the 
components that should be assessed together in order to determine the extent to which transfer 
of training has occurred.  
The research approaches and paradigms applied in assessing the effectiveness of training 
programmes are of significance. The majority of the reviewed studies (Ali 2016; Abdulwahed 
& Nagy 2009; Alliger & Janak 1989;  Chang 2010; Foxon 1989; Morkel & Ramasobana 2017; 
Wao et al. 2017) predominantly used a positivist research paradigm to conduct their research. 
Thus, in assessing the training programmes, examining the perceptions of the participants was 
not the primary purpose of the research. This provided a limited perspective into the way in 
which individuals assess and perceive the usefulness of training programmes to their skills and 
knowledge acquisition, as well as their ability to perform better in their job. 
 As an alternative, Cooley et al. (2015) illustrated how researchers can use Kirkpatrick’s (1959) 
model’s interpretive research paradigm to assess  the perceptions of participants. Their study 
evaluated the perceived efficacy of outdoor group work skills programmes for undergraduate
<<<PAGE=62>>>
61 
 
and postgraduate students and factors that influence its success (ibid). Semi -structured 
interviews with probing techniques were used to assess efficacy. This grounded  the 
researcher’s decision to use a qualitative approach in assessing the perspectives of past DETPA 
participants according to Kirkpatrick’s (1959) four-level training evaluation framework.  
This section explored the research conducted on the effects of training on individuals over the 
last couple of decades, particularly focusing on ECB. The next section specifically examines 
the components of Kirkpatrick’s (1959) model as used by various scholars (Alliger & Janak, 
1989; Brown, & Reed, 2002; Chang, 2010; Sharma, 2016)  which provides the conceptual 
framework for this study. It seeks to provide a framework to assess whether or not the DETPA 
programme has contributed to improving trainees skills and knowledge on all four levels.   
Remarkably, the majority of these cited studies, which represents three decades of research,  
were conducted outside the Africa continent. Therefore, it could be argued that contextual 
issues were not sufficiently catered for in these studies.   
In addition, these studies assessed participant’s perceptions using mixed, while this current 
study used qualitative  research methods.  The next section discusses the components of 
Kirkpatrick’s (1959) training effectiveness model.  
2.9 COMPONENTS OF KIRKPATRICK’S MODEL O F EVALUATING TRAINING 
EFFECTIVENESS 
This section specifically examines the components of Kirkpatrick’s (1959) model as used by 
various scholars such as Alliger and Janak (1989); Brown and Reed (2002); Chang (2010); and 
Sharma (2016), which provides the conceptual framework on which the study is based. 
Level 1: Reactions  
According to Kirkpatrick (1959), evaluation at level 1 seeks to measure participant perceptions 
and reactions to the training intervention as a whole. In other words, participant s’ general 
reactions to the training intervention , including their subjective views , are assessed. This is 
important because it determines the success and failures of the training intervention; moreover, 
it gauges the participant’s level of satisfaction and their attitudes towards the training 
intervention.
<<<PAGE=63>>>
62 
 
Furthermore, Kirkpatrick (1959) argues that participant’s reaction to the training content and 
its usefulness post the training intervention should be evaluated as it has a direct effect on the 
usage of the new skills and knowledge. All elements of the training should ideally be measured, 
including the facilitators’ skilfulness in facilitating teaching and learning, the learning 
environment, and the instructional design employed by the facilitator. Participant’s 
perceptions/reactions to the training programme , therefore, contribute to the improvement of 
the training intervention if relevant modifications are made. 
Level 2: Learning  
Level 2 moves beyond assessing the reactions of participants and attempts to measure learning 
in terms of positive changes in skills, capabilities and attitudes post the training intervention. 
It proposes that the change of behaviour is a reliable proxy to measure learning.  This study is 
focused on measuring participants’ perceptions of their learning, which are  identified through 
their own observations of their behavioural change as individuals.  At this level, the evaluation 
entails establishing whether learning outcomes were achieved.  
Level 3: Transfer 
Level 3 is preoccupied with establishing the extent to which employees apply the newly 
acquired knowledge and change in behaviour and performance within their respective work 
environments post the delivery of the training interventions. However, this level acknowledges 
that the applicability of the new knowledge is dependent on the opportunity for application. 
Furthermore, the model acknowledges that behavioural change is complex and requi res 
persistence. At this level, surveys and interviews are the prefer red methods to evaluate the 
effectiveness of training interventions.    
Level 4: Organisation   
Level 4 seeks to assess the effects of a training intervention towards the overall performance 
improvement of an organisation. Some of the indicators evaluated at this level include (i) higher 
revenue sales, (ii) higher productivity levels, (iii) improved quality of work and improved 
organisational culture. This is often undertaken from an economic lens as indicated earlier.  
2.8.1 Conclusion 
The analysis of the revie wed literature illustrates that the work atmosphere  and the senior 
manager’s support are elements used to assess training effectiveness. Different scholars applied 
various models to assess the acquisition of skills and knowledge. Significantly, literature posits
<<<PAGE=64>>>
63 
 
that Kirkpatrick’s (1959) training effectiveness model has been pioneering the discourse for 
over a period of 60 years (Alliger & Janak, 1989; Axtell et al., 1997; Chang, 2010). In contrast, 
the African region has limited studies assessing the effectiveness of ECB programmes  which 
were commissioned, regardless of  the rapid growth of M &E training programmes witnessed 
over the years. In addition, it is noted that positivist research approaches have been preferred. 
Thus, more research focusing on assessing participants ’ perceptions are yet to be undertaken. 
This signifies the timeliness of the current study in contributing to document ing participant 
perceptions. The next section presents the methodology which was applied in this study.  
 
CHAPTER 3: METHODOLOGY  
3.1 Introduction 
This chapter discusses the research approaches: research paradigm, strategy, research design, 
data collection, and analysis used in this study. It is structured in five (5) main parts. The first 
section presents various research paradigms and strategies that could be relevant to this study, 
and then explains the  commitment to on e research strategy  used to assess participant 
perceptions in this study . The second section highlights various aspects of the qualitative 
research design as a suitable approach for this study. The third section presents the procedures 
and methodologies used in this study. The fourth section discusses how the researcher ensured 
trustworthiness in this study. These include ethical considerations and procedures used by the 
researcher in this study. Finally, the last section discusses relevant administrative and technical 
limitations 
3.2 Research paradigm and strategy 
Chilisa and Kawulich (2012) argue that researchers have views, beliefs , and assumptions on 
what constitutes truth and knowledge. These views, beliefs,  and assumptions influence the 
individual researcher’s perceptions of themselves and the world around them. In some circles, 
it is known as a paradigm (ibid.). 
This study used a constructivist or interpretive research paradigm. Chilisa and Kawulich (2012) 
argue that the constructivist or interpretive worldview enables researchers to understand the 
world based on the research participant’s experiences. The choice of this research paradigm is 
premised on the fact that the researcher aims to understand the participant’s realities according 
to their perceptions culminating from their experiences of the DETPA 2017 interventions
<<<PAGE=65>>>
64 
 
delivered by CLEAR -AA. The justification of this research paradigm is informed by the 
contribution that this study make s to understanding participant perceptions on training 
effectiveness because there are multiple socially constructed realities (Chilisa, 2012; Chilisa & 
Tsheko, 2014). In addition, Keaton and Bodie (2011) denote that the constructivist research 
approach is preoccupied with deepening the researcher’s understandings of social meanings 
implicit in the world and avoiding the notion of causality.   
Qualitative researchers study phenomena in their natural settings, attempting to make sense of, 
or interpret, phenomena in terms of the meanings people bring to them (Alshenqeeti, 2014; 
Sarantakos, 2013). The rationalisation of the qualitative research strategy opted by the 
researcher is premised on the description elucidated by Bryman (2014) who defines qualitative 
research as a strategy that promotes the usage of words as opposed to quantification in the 
collection and analysis of data. Underpinning this strategy is the recognition that (i) individuals 
interpret their own socialis ation, (ii) the perception of individuals  changes from time to time 
on a continuous basis.  
The contextual relevance of capacity building interventions  such as the
 curriculum content 
customised to the working environments of participants  has been cited earlier. This signifies 
that initiatives such as MAE remain  important. Even though understanding the acquisition of 
skills and knowledge is broad, scholars such as Chilisa (2012) and Chilisa and Tsheko (2014) 
have been instrumental in arguing that contextually relevant methodologies and approaches, is 
important for  capacity building in the continent . This was substantiated by indigenous 
knowledge systems (IKS)  discourses which champion approaches to localise skills 
development by African researchers such as Chilisa (2017) and Keane et al. (2016). 
3.3 Research design 
By definition, research design refers to a blueprint or framework of how the researcher plans 
to conduct their research: it justifies the methodology applied; the method of data collection; 
and techniques for analysing the data (Wagner,  Kawulich & Garner, 2012; Wotela, 2016) .  
This study applies a qualitative research design as it seeks to deepen the researcher’s 
understanding of the perceptions of the DETPA 2017 programme as opposed to quantification 
or generalising of results.  In addition, Sarantakos (2013, p. 120) argues that research design 
comprises two major phases: the first phase entails planning the research activity, whilst the 
second phase entails the execution of the research.  The seven stages process developed by 
Steiner Kyle and cited by Babbie and Mouton (1998, p. 290), deconstructs these two phases in 
a more detailed manner, which w as used as a guiding framework for this study. The stages
<<<PAGE=66>>>
65 
 
include the following: thematising, designing, interviewing, transcribing, analysis, verifying 
and reporting, and each of these was applied in this study.  
3.4 Data and data gathering 
The researcher collected primary data, which refers to the data collected internally or externally 
for the first time at the source of the collection (Wagner & Kawulich, 2012; Sarantakos, 2013). 
The constructivist or interpretivist research paradigm compelled the researcher to acknowledge 
their subjectivity and biases steered by ontological, epistemological, and axiological paradigms 
during the data collection process (Wagner & Kawulich, 2012) . This was taken into 
consideration when designing the interview questions, as well as the interview process itself, 
and required consistent mindfulness of the researcher.  The study lent itself well to respondents’ 
subjective description of their perceptions and experiences, as learning and behavioural change 
are not linear events, and may be influenced by a number of internal and external variables 
within and outside of an individual.  This required a deep reflection of personal experiences 
and perspectives by respondents.   
In doing so, the researcher conducted interviews with ten (10) DETPA 2017 participants, one 
(1) DETPA facilitator, two (2) DETPA moderators and four (4) line managers of participants 
as the main data collection source.  Interviews are defined by Gopane (20 12, p. 10) as a two-
way conversation in which the researcher seeks answers from the participants by asking them 
questions about the phenomenon under investigation (for example, the participants ’ beliefs, 
ideas, views, opinions, and behaviours). Interviews were conducted via Whatsapp call, face-
to-face or Skype.  In particular, semi-structured interviews were conducted, for which a semi-
structured interview schedule was designed by the researcher. The interval of the study was a 
year post the implementation of the DETPA 2017 programme.  
 A semi -structured approach has benefits for both the researcher and the interviewee. For 
example, an enabling environment was available to the interviewee to speak through their own 
voices in order to express their opinions and feelings (O’Keeffe, Buytaert, Mijic, Brozovic, & 
Sinha, 2015). Therefore, a broader snapshot was built, words w ere analysed and interviewee 
perceptions were captured (Bryman, 2014).  Thus, the researcher scheduled thirty minutes  of 
interviews with respective respondents purposed to collect data  (see Appendices 1.1 for the 
data collection instrument). 
Stemming from the researcher’s interest to accurately capture people’s experiences, open -
ended questions and non- directional questions were used (Alshenqeeti, 2014; Wagner, &
<<<PAGE=67>>>
66 
 
Kawulich, 2012).  This is in line with the interpretivist approach within the qualitative 
paradigm, where the individual perspectives of respondents are of critical importance (Chilisa 
& Tsheko, 2014). The researcher sought permission to record and transcribe the responses of 
the respondents whilst conducting the interviews with the respondents. The transcribed 
interviews were emailed to the respondents for validation purposes. In other words, the 
researcher ensured that the responses elucidated from the respondents were captured verbatim 
and represented respondents’ own perceptions, as envisaged by the research. 
 Thereafter, the researcher analysed the data gathered from the semi-interviews conducted with 
CLEAR-AA’s DETPA 2017 participants, line managers, moderators and facilitator (O’Keeffe 
et al., 2015).  The responses to the questions informed the key findings, then conclusions were 
drawn using key findings as a basis.    
3.5 Sampling 
Traditionally, the sampling procedure used for qualitative studies is  not designed to achieve 
sample representivity (O’Keeffe et al., 2015).  Steered by the qualitative research design, both 
purposive and convenience  sampling was the appropriate technique because it enabled the 
researcher to collect data from respondents who are able to provide in- depth information on 
the subject under study (Babbie & Mouton, 1998; Yin, 2016). This is informed by the fact that 
each participant offered different views based on their different locations in the continent, their 
different positions and level of seniority and the nature of their work.  For this reason, purposive 
sampling remained the most appropriate method (Yin, 2016) , and convenience sampling in 
others.  
 
3.6 Data analysis 
Alshenqeeti (2014) acknowledges that analysing qualitative interviews tends to be 
overwhelming and time -consuming.  Sufficient time was therefore allocated to the data 
collection and analysis process.  The researcher gathered rich data from the fourteen (14 ) 
interviews. Thematic Content Analysis, as defined by Gopane (2012), as a process of looking 
at data from different angles with a view to identifying keys in the text that will help us 
understand and interpret the raw data, was used.  In turn, the key concepts were coded. Data 
coding refers to a process whereby the data sets are broken down into their component parts 
and those parts are then given labels (Henning, van Rensburg, & Smit, 2004). The process of 
data analysis required the resear cher to read and reread the transcripts, as this enabled the 
researcher to accurately document the themes and sub -themes emerging from the data
<<<PAGE=68>>>
67 
 
collected. This iterative process enabled the researcher to inductively identify and document 
concepts or themes that emerged from the datasets. In turn, the researcher compared the themes 
from the interviews against the ones drawn from the conceptual framework (ibid.).  
3.7 Validity and reliability 
Qualitative studies are more concerned with trustworthiness than generalisability . This study 
acknowledges the importance of trustworthiness, which is aimed at ensuring credibility, 
dependability, and confirmability of the findings (Wagner, & Kawulich, 2012). In order to 
ensure these, the researcher provided a detailed description of the research procedures and the 
theoretical framework guiding the study. Secondly, the researcher defined the research process 
in detail, documenting: the research problem, dat a gathering, analysis, and report writing 
process. The researcher kept records of emergent themes, responses to the research questions 
as well as any amendments of the research questions.  The researcher piloted the interview 
schedule with two participants in order to establish the appropriateness and clarity of the 
interview questions. Piloting enabled the researcher to pre -empt tentative research challenges 
and gave early signs on whether or not the research questions, instruments and approaches 
avoided being complicated and unfriendly to the interviewees (Phawe, 2016). During the pilot, 
interviewees were given an opportunity to comment on the research questions. This empowered 
the researcher to amend the questions when deemed necessary.   
3.8 Limitations and delimitations 
The constructivist/interpretative research paradigm implores the researcher to acknowledge 
their axiological, ontological and epistemological biases during the data collection process. 
Hence, the researcher recognises that his employment by CLEAR-AA has the potential to deter 
the researcher’s impartiality during this study. In addition, the researcher is involved in the 
conceptualisation and implementation of the DETPA programme;  therefore, this implies that 
there are cordial relationships with participants . Therefore, this raises the issue of the 
researcher’s bias. In order to mitigate the limitation of bias, the researcher interviewed different 
categories of respondents as a way to achieve triangulation of data. This explains why four 
cohorts comprised of DETPA participants, moderators, facilitators and participants line 
managers were interviewed by the researcher.  
The use of technology via Skype and Whatsapp was cumbersome and slightly delayed the data 
collection process. This was because respondents came from different countries with disparities 
in the quality of internet connections. As a result, the researcher , in agreement with the 
participants, opted to reschedule some of the interviews. In cases where the internet connection
<<<PAGE=69>>>
68 
 
persisted (such as was the case with respondents from Benin, Ghana and Kenya) to be 
problematic, the researcher in consultation with the participants emailed the interview guide to 
the respondents for self -administration. Self-administration of the interview guide was also 
extended to some of the respondents who were travelling in remote areas with less access to 
the internet. This included two respondents comprising respondents from Ethiopia and Uganda 
who were on a mission during the data collection process of this study. Overall, the number of 
the self-administration respondents are as follows: three (3) participants, one (1) line manager 
and one (1) moderator.  
Guest, Bunce and Johnson (2006) explain that using purposive sampling often result in rapid 
data saturation. These authors further allude that s aturation implies that no new evidence or 
themes are observed from the data. M ason (2010) further agrees that
 data saturation happens 
as soon as after the fifth interview. In the case of the current study, the researcher checked for 
outliers and widely divergent views from all the self -administered questionnaires. This then 
confirms that saturation had occurred with the rest of the interviewees. 
3.9 Ethics  
Bryman (2012); Creswell (2007)  both agree that ethical cons iderations should be integrated  
into the conceptualisation, implementation and report writing processes of each research 
project. Nhlabathi (2016) further adds that this i s cognisant of the fact that research involves 
gathering data from respondents whose responses ought to be confidentially respected and 
protected and respondents should not be harmed at any given point during or after the research. 
In summary, all three cited authors agree on the following ethical principles: respondents 
should consent to participat e in each research, the researcher should cease from deceiving 
participants and the researcher should uphold confidentiality as well as commit not to cause 
any harm to the research participants (Nhlabathi, 2016) . Therefore, respondents provided 
verbal consent to participate in the study to the researcher.  
The researcher disclosed  to the respondents at the beginning of the interviews  that he works 
for CLEAR-AA, which provided tuition fees for this study, as well as the bursary provided by 
Twende Mbele. However, the researcher indicated that no financial benefit was accrued during 
this research.  
 Post the presentation of the research proposal to the
 University Proposal Defence Committee, 
which approved the research topic and research proposal, the researcher obtained permission 
from the Wits University Ethics Committee prior to commencing with this research. An ethics
<<<PAGE=70>>>
69 
 
clearance letter was issued which the researcher to conduct interviews for data collection then 
used. In practice, the researcher adhered to the ethical norms and standards of research as 
envisaged by the Wits University Ethics Committee.  
3.10 Informed consent 
The researcher is persuaded by Silverman (2011) who argues that the guiding principle of 
informed consent is an individual's personal right to agree (or not) to participate in a research 
study after fully understanding the total research process and the consequences. As a result, the 
researcher designed a consent form which requested the participants to sign as a confirmation 
of their willingness to participate during the study (see Appendix 1.3). Phawe (2016) describes 
that informed consent is designed to ensure that prospective participants are informed about 
the purpose and procedure of the research study which might influence their decision to give 
consent to participate in the study. An informed consent form requesting the DETPA 2017  
participants’ participation in the study was developed. Prior  to the commencement of each 
interview, the researcher spelt out the aims of the research, how participants were sampled, as 
well as clarifying confidentiality and voluntary participation of the participation. Furthermore, 
the researcher asked permission to transcribe the responses of the participants during the 
interviews. In some cases, the respondents preferred verbal consent as opposed to signing the 
consent form.   
The researcher disclosed the fact that participants are permitted to use their discretion to 
withdraw from the study when deemed necessary and at any given point. The principle of using 
pseudonyms to protect participants was disclosed to the respondents.  The researcher ensured 
that the records of the consent forms and transcribed interviews are kept safely, the details of 
which are explained below. 
3.11 Confidentiality and harm to participants 
The researcher ensured confidentiality to the participants in data collection and analysis. This 
was done by mechanisms that include storage and archiving of documentation that was put in 
place by the researcher in order to maintain the confidentiality of the records. This includes the 
fact that  the researcher alone conducted  interviews and transcribed them . Furthermore, the 
informant’s information was archived in a locked (password) folder on the researcher’s laptop 
and external hard drive. The researcher asserted confidentiality to respondents by making 
available the preliminary findings of the research to respondents. At the completion of the 
analysis, respondents were affirmed that their names were not mentioned in the report.
<<<PAGE=71>>>
70 
 
CHAPTER 4:  DESCRIPTION OF RESULTS  
4.1 Introduction 
This chapter presents the findings from the interviews conducted with fourteen (14) 
respondents comprising the DETPA 2017  participants, moderators, facilitators and 
participant’s line managers.  
This study aim ed to assess the participants ’ perception of the effectiveness of the DETPA 
programme. DETPA was used as a case study. The primary research question that this study 
addresses is participant perceptions regarding the DETPA  2017 programme as delivered by 
CLEAR-AA, and specifically, whether  it has been effective across the four components  of 
Kirkpatrick’s (1959) model which are reaction, learning, transfer and organisational impact as 
posed by this research. The research questions aimed at assessing each of the Kirkpatrick 
training effecti veness model ’s four levels , including the gaps i n programmes such as the 
DETPA programme. S ections 4.2 to 4.6 present the findings that respond to the research 
questions, through the qualitative analysis of the data including the steps undertaken in the 
analysis.  
The researcher conducted semi -structured interviews with ten (10) DETPA participants; one 
(1) DETPA facilitator, two (2) DETPA moderators and four (4) line managers of participants 
(see Figure 4). A formal invitation to participate in the study was sent via email and Whatsapp 
to the 2017 cohort, which consists of DETPA participants, facilitators, moderators and 
participants’ line managers.  A census approach was applied to the participants, facilitators and 
moderators, where the entire population was invited to participate in the study.   
Convenience sampling was thereafter employed, where the first respondents to the invitation 
were selected for interviews.  In the case of participants’ line managers, a purposive approach 
was adopted wherein a maximum of n=four (4) line managers, who were willing to participate 
in the study at the request of their direct reports, were interviewed. The DETPA participant 
population for 2017 was fifty-five (N = 55), moderators (N=three (3), and facilitators (N= one 
(1) and participants (N =ten (10).  The sample  size was as follows:  the first ten DETPA 
participants who responded to the communique from the researcher were interviewed (n = 10). 
This was followed by the first two moderators (n = 2) and a minimum of one facilitator (n = 
1).
<<<PAGE=72>>>
71 
 
 
Figure 4: Profile of the respondents  
The chapter also  provides themes generated from the analysis. Several important themes 
emerged from the study: positive feedback that the overall programme improved and sustained 
the transfer of skills and knowledge; mixed feedback around the change in behaviour, and 
proposed areas of improvements. Below , the chapter presents a collation of some of the 
selected responses to converge the respondents’ views to the above themes.  
4.2 Participants’ overall reaction to the DETPA programme 
 
Primary research question 
What are participants’ perceptions regarding the DETPA 2017 programme in terms of its 
effectiveness in respect of Kirkpatrick’s four levels of training effectiveness: reaction, learning, 
transfer and organisational impact?  
Secondary research question number one, which examines DETPA participants’ overall 
perceptions and reactions to the DETPA 2017   programme is addressed in this section. Overall, 
both participants and moderators had a positive reaction to the programme as a whole. Some 
participants simply remarked, “Positive! A good initiative”  (Participant 2). The key themes 
that emerged from participants’ reactions regarding the overall programme include the 
relevance of DETPA  the insightfulness of the programme, and perceptions that it had 
contributed to participants skills and knowledge acquisition.  
(59%)(23%)
(12%)
(6%)
RESPONDENTS
Participants Line managers Moderators Facilitator
<<<PAGE=73>>>
72 
 
In the main, both participants and moderators perceived the programme to be of good quality. 
There are wide-ranging explanations, which were provided by the respondents as a justification 
of why the programme was perceived to be of good quality. One of the cited reasons included 
the timeliness of the programme.  It was implied in some responses that the programme was 
long overdue and was timeous, as one remarked, “…first, it was a long overdue intervention 
for a regionally contextualised programme after the global one (IPDET) ” (Participant 1). In 
support of this sentiment, one respondent said: 
“So, the emphasis on ‘made-in-Africa’ is very much needed to bring in the cultural 
sensitivities and well as clearly articulating the demand for and utilization of evidence within 
the continent because my observation is that this is still external and not endogenously 
located” (Participant 2). 
Overall, both participants and moderators had a positive reaction to the programme as a whole. 
The key themes that emerged from participants’ reactions regarding the overall programme are 
the relevance of the DETPA  and insightfulness of the programme , which were  perceived to 
have contributed participant’s  skills and knowledge acquisition. These themes are presented 
below. 
Some participants simply remarked, “Positive! A good initiative” (Participant 2). The themes 
that emerged in assessing respondent’s perception of the overall programme are good, relevant, 
and insightful and contributed. In the main, participants and moderators who were interviewed 
perceived the programme to be of good quality. There are wide -ranging explanations, which 
were provided by the respondents as a justification of why the programme was perceived to be 
of good quality. Some of the cited reasons included: the timeliness of the programme, this 
implied that according to the respondents, the programme was long overdue and was timeous, 
as one remarked, “ …first, it was a long- overdue intervention for a regionally contextualised 
programme after the global one (IPDET)” (Participant 1). An inference could be drawn from 
this statement that this programme could have been perceived to contribute towards increasing 
a cohort of practitioners with evaluation skills to conduct and commission evaluations in the 
region as envisaged by the objectives of the DETPA programme.  
Additionally, one of the respondents remarked that the programme should be scaled up;  
“DETPA is an overdue intervention that should be scaled up in terms of attendance. 
There are many commissioned evaluations (and other related assessments which might
<<<PAGE=74>>>
73 
 
be called evaluations) within the continent and which need capacity augmentation to 
those carrying out the assignments” (Moderator 1). 
Respondents indicated that such training could ensure that local experts capacitate African 
evaluators with relevant examples without travelling outside the continent for a similar training 
programme. In emphasising this point, one of the respondents asserted that; 
“Well-designed program to enable Africans to be trained in African realities instead of 
travelling to Canada or elsewhere for the similar but contextually different programme. From 
this point of view , it was relevant and once allowed the Twende Mbele team to gain global 
evaluation competencies as well as learn useful evaluation tools” (Participant 6). 
This implies that there is a growing perception that locally based facilitators are familiar with 
the nuances and examples that are related with the evaluation practice in the region.  
 Additionally, the programme was perceived to have been well conceptualised and thought out 
because its curriculum was contextually designed to respond to an African context. For 
instance, one respondent stated that  
“thinking about the programme and the rationale behind it, there was an attempt to ensure 
that M&E training is sensitive to the Africa context” (Participant 2). 
Another participant echoed a similar sentiment by saying, “ …in addition, it covered the 
contextually relevant topics such as how evaluation fits into the African context by delving into 
the MAE concept” (Participant 3).  
“I learnt a lot from a conclave that gathers African minds and share pertinent issues on 
capacity building for evaluation practice. The gathering also creates networking as 
participants bring on board their own unique challenges from their different countries in the 
region” (Participant 1) 
The programme was commended for including a stream for entry-level participants and another 
one for experienced M&E practitioners, which means the programme, was tailored according 
to the needs of practitioners. As stated in one participant’s comments,  
“I also liked the fact that we had foundations for “beginners” and advanced for those already 
practising, thus giving everybody the opportunity to learn” (Participant 4).   
This ensured that various M&E practitioners who have various responsibilities and levels of 
authority in their respective organisations are accommodated in the programme. A respondent
<<<PAGE=75>>>
74 
 
who commented that affirmed this sentiment,  “the programme was attended by middle and 
senior-level African participants across 11 multiple countries in the continent” (Moderator 2).  
In addition, the DETPA co-ordination team were stated as being at  the forefront of providing 
participants with a pleasant experience during the delivery of the programme. To put it 
differently, the co- ordination team positively contributed to participants and moderators ’ 
perceptions in terms of their in- and out-of-class experiences. Thus, co-ordination was cited as 
one of the reasons elucidated to justify the good quality of the programme for example; “It was 
participatory, and the organisation was great,” (Participant 5) while another said, “It was well- 
organised and the co-ordinating team was hands-on” (Participant 6).  
 
This positive feedback was also mentioned in the assessment of the role of facilitators as well 
as their teaching approaches. One of the moderators applauded the decision to utilise 
experienced facilitators who are of African descent; “In addition, the majority of the facilitators 
who worked during the delivery of the programme were of Africa n descent” (Moderator 2). 
According to him, this is one of the good gestures towards ensuring that Africans are in the 
forefront of using their in-house expertise to facilitate training sessions as well as creating an 
enabling environment for learning and knowledge sharing platforms.  
The respondents appreciated the instructional design and andragogy, which were employed by 
the local facilitators during various sessions;  
“Facilitators did their splendid role particularly in orienting the learning experience 
towards adult learning methodology. Practical examples were used stemming from real 
cases. The passion was there especially on the ‘impact investing’, ‘made -in-Africa’ and 
cultural sensitivity; and the rigorous impact evaluation module” (Participant 7). 
The use of learning by doing exercises; group work and case studies were perceived to be useful 
in guiding the perceptions of the programme. One of the respondents confirmed this by saying 
“The use of exercises and other interactions among the participants contributed a lot” 
(Participant 8). Another appreciated the manner in which facilitators engaged with participants 
saying;  
“All of them were fantastic, at no point in time, they were never found to be dismissive to 
participant’s questions (always willing to listen to participants). They used andragogy 
approach in teaching their courses. More especially Lewis ” [One of the facilitators]  
(Participant 4).
<<<PAGE=76>>>
75 
 
 
Although the feedback from the respondents was generally positive, however, time allocation, 
logistics, curriculum content and unequal quality of facilitators were cited as some of the 
limitations of the programme. The issue of quality of facilitators was captured in the words of 
a respondent who said “Inconsistent quality. Some good, others novices” (Participant 3). One 
of the participants cited that there was less time allocated to exhaust all the allocated curriculum 
content per session;  
“The content is quite good, but one needs more time for learning in such a very short time. It 
simply means learners need more time or you might consider reducing the number of topics or 
modules” (Participant 9).  
This has the potential to deter participants from the learning process envisaged by the 
programme.  In addressing the limitation of time and curriculum content, some of the 
respondents proposed that the curriculum content be reduced and that case study approaches 
are integrated and applied by all facilitators. Thus, they suggested,  
4.2.2 Key theme: Relevance of the DETPA programme  
Participants perceived the programme to have been relevant. As an illustration, nine out of the 
ten participants interviewed underscored that the programme was relevant. The moderators of 
the programme corroborated this. Some of the elements that were cited to justify the relevance 
of the programme included  a peer-to-peer learning approach, which entailed enabling 
participants to share their practice- based experiences from several  countries and sectors. A 
participant who noted that expressed the latter point on diversity:  
“The course design enabled for diversity in skillsets allowing for the different participants to 
learn from one another, enabling for implementation of the different approaches back home” 
(Participant 10).  
In tandem, the programme was perceived to have been tailored to meet the capacity needs of 
African practitioners. Significantly, respondents indicate d that the programme created  a 
networking platform for practitioners to connect and to establish professional relationships.  
 
The ‘Made in Africa’  approach was a contextually appropriate approach to instil and 
disseminate the importance of cultural sensitivities in evaluations commissioned in Africa.  
Respondents referred to the approach in their remarks;
<<<PAGE=77>>>
76 
 
“So the emphasis on ‘made -in-Africa’ is very much needed to bring in the cultural 
sensitivities and well as clearly articulating the demand for and utilization of evidence 
within the continent because my observation is that this is still external and not 
endogenously located” (Participant 1). 
Another one also highlighted the significant contribution of DETPA to improving capacity for 
M&E;  
“DETPA is an important contribution towards improving the quality of M&E service 
delivery in the continent. The Program me was developed based on a number of 
observations and assessments which revealed M&E capacity gaps at different levels in 
Africa. The training contents covered by DETPA meet M&E capacity demand and 
stakeholders’ expectations to some extent” (Moderator 1). 
However, it is worth noting that not all the participants perceived the programme to have been 
wholly relevant. For instance, one respondent remarked,  
“Training. Ok, but not sufficiently customised for the government sector” (Participant 4).   
Although the comment by the respondent is important, the DETPA programme was not 
envisaged to cater for a specific sector. Perhaps, in the future, the project team should consider 
customising the programme for specific sectors such as government or development sectors. 
 One of the respondents proposed areas of improvements with the objective to ensure that the 
programme attains far-reaching relevance. For instance, a government official emphasised that 
the programme should be densely tailored to address the capacity needs of the public officials 
working in the government sector. In addition, some of the rationale behind why the 
programme was perceived not to be comprehensively relevant included the fact that some of 
the content was not fit for purpose. By way of an example, one of the respondents attested that 
“the content of the Impact investing module was perceived to be incoherent” (Participant 6).  
4.2.3  Key theme: Insightful and good use of participant’s time 
In answering the question regarding whether the DETPA programme has been a good or poor 
use of their time , the  majority of the respondents , constituted by both the participants and 
moderators, cited that the DETPA programme was an insightful and good use of their time. 
Unlike some of the responses that highlighted some of the limitations mentioned earlier by the 
respondents, such as time limitations, there w as a consensus amongst participants and 
moderators that the programme was insightful. Some of the elements, which were quoted as
<<<PAGE=78>>>
77 
 
responsible for the insightfulness of the programme, included the perception that the 
programme was culturally sensitive. This means participants perceived the programme to have 
made strides to appreciate the contextual and methodological approaches fitted for the 
continent. This is helpful towards capacitating local evaluators with the ‘fit for purpose ’ 
evaluation tools and methods. 
Similar to the relevance theme discussed earlier, the MAE approach was elevated as one of the 
key components that grounded the perception o f the insightfulness of the programme. For 
instance, one respondent said,  
“It was good because it gave me tools, knowledge and skills relevant to my situation.” 
(Participant 4).  
This is because MAE was perceived to have been a useful case study approach which promoted 
African scholarship and was perceived to be fitted to address the regional capacity needs and 
gaps of government employees and practitioners. Thus, the curriculum content was cited as 
one of the determinants that contributed to the perception that the programme was insightful in 
accordance with the respondents . One of the respondents highlighted the importance of the 
content by saying; 
“If one uses content and in-class participation. One hopes that it would have helped in 
shaping their thinking around context relevance and specific evaluation skills and 
knowledge informed by the sessions discussed during the programme” (Moderator 1). 
4.2.4 Key theme: Improvement in the skills and knowledge acquisition 
 
Both the participants and moderators reached an agreement that the programme has contributed 
towards improving their skills and knowledge. As an illustration, all ten participants and two 
moderators concurred that the programme contributed to their skills and knowledge 
acquisition. Some of the participants have further indicated that participating in the programme 
has led to their job promotions and they are willing to recommend the programme to their 
associates and colleagues. Some of the elements that were cited as evidence for the contribution 
of the programme are as follows : Firstly, the programme was perceived to have value for 
money. In other words, participants and moderators believe that both financial and time 
invested in the programme was economically justifiable ; Secondly, respondents cite that the 
programme has contributed towards skills and knowledge acquisition on how to commission 
evaluations in their respective professions. As an illustration, one of the respondents said;
<<<PAGE=79>>>
78 
 
“In terms of knowledge, Yes. Theoretically, I know how to prepare a randomised 
control trial (RCT). I can read and understand studies that have used RCT and able to 
critique. In terms of skills, there are limited opportunities to apply the ir skills. The 
whole discussion around MAE has been helpful because I become sensitive to the 
concept of inclusion and the context in doing my work” (Participant 5) 
Another respondent who cited the importance of skilled facilitators and the relevance of content 
corroborated this, alluding that; 
“The first reaction observed from the 2017 participants during the training sessions 
had shown some enthusiasm and satisfaction of these participants. It will be quite 
difficult to ascertain the improvement of their skills since the interaction had only taken 
place in the classroom during the training session. However, given that the training 
contents are more or less relevant , and that the facilitation was appropriate, one can 
assume that the training had contributed to the improvement of participants’ skills and 
knowledge” (Moderator 2). 
4.3 Learning attained during the DETPA programme 
 
In answering the question focusing on assessing the participant’s
 perception on whether or not 
they have learnt from participating during the delivery of the DETPA programme, four 
categories of responses emerged from the findings: intended ( the programme contributed to 
participants learn ing what was intended to be learnt ), contributed ( the programme made a 
contribution to participants skills and knowledge ), advanced ( the programme contributed to 
advancing participants skills and knowledge sets ) and undecided (participants were uncertain 
of whether the programme ).  The categories of responses are examined in detail below.  
4.3.1 Learning Intentions Achieved 
In answering the question focusing on assessing the participant’s perception on whether or not 
they have learnt from participating during the delivery of the DETPA programme, the 
respondents elucidated four categories: intended, contributed, advanced and undecided.  
The majority of the respondents cited that the DETPA training enabled them to attain learning 
as envisaged in the learning outcomes of the programme as well as their own intentions to 
learn. One remarked that;
<<<PAGE=80>>>
79 
 
“I had a little M&E background before, but I had a challenge with the theory of change. 
The instructors emphasized concepts and related them to our African context enabling 
for applicability and learning. By the end of the course, I had achieved my intention to 
learn the Theory of Change” (Participant 3) 
4.3.2 Contribution to Learning 
The respondents asserted that their participation in the programme enabled them to learn new 
as well as improve their M&E skills and knowledge. Hence, the programme has been perceived 
to contribute to their intention to learn and acquire new M&E skills and knowledge.  
Additionally, the respondents highlighted some of the practical advanced skills and knowledge 
learnt because of their participation in the programme. As an illustration, respondents indicated 
that the programme has enabled participants to acquire various a dvanced evaluation technical 
skills and knowledge. This was articulated via a comment made by one respondent who 
indicated that; 
Clear and practical lessons and skills on developmental evaluation was imparted as 
intended as well as highlighting cultural sensitivities within Africa, which need to be 
embedded in the practice” (Participant 6) 
These include analysing and using quantitative data for reporting and planning purposes. Some 
of the practical skill sets learnt included: how to design terms of references (TOR) for 
conducting and commissioning evaluations, evaluation methods and approaches as well as skill 
on how to conduct and commission Impact evaluation. One of the respondents remarked that 
she has applied the practical skills learnt already;  
“Of course, I learnt, I got the opportunity to evaluate a programme in 2018. I 
encouraged the client to move beyond secondary data and start to plan for impact 
evaluation from the start of their programmes. This was because it was difficult to 
attribute the impact as it was not planned. From such an institution, their staff have 
registered for an M&E course” (Participant 3) 
One of the respondents also mentioned that they also learnt about the theory of change 
concept. In their own words, the respondent said;  
“I am able to perform my roles both voluntary at work from a more informed point of view 
especially with the idea of the Theory of Change at the back of my mind” (Participant 4)
<<<PAGE=81>>>
80 
 
4.3.3 Learning Advanced 
According to the respondents, the programme has contributed to participants learning skills 
such as conducting quality assurance in their respective working environments, improving their 
planning and reporting skills as well as facilitation skills. Additio nally, the programme was 
perceived to be a learning platform. Practical and theoretical evaluations tools such as 
developing logframes and the philosophical approach of MAE curriculum content was cited as 
a gateway that facilitated learning to occur. In tandem, they have learnt advanced skills such 
as policy planning, promotion, including the institutionalisation of evaluation from different 
countries. Thus, it can be argued that the programme promoted peer learning amongst 
participants from different countries as well as learning both the theoretical and practical M&E 
tools. To illustrate how the programme facilitated peer learning, one participant commented 
that; 
“Participation in this program made it possible to learn more about evaluation 
practices in other countries and African institutions such as the African Union. It made 
it possible to know the level of institutionalisation of the evaluation at the level of 
certain countries like Niger, Ghana and Kenya etc.” (Participant 1). 
The programme has also improved participants’ facilitation skills and to their own 
consultancies. This was evidenced via a comment made by one of the respondents who 
articulated that; 
“I am invited to facilitate workshops and training. It helps me to facilitate M&E 
training. I have been reviewing people’s work. I am looking at initiating my own Impact 
Evaluation consultancy” (Participant 7). 
 
4.3.4 Undecided 
However, some of the respondents indicated that it is too early to attribute the role of the 
DETPA programme in bringing about changes at an individual or organisational level. In 
particular, one of the respondents mentioned that the improvements cannot be  only attributed 
to the contribution of the DETPA programme only. The respondent said;  
“Yes somewhat, growing drive towards evidence – based decision – making, but not 
only because of DETPA” (Participant 8)
<<<PAGE=82>>>
81 
 
This implies that that there is an appreciation of the fact  that training forms part and not the 
only part of the broader of ECD offerings. In addition, it further suggest s that at times the  
effects of training programmes take time to yield results  at and individual and organisational 
level.  
4.4 Improved and sustained transfer of skills and knowledge 
 
In addressing secondary research question number three around participants’ perceptions on 
whether or not they have acquired new knowledge, skills and learning from participating in the 
delivery of the DETPA 2017 programme, participants and line managers overwhelmingly 
indicate that the transfer of skills was achieved. Interestingly, there w as a consensus from all 
14 respondents. In other words, all respondents (participants and their line managers) believed 
that the DETPA programme has contributed towards improving and sustaining the participant’s 
skills and knowledge as well as contributed towards improving their job performances. DETPA 
was attributed to have afforded  some respondents an opportunity to become recognised 
consultants;  
“Most notably, DETPA has given me leverage because international organisations 
looking at local partners are receptive to my CV. I have done work with Amnesty 
International. Oxfam appointed me as a local partner as a result of my participation 
during the DETPA training programme” (Participant 3).  
Another respondent reported that they had improved and been promoted “I can confirm that 
through this programme my job performance has improved. In fact, it contributed to my 
promotion, which is another motivating factor” (Participant 4).  
Some indicated that the programme has empowered them to interface M&E key concepts with 
their practice. For example, the Theory of Change (TOC) and evaluation methodologies are 
some of the concepts learnt and then applied by participants.  Line managers confirmed this;  
“It has actually improved his performance in the work that he has been doing. Before he 
came for the programme, he was at an entry point of his M&E career. After the 
programme, he showed an increase in knowledge, particularly TOC and developing 
indicators. Prior to the training, he handled fewer training activities, however, post the 
delivery, he made my own job easier because he became part of the core training team. 
As a result, we managed to reach more training participants because he was a party to 
training participants in terms of developing indicators” (Line Manager 1).
<<<PAGE=83>>>
82 
 
This implies that the re is a strong perception that participants  gained skills and knowledge 
during training, which was applied in their workplaces.  
4.5 Organisation improvement 
 
This section presents data on the following research sub-questions: 
(i) What are the participants’ perceptions regarding their job-related performance in 
M&E after their participation in the DETPA 2017? 
(ii) What are participants’ perceptions regarding the impact of the training on their 
organisations, specifically in terms of M&E practice? 
The analysis of organisational improvement indicates positive feedback from respondents. As 
an illustration, nine out of the ten participants are convinced that the programme improved their 
organisational performance. A similar finding applies to the responses elucidated from the line 
managers. This is represented by the fact that three, out of the four, line managers interviewed 
indicated that the programme has facilitated change in behaviour within their respective 
organisations.  
There are varied  examples which were mentioned as evidence for the organisation by the 
respondents. These include the fact that some  of the organisations that were employers of the 
participants of the DETPA programme  integrated M&E as a strategic or  planning approach. 
As an illustration, one of the respondents argued that; 
“The programme has improved my subordinates work and their reporting  and ability 
to interact with government officials. They are able to hold conversations with a 
government official at a high level. In turn, it improves the efficiency of the Twende 
Mbele programme.  Some of the staff gained interest to pursue a career in M&E. In the 
end, we hope the staff will become experts in the sector” (Line manager 2). 
This resulted in the inclusion and integration of the results framework  as a component of the 
organisation’s strategic plan. A senior manager in that particular organisation championed this 
integration. Interestingly, one of the participants in the DETPA programme led the task of 
developing the results framework. This could imply that M&E was  legitimised in the 
organisation and this has led to the emergence of champions in that particular organisation.  
In addition, the researcher also noted that there were divergent opinions regarding the change 
in behaviour presented by respondents. This means that participant s and their line managers 
had uncertainties on whether or not change in behavi our was attained in their respective
<<<PAGE=84>>>
83 
 
organisations. As an illustration, one of the respondent remarked … “Yes somewhat. Growing 
drive towards an evidence -based decision – making, but not only because of DETPA”  
(Participant 4). Another participant corroborated this sentiment by noting that, “DETPA is a 
resource for capacity building. So far, limited impact, but over time as they apply to learn, this 
may be possible” (Line manager 3). 
However, respondents further asserted that it is too early to link improvements to the DETPA 
programme yet. One of the participants specified that …  “DETPA is a resource for capacity 
building. So far, limited impact, but over time as they apply to learn, this may be possible”.  
4.6 Proposed areas of improvements 
The following research sub- question is addressed in this section: what are participants’ 
perceptions regarding the gaps of programmes such as the DETPA 2017 in building skills, 
knowledge, individual performance and organisational improvement and how they c an be 
enhanced?  
Respondents identified three areas of improvements with the objective for the consideration of 
the DETPA project team aimed at improving the planning and the delivery of the future 
DETPA programmes. These include curriculum design, the use of case studies and the Made 
in Africa evaluation concept, which are explained below.   
4.6.1 Curriculum design 
Participants and line managers identified curriculum design as one of the significant areas for 
improvement. Within this area, curriculum design included focusing on a case studies approach 
(practice-oriented content), and deepening the use of the ‘Made in Africa’ evaluation concept. 
Tracer studies were also mentioned as a way of ensuring that the organisation institutionalises 
the implementation of a monitoring and evaluation plan for the DETPA . Below follows some 
of the discussions proposed by the respondents. 
Case studies approach 
Participants and line managers perceive the use of case studies and a practical approach as an 
important component for improving the programme. As an illustration, eight out of ten 
participants proposed that the programme should increase the use of case studies approaches. 
One of the respondents declared that;
<<<PAGE=85>>>
84 
 
“it would be very interesting for the program to address specific topics adapted to the 
participants' needs such as the elaboration of the terms of reference, how to communicate on 
the results of the evaluation in the African context, etc.” (Participant 8). 
Three, out of four, line managers argued that the use of case studies will contribute towards 
embedding skills and knowledge acquisition, learning, job performance and improved 
organisational performance. One of the line respondents corroborated this by saying;   
“CLEAR should consider using a case study approach whereby participants are able to do 
RCT’s because it is perceived as a scientific method. This allowed participants to conduct 
calculations and practice in preparation of real-life cases” (Line manager 3).  
Moreover, it was argued that this approach would entrench M&E concepts within the  
participant’s evaluation practice.   
a. Made in Africa evaluation concept 
Building on the previous paragraph, there is a need to ensure that the ‘Made in Africa’ concept 
should be used as the organising framework for the entire programme. This suggest s that 
Afrocentric theories and methodologies should be fused into the all -inclusive curriculum 
content of the programme. In other words, respondents propose that culture, beliefs, and values 
are components that should distinguish the DETPA programme against other international 
programmes such as IPDET. One of the respondents emphasi sed the importance of ‘Made in 
Africa’ by stating that;  
“The emphasis on ‘ Made-in-Africa’ is very much needed to bring in the cultural 
sensitivities and well as clearly articulating the demand for and utilisation of evidence 
within the continent because my observation is that this is still external and not 
endogenously located” (Participant 1). 
This will ensure that contextually tailored case studies and approaches to evaluations are one 
of the outcomes enshrined in the conceptualisation and implementation of the DETPA 
programme. The ‘Made in Africa ’ module offered during the programme was positively 
received. Once again, one of the respondents affirmed the significance of MAE and mentioned 
that;  
“basically, international NGO perspectives traditionally inform most programmes , such as 
DETPA. However, practical M&E in government is neither understood nor properly taught by
<<<PAGE=86>>>
85 
 
most programmes. Hence, the DET PA programme should consider integrating the MAE 
approach in all the modules” (Line manager 4). 
On the other hand, respondents proposed that DETPA programme managers  should consider 
undertaking pre and post -tests or tracer studies. They argued that the pre -tests would ensure 
that the curriculum content addresses the training needs and expectations of the prospective 
participants. Whilst tracer studies w ould make sure that participant’s experiences post the 
programme are documented and applied towards improving the programme.  
These will avert s ome of the criticisms  noted from  a few respondents in relation to their 
perception that the programme did not cater to  their specific needs. For example, three out of 
ten participants mentioned that the programme did not sufficiently appreciate  continental 
representation and the diversity of the public sector. In expressing their opinions regarding the 
continental representation, one of the respondents asserted that; “ 
Facilitators should familiarise themselves with examples outside of SA considering that the 
programme caters for an international audience” (Participant 1). 
 This was corroborated by another respondent who argued that ; “Training was Ok, but not 
sufficiently customised for the diversity of government s in the region”  (Participant 2). 
However, it must be noted that DETPA is yet to customise its curriculum content to cater for 
specific sectors.  
4.6.2 Instructional design 
The majority of the respondents appreciated the facilitation approaches used by facilitators. As 
an example, nine out of ten participants mentioned that the instructional design applied by 
facilitators has contributed to their learning. As an example, one of the respondents believed 
that; 
“Facilitators did their splendid role particularly in orienting the learning experience 
towards adult learning methodology. Practical examples were used stemming from real 
cases. The passion was there especially on the ‘impact investing’, ‘made-in-Africa’ and 
cultural sensitivity; and the rigorous impact evaluation module” (Participant 10). 
However, a few ideas geared to improve the facilitation of sessions were proposed by 
respondents. This includes clinical learning and andragogy approach. As an example, one of 
the respondents proposed that “ clinical learning approach via role -plays and group works 
should be explored in designing the Terms of Reference (TOR) session” (Participant 7).
<<<PAGE=87>>>
86 
 
Using this approach allows participants to tap into their own experiences and facilitate learning 
by doing as opposed to a theory-based approach. In addition, one , out of four, line managers 
proposed that the programme co-ordination team should explore the use of two facilitators per 
class who will take students through the entire course. One of the respondents (one of the 
facilitators) who  argued that “this will ensure that the participants will build rapport and 
reduce duplication”, corroborated this. 
However, some of the respondents raised concerns regarding the lack of uniformity in applying 
the instructional design used. One of the respondents mentioned the following:  “Some of the 
facilitators who are academics seemed to have focused on theories as opposed to their 
practitioner's counterparts who provided practical examples. This made a difference, in terms 
of infusing M&E concepts with theories” (Participant 3).   
4.6.3 Implementation period 
Respondents highlighted time allocation as an important element of the DETPA programme , 
but there were mixed responses regarding this. Four participants who cited the significance of 
allocating more time illustrate this. Hence, they recommended that the programme should be 
extended beyond two weeks. One of the respondents alluded that; 
“I found 2 weeks short considering this is supposed to be a hands-on course. Can we also think 
of having two sessions in a year?” (Participant 4). 
However, there is a different school of thought regarding time allocation. One out of four-line 
managers indicated that the programme’s target group are in the middle and senior management 
positions. Therefore, availability might be a challenge. Hence, he recommended that  
“Consider course duration of 1 week - it appears target participants are busy people may 
struggle with being away for 2 weeks” (Line manager 2).  
4.7 Limitations 
 
This study has several limitations. The researcher is involved in the conceptualisation and 
implementation of the DETPA programme. Therefore, there is a professional relationship 
between the researcher and participants. CLEAR-AA as a single organisation was applied as a 
case study. Participants , moderators, facilitators, and line managers provided perceptions on 
the DETPA programme only; therefore, these findings cannot be generalised with other M&E 
programmes.
<<<PAGE=88>>>
87 
 
4.8 Conclusion 
This study contributes towards deepening scholars ’ understanding on how to assess the 
effectiveness of short- course training programmes in M&E from the perspective of 
participants, which may contribute towards understanding how to better design and implement 
such programmes with the target audience’s needs in mind.  The study concludes that positive 
transfer of training is dependent on the premium that is placed on the skills and knowledge 
acquisition, which then contributes to improving participant’s job and organisational 
performance. However, the curriculum content, u se of case studies and instructional design 
applied during training are some of the key determinants that should be highly prioritised in 
capacity building initiatives.  
The next chapter considers the findings in the context of Kirkpatrick’s  training effectiveness 
model. 
CHAPTER 5 ANALYSIS AND DISCUSSION OF RESULTS  
5.1 Introduction 
This chapter discusses the findings elucidated in the previous chapter. The researcher uses the 
presented literature (see chapter 2), in particular the conceptual framework, to strengthen their 
arguments. The previous chapter was dedicated to presenting the data by discussing the key 
themes and categories of responses that emerged from the data. As an attempt to contribute to 
understanding the effect of evaluation training programmes in strengthening evaluation 
practice in Africa, this chapter turns to providing an in- depth examination of the data  as well 
as insights and discussions in tandem with the literature.  This discussion sets the tone for the 
proceeding chapter in which the researcher draws conclusions and recommendations for 
prospective areas of research and practice. Kirkpatrick’s 4 levels are used to frame the analysis 
below.  As an illustration, section 5.2 addresses Level 1, which discusses the research question 
focusing on the participant’s overall reactions to the DET PA 2017 programme . Section 5.3 
addresses the research question on whether or not  they have acquired new knowledge, skills 
and learning from participating in the delivery of the DETPA 2017 programme. Section 5.4 
focuses on the research question regarding their job- related performance in M&E post their 
participation in the DETPA 2017. Section 5.5 discusses research question focusing on  the 
impact of the training on their organisations, specifically in terms of M&E practice . Lastly, 
section 5.6 will present a discussion on the
 research question concerning the  gaps of 
programmes such as the DETPA 2017 in building skills, knowledge, individual performance 
and organisational improvement and how they can be enhanced.
<<<PAGE=89>>>
88 
 
 
Figure 4: Summary of the findings  
 
 
5.1.1 Framing the context in which the analysis of the study is conducted  
 
As part of the analysis of this study, the researcher deems it necessary to frame the context in 
which the analysis is premised on. In doing so, the study briefly highlights the context in which 
a training programme such as DET PA occurs. This was done in tandem with how it links to  
individual participants and their respective organisations. This is underpinned by the 
recognition that individuals as employees and organisations as employers are both responsible 
for contributing to organisational performance. 
 
Since the early 1990’s, African public and private organisations  pursued the NPM  as an 
approach to achieve developmental results often referred to as results-based management. As 
part of implementing the NPM approach, government institutions and private organisations 
apportioned resourced to support capacity -building initiatives including training . These
<<<PAGE=90>>>
89 
 
resources availed opportunities for t heir employees to enrol for M&E training programmes  
offered by various training providers. In turn, these M&E training programmes were expected 
to capacitate organisational employees with responsive M&E skills and knowledge sets. Most 
importantly, these skilled employees were envisioned to  maximally contribute towards 
improving their organisational performances in terms of service delivery (public sector) or 
profit-making (private sector).  
5.2 Level 1 - Overall reactions to the DETPA programme 
This section discusses the research question focusing on the participant’s overall reactions to 
the DETPA 2017 programme. This study was conducted in an era in which very few studies 
aimed at assessing the effectiveness of training programmes in Africa are conducted , as 
previously elucidated by Morkel and Ramasobana (2017) and Wao, et al . (2017). When 
assessments are conducted to measure reactions, there are mainly conducted immediately after 
the delivery of programmes. Consequently, participant’s perceptions of programmes , such as 
DETPA, are yet to be known.  
At this level, Kirkpatrick’s (1959) model measure s the participant’s reaction to the entire 
training programme. In the case of this study, the findings that emerged could be summed into 
five themes. These include ; the role of the coordination team, the curriculum content of the 
course and the context in which it was implemented, facilitators role including their facilitation 
style, the alignment of the content to their working environment as well the M&E 
competencies. The study was further mindful of the inherent subjective views and biases of the 
participants. Traditionally, Level 1 assessments are generally completed immediately after the 
delivery of the training interventions argue Morkel and Ramasobana (2017).  
The pervasive reviewed literature emphasised the cyclical nature of Kirkpatrick 1959 model. 
Put differently, Level 1 (reactions) serves as a progressive building block leading up to Level 
4 (results). This implies that the four components of the model are both sequential and mutually 
dependent on each other . Therefore, it is almost impossible that the findings at Level 1 
(reactions) to Level 4 (results) could be discussed extensively without appreciati ng the 
interconnectedness of Kirkpatrick’s  model and the  importance of participant’s sense of self -
identity as individuals, groups and their different organisations.  
In the case of the current study, the researcher solicited participants reactions two years post  
the delivery of the programme. Significantly, respondents perceived the programme to be of 
good quality. Without following any level of significance or sequencing, themes will be
<<<PAGE=91>>>
90 
 
presented. These themes include the coordination of the programme, the curriculum content 
and the context in which it was implemented of the course, the facilitator ’s role and their 
facilitation styles and skills, knowledge and participant’s attitudes/attributes. Furthermore, the 
relevance of the programme with the participants working environment s and its contribution 
in meeting the participant’s expectations is a common thread. A discussion on these themes is 
discussed below.  
5.2.1 The coordination of the DETPA programme 
 
The majority of the respondents noted that the coordination of the DETPA programme led by 
the project team facilitated and enhanced the opportunities for learning to occur. They were 
under the impression that the programme was well organised. Below follows  some of the 
specific responses from the interviewed participants.  
“It was participatory, and the organisation was great,” (Participant 5) 
“It was well- organised and the co-ordinating team was hands-on” (Participant 6). 
As early as 1970, the outside classroom issues such as the coordination role steered by the 
project team were championed by scholars such as Browning (1970) . Browning study which 
evaluated the effectiveness of a short -term training programme focusing on the employees 
working in the social development sector in the US established that there are synergies between 
the training programme, the classroom and the factors outside the classroom. T hese synergies 
are independent but jointly liable to the success or failure of a programme. In addition, the 
coordination of the programme led by the project team was mentioned as the holding glue that 
supplemented and entrenched these synergies. To this date, it seems like the coordination of a 
training programme as an external influential factor outside the classroom  and continues to 
require some level of detail in the development of a training intervention.   
5.2.2 The curriculum content and the context in which it was implemented 
  
A large number of participants and moderators  mentioned that the curriculum content of the 
programme met their expectations and therefore was of good quality. This is informed by the 
fact that the content was tailored to accommodate the different skills and knowledge needs of 
participants who are practising in varied working environments within the African continent. 
Generally, the respondents argued that the content was relevant, insightful and contributed 
towards improving their skills and knowledge acquisitions. This infers  that the curriculum
<<<PAGE=92>>>
91 
 
content of the programme met the expectations of the DETPA participants. The f ollowing 
responses affirm the view t hat the curriculum content was aligned with the participant’s 
expectations:  
“DETPA is an important contribution towards improving the quality of M&E service 
delivery in the continent. The Programme was developed based on a number of 
observations and assessments, which revealed M&E capacity gaps at different levels in 
Africa. The training contents covered by DETPA meet M&E capacity demand and 
stakeholders’ expectations to some extent” (Moderator 1). 
“The course design enabled for diversity in skillsets allowing for the different 
participants to learn from one another, enabling for implementation of the different 
approaches back home” (Participant 10). 
“I also liked the fact that we had foundations for “beginners” and advanced for those 
already practising, thus giving everybody the opportunity to learn” (Participant 4). 
Few authors highlighted the role of aligned curriculum content. As an illustration, a study by 
Tonhäuser and Büker (2016) which interrogated the transfer of training highlighted a tailored 
course curriculum content adapted to the trainee’s working environments embed the knowledge 
gained from training. Another study by Aluko and Shonubi which investigated how working 
environment factors affect distance learning put emphasis on the prominence of integrating the 
expectations of the participants with the outcomes of a  training programme. In the Aluko and 
Shonubi study, the reactions of the attendees of a programme, the objectives of the programme 
versus the vision of the training institution in relation to the programme were collated (Aluko 
& Shonubi, 2014) .  Their study affirmed  that training providers should endeavour  to ensure 
that their institutional objectives, which initiated  the development of training programmes,  
align with participant’s expectation and as well as wider capacity-building strategy.  
In the same year with Aluko and Shonubi, Podems (2014) via a different study focusing on the 
professionalisation debate stressed that contributing towards a wider capacity strategy demand 
that training initiatives led by training providers should be cognisant of the evaluation 
competencies needed by organisations or countries in which these programmes are offered. 
Punia and Kant (2013) corroborate this assertion by proposing that the curriculum content of 
training initiatives offered by training providers should be in line with the skills and knowledge 
needs of the practitioners and their relevant organisations. Punia and Kant (2013) further assert
<<<PAGE=93>>>
92 
 
that this harmonised approach goes  a long way in ensuring that  trainees have positive 
perceptions concerning the effectiveness of a training programme.  
Other s cholars such as Axtell et al. (1997) ; (Tarsilla, 2017)  (see Chapter 2 ) signified the 
importance of the working environment in relation to Level 3 (transfer). Although this section 
focuses on Level 1 (reactions) the argument  around the embedding the skills and knowledge 
acquired by the trainees as articulated by Axtell et al. (1997); (Tarsilla, 2017) remain applicable 
at this level as well. These authors contend that it is almost impossible that trainees will react 
positively to a programme if it is not synchronised to their working environments.  
 
Notwithstanding the importance of the working environments as a micro- level matter, which 
there remains a need to locate the concept of a working environment within a macro or broader 
level. Thus, the context in which the M&E practitioners practice their practice is the macro or 
broader context. Appreciating the context in which practice occurs cannot be neglected . This 
means that it is not sufficient to only strive for the harmonisation of the curriculum content of 
a training programme with trainees working environments. This calls for a need to ensure that 
training interventions are guided by the evaluation capacity building framework s and are in 
accordance with the needs of each country or region. This argument was emphasised  by the 
majority of the respondents.  Below follows some of their assertions: 
“…First, it was a long-overdue intervention for a regionally contextualised programme after 
the global one (IPDET)” (Participant 1). 
“Thinking about the programme and the rationale behind it, there was an attempt to ensure 
that M&E training is sensitive to the Africa context” (Participant 2). 
In a n attempt to contribute towards adapting HRD to an African context, Mehlape (2017) 
appeals that African M&E, HR practitioners and scholars must consider delivering capacity-
building interventions that embrace the role of context. In other words, training interventions 
implemented in Africa should be cognisant of Africa’s underdevelopment and seek to mitigate 
the ski lls disparities in the continent. Chilisa and Tsheko (2014) and Cloete (2016) both 
reinforce this argument by highlighting that the evaluation ca pacity interventions in Africa 
should be adapted to cater for the skills needs of practitioners in the continent. This was 
corroborated by Sydhagen and Cunningham (2007) , who posit that capacity building 
interventions should appreciate the appropriateness of contextual factors.
<<<PAGE=94>>>
93 
 
In emphasising the issue of a contextually fitted curriculum, the majority of the respondents 
cited that the MAE agenda was a key component of the programme. Below follows a response 
from one of the interviewees: 
 “So the emphasis on ‘made -in-Africa’ is very much needed to bring in the cultural 
sensitivities and well as clearly articulating the demand for and utilization of evidence 
within the continent because my observation is that this is still external and not 
endogenously located” (Participant 1). 
Gaotlhobogwe, Major, Koloi-Keaikitse and Chilisa (2018) promote African ways of knowing 
and doing things. Essentially, these authors purport  that these ways of knowing and doing 
should be incorporated into capacity building initiatives and the far -reaching evaluation 
ecosystem, particularly in Africa. This is an attempt to ensure that values, culture, and beliefs 
are interwoven as part of capacity building initiatives. Chilisa (2017) and Keane, Khupe and 
Muza (2016) validated this  argument by emphasising the prominence  of localised African 
evaluation capacity-building approaches. Included in these cited approaches are the theoretical 
and methodological approaches practised in the region but densely undocumented. This call 
propels training providers to think sharply arou nd incorporating these approaches into the 
training offerings.  
Linked to these debates, Tirivanhu et al. (2017) earlier emphasised the urgency to capacitate a 
cohort of evaluators with skills to conduct and commission evaluations in the region. Therefore, 
it might be helpful that the process to capacitate these practitioners bears the contextual 
bearings underpinning the African evaluation context (Mbava & Dahler -Larsen, 2019).  The 
capacitation of African based evaluators was also envisaged to contribute towards mitigating 
the noticeable and stubborn challenge of the dominance of non- African evaluators in the 
evaluation field practising in the region.   
Therefore, curriculum  content, which embrace s participants working environments and the 
context in which practitioners are practising, are inseparable. A combination of these factors  
could potentially avail a seamless process interlinking the relationship between a trainee as an 
individual as well as corporate (employee, organisation and evaluation environment) 
knowledge systems. Effectively, it might contribute to participant’s positive reaction to the 
programme which then avails opportunities for participants to acquire skills and knowledge 
(learning) which can be transferred (transfer) to their working environment (organisation). T
<<<PAGE=95>>>
94 
 
5.2.3 Facilitator’s role and their facilitation style 
 
The effectiveness of a training programme is broader and cannot be limited to only the 
curriculum content. Training effectiveness further covers the delivery team and the delivery 
techniques applied during the implementation of the programme. This argument moves the 
discussion away from focusing on the content of the curriculum by paying some attention to 
the faculty of facilitators and the delivery styles used in delivering the programme. B elow 
follows some of the responses solicited from the interviews: 
“Facilitators did their splendid role particularly in orienting the learning experience 
towards adult learning methodology. Practical examples were used stemming from real 
cases. The passion was there especially on the ‘impact investing’, ‘made -in-Africa’ and 
cultural sensitivity; and the rigorous impact evaluation module” (Participant 7). 
“All of them were fantastic, at no point in time, they were never found to be dismissive to 
participant’s questions (always willing to listen to participants). They used andragogy 
approach in teaching their courses. More especially Lewis ” [One of the facilitators] 
(Participant 4).  
Various scholars elevated the prominent role played by the facilitators and the facilitation style 
in assessing training effectiveness. Abdulwahed and Nagy (2009) recorded that 86,4 percent of 
the students reacted positively to the facilitation nuances and presentation styles such as  
(Powerpoint and Visual presentation) employed by the lecturers during the induction of a  lab 
class. What's more important, is that the mixture of facilitation nuances and facilitation styles 
stirred students to spend more time in the lab studying this particular module.  In a different 
study, Healey and Jenkins (2000) endorsed the role of experienced facilitators and teaching 
approaches. In support of this argument, Foxon (1989) individual facilitation skills and the 
techniques used during the delivery emerged as some of the key findings of a systematic review 
study focusing on the American and Australian literature.). To be more specific, Foxon (1989) 
alludes that the teaching approaches and techniques applied during the sessions have a direct 
bearing on participant’s reactions  (which was in this case po sitive) of the programme . 
Separately, another
 study by Cooley et al. (2015) present that the use of innovative facilitation 
style such as outdoor group work was instrumental in determining students perceptions the 
effectiveness of the programme.  Cooley study evaluated the perceptions of undergraduate 
students in terms of whether or not a training skills programme underwrote their reaction to the
<<<PAGE=96>>>
95 
 
programme. At the conclusion of a systematic literature review for the period, 1970- 1986 (16 
years) in the American and Australian journals conducted by Foxon (1989, p. 89) that trainee 
reactions, participants’ grading of individual facilitator’s facilitation skills and their teaching 
techniques were ranked to be key elements of a training programme. Once again, this 
conclusion amplifies the facilitation and facilitation style narrative elucidated by the findings.  
5.2.4 Skills, Knowledge and Trainee attitudes/attributes 
 
Thus far, the presented analysis leaned towards the factors around the actual training, the role 
of facilitators and their facilitation style . This might deflate the influence of  participants own 
personal attitudes towards the programme. Below follows, a remark from one of the 
moderators: 
“The first reaction observed from the 2017 participants during the training sessions had 
shown some enthusiasm and satisfaction of these participants (Moderator 2). 
Individual personal attributes have an effect on how they interact with the opportunity to learn 
including their reaction to a training programme. In support of this argument, Noe and Schmitt 
(1986) posit that participant’s attitudes and their expectations prior  to the commencement  
(preconceived ideas) of the programme pre-empt their reaction to the programme. Bansal and 
Thakur (2013) elaborate that trainee attitudes or attributes during the delivery of a training 
programme encode their appetite for  skills and knowledge  acquisition. In a different setting 
where a debate on the professionalisation of evaluation was discussed, Stevahn, King, Ghere 
and Minnema (2006) concluded that skills, knowledge and attitudes/attributes (SKAs) as 
competencies that influence the development of training programmes. This argument was 
highlighted by Morell and Flaherty (1978) framework depicted in Figure 2 titled “five phases 
towards professionalisation” which cited evaluator personal skills, knowledge and 
characteristics as key competencies for practising evaluation. Although Noe and Schmitt 
(1986) –focused on Level 1 (reaction) whilst Stevahn, King, Ghere and Minnema (2006) paid 
attention to the transfer of training (Level 3), both studies highlighted trainee attitude/attribute 
as an antecedent to participants reactions followed by an opportunity to learn.  
Since the levels of the Kirkpatrick’s (1959) model are cyclical and interlinked, the findings 
(which are generally positive) solicited from Level 1 preempt whether or not the respondents 
will acquire skills and knowledge as envisaged in (Level 2) learning. Below follows the 
discussions.
<<<PAGE=97>>>
96 
 
 
5.3 Level 2 - Learning 
This section addresses the research question focusing on whether or not they have acquired 
new knowledge, skills and learning from participating in the delivery of the DETPA 2017 
programme. Generally, this level assesses learning in terms of whether or not there are positive 
changes in skills, capabilities and attitudes because of a training intervention.  B ased on the  
interdependence of the four levels of Kirkpatrick’s (1959) model, it is anticipated that Level 2 
(learning) findings will flow from the trends documented in Level 1 (reactions). This is because 
Level 1 serves as a pedestal or a building block to the current (Level 2) under discussion.  
5.3.1 Skills and knowledge acquisition 
A large number of r espondents purported that learning was attained  during the DETPA 
programme. Below follows some of the trainee’s response that gives an account of how the 
DETPA programme has contributed to their learning:  
“I had a little M&E background before, but I had a challenge with the theory of change. 
The instructors emphasized concepts and related them to our African context enabling 
for applicability and learning. By the end of the course, I had achieved my intention to 
learn the Theory of Change” (Participant 3) 
In terms of skills, there are limited opportunities to apply their skills. The whole 
discussion around MAE has been helpful because I become sensitive to the concept of 
inclusion and the context in doing my work” (Participant 5) 
Axtell et al. (1997), endorse that participant’s preliminary reactions to the programme influence 
their level of learning. In addition, authors such as Turab and Casimir (2015); Machin (2002); 
Tonhäuser and Büker (2016) further contend that elements such, the curriculum content, 
facilitators and their facilitation style as well as trainee attitudes/attributes (which is part of the 
characteristics of Level 1  inform the extent to which learning, as well as the applicability of 
learning, occurs. This affirms perceptions by Alliger and Janak (1989) and Sharma (2016) who 
posit that Kirkpatrick’s model is chronological in approach. In other words, it is highly unlikely 
that the findings recorded in Level 1 - reactions will be dissimilar to Level 2 - learning. This 
explains why the programme was perceived to have contributed to improving the participant’s 
skills and knowledge . Below follows a response that asserts new skills and knowledge were 
acquired:
<<<PAGE=98>>>
97 
 
“Clear and practical lessons and skills on developmental evaluation was imparted as 
intended as well as highlighting cultural sensitivities within Africa, which need to be 
embedded in the practice” (Participant 6) 
Given that the training contents are more or less relevant, and that the facilitation was 
appropriate, one can assume that the training had contributed to the improvement of 
participants’ skills and knowledge” (Moderator 2). 
Sharma (2016) concedes to the importance of measuring whether or not new skills, knowledge 
and attitudes  have been  acquired owing to the delivery of a training programme .  Put 
differently, this level seeks to differentiate what the trainees knew before (pre) attending the 
training and after (post) the training.  It  pursues to ascertain the extent to which  the learning 
objectives versus the learning effects of a training intervention were recorded. 
5.3.2 Increased trainee’s evaluation technical, and cross-cultural competencies 
 
 
There are a growing and pertinent appreciation that M&E professionals are required to improve 
their evaluation competencies. A large number of interviewees consented that their evaluation 
skills and knowledge were improved.  Below follows some of the responses from the trainees. 
“Of course, I learnt, I got the opportunity to evaluate a programme in 2018. I 
encouraged the client to move beyond secondary data and start to plan for impact 
evaluation from the start of their programmes. This was because it was difficult to 
attribute the impact, as it was not planned. From such an institution, their staff have 
registered for an M&E course” (Participant 3) 
“After the programme, he showed an increase in knowledge, particularly TOC and 
developing indicators. Prior to the training, he handled fewer training activities, 
however, post the delivery; he made my own job easier because he became part of the 
core training team. As a result, we managed to reach more training participants 
because he was a party to training participants in terms of developing indicators” (Line 
Manager 1). 
Phase 2 of Morell and Flaherty (1978)  as depicted earlier in Figure 2 (Chapter 2) titled “five 
phases towards professionalisation” recommends that training programmes that demonstrate 
how practitioners must perform their M&E duties should be offered to the M&E practitioners. 
This concedes to the importance of theory but  also adds that training programmes should be 
comprised of learning by doing as a method. This evokes the necessity to be cognisant of the
<<<PAGE=99>>>
98 
 
need to struck a balance between the theory and the practice of evaluation as one of the delivery 
modes of a training programme. This approach is envisaged to contribute towards building the 
soft and technical competencies.  
Speaking about competencies, Stevahn, King, Ghere and Minnema (2005) have developed an 
evaluation competency framework .  The framework introduced six domains of evaluation 
competencies. Out of the six domains, a  US-based study conducted by Galport and Azzam 
(2017) concluded that systematic e nquiry, cross -cultural, and project management 
competencies are the three key evaluation competencies that are compulsory to practice 
evaluation effectively in the 21
st-century era.  These domains promote that evaluators should 
be capacitated with M&E technical skills (such as developing the theory of change)  as well as 
people-centric skills (negotiation and cultural sensitiveness nuances) as part of building their 
competencies to conduct and commission evaluations  (Stevahn, King, Ghere & Minnema, 
2005. These competencies include the abilities to utilise the evidence emerging from the 
evaluations towards improving policy or programme. This elevates the role of learning and 
applying evaluation of technical skills which were presented as part of the DETPA programme. 
In support of this argument, some of the interviewees mentioned that: 
“In terms of knowledge, Yes, I know how to prepare a randomised control trial (RCT). I can 
read and understand studies that have used RCT and able to critique (Participant 5) 
“From this point of view, it was relevant and once allowed the Twende Mbele team to gain 
global evaluation competencies as well as learn useful evaluation tools” (Participant 6). 
Equally, learning technical skills was cited as one of the three key evaluation domains.   Galport 
and Azzam (2017) mention that the use of quantitative data methods including data collection 
and analysis are three systematic inquiry elements, which each evaluator should aspire to 
possess. Over 50% of the respondents who were interviewed during a study commissioned by 
Galport and Azzam (2017) mentioned that they need further training on conducting meta -
evaluations, followed by the skills to apply quantitative meth ods in conducting evaluations. 
This affirms the notion that the conceptualisation of training programmes should continue to 
cover the technical competencies.   
Beyond the technical skills, a sizeable number of the respondents commented that cultural 
sensitivity applied in the delivery of the programme . Using their own voices, they argue that 
strides to apply cultural sensitivity in the programme facilitated learning. Below follows two  
of the responses:
<<<PAGE=100>>>
99 
 
“Clear and practical lessons and skills on developmental evaluation was imparted as 
intended as well as highlighting cultural sensitivities within Africa, which need to be 
embedded in the practice” (Participant 6) 
The whole discussion around MAE has been helpful because I become sensitive to the 
concept of inclusion and the context in doing my work” (Participant 5) 
Despite the fact that Galport, and Hazzam (2017); Tirivanhu et al. (2017) are different authors 
by virtue of their geographical locations (US and Africa concurrently listed) and research focus 
areas, they seem to have consensus on the significance of cultural sensitivity in the evaluation 
field. As an illustration, Galport and Azzam asserted that evaluator's interpersonal skills (such 
as written and oral communication, negotiation and cross-cultural skills) in the implementation 
of evaluations. Similarly, Tirivanhu et al. (2017) proposed that delivering a contextually fitted 
training programme premised on a culturally sensitive curriculum increases the possibility of 
learning from the programme . This further coincides with the argument that a culturally 
sensitive training programme take cognisant of trainees expectations and their working 
environment guided by countrywide competencies championed by Podems (2014). Tirivanhu 
et al. (2017) add that delivering culturally sensitive programmes has the potential to contribute 
towards increasing the pool of eval uators with relevant M&E skills to improve their own 
practices including their working environments followed by their organisations. Keane, Khupe 
and Muza (2016) corroborated the argument by recommending localised African evaluation 
capacity building initiatives, which appreciate cultural  complexities which are generally 
inclined to African ways of knowing, doing and behaving. This infers that in the eyes of the 
DETPA participants, the inclusion of the MAE module in the programme was a step in the 
right direction. 
  
On a different note, Turab and Casimir 2(015); as well as Sharma (2016). Chang (2010), 
caution that the majority of training effectiveness assessments focus on Level 1 (reactions) and 
Level 2 (learning), with less attention paid to the last two (transfer and organisational imp act) 
levels. This is primarily because Level 1 and Level 2 are perceived to the simplest as well as a 
self-reporting mechanism mainly from the lenses of the participants only (Chang, 2010). This 
has been previously been perceived to be subjective and bias. In addressing this limitation, line-
managers who are responsible to provide oversees the work performed by the DETPA trainee’s 
views were also solicited via the interviews.
<<<PAGE=101>>>
100 
 
5.4 Level 3- Improved and sustained transfer of skills and knowledge 
 
This section presents a discussion on the research question focusing on establishing whether or 
not participants’  M&E job-related performance were improved after their participation in the 
DETPA 2017. According to Kirkpatrick’s model, Level 3 intends to establish whether or not  
the participants applied their newly acquired skills and knowledge (learnings) to improve their 
practices. This is also linked to the  enabling working atmosphere that aids the application of 
the newly acquired skills and knowledge  including the management support. As previously 
indicated, this exhibits the interconnectedness of Kirkpatrick’s four levels model.  
 This means that  the transfer of training is multifaceted and goes beyond the actual training 
interventions and involves a myriad factors beyond a training programme. Therefore, assessing 
this level entailed interviewing participants and line managers . Traditionally, the various 
studies agreed that the findings of Level 1 (reaction)  and Level 2 (learning) ha ve a direct 
impact on Level 3 (transfer of skills). In the case of this study, a  number of respondents 
mentioned that they used the skills sets learnt during the delivery of the DETPA programme in 
conducting their work. Below follows a response from one of the trainee and their line manager 
attesting to the transfer  of training: 
“It has actually improved his performance in the work that he has been doing. Before he 
came for the programme, he was at an entry point of his M&E career. After the programme, 
he showed an increase in knowledge, particularly TOC and developing indicators (Line 
Manager 1) 
“Of course, I learnt, I got the opportunity to evaluate a programme in 2016. I 
encouraged the client to move beyond secondary data and start to plan for impact 
evaluation from the start of their programmes (Participant 3) 
Various scholars have defined transfer of training as a degree to which trainees effectively 
apply the knowledge, skills and attitudes gained in a training context to the job (Baldwin & 
Ford, 1988; Tracey, Tannenbaum, & Kavanagh, 1995). These elements seem to have focused 
on the application of what has been learnt. Axtell et al. (1997) presented course characteristics, 
participants characteristics and working environment as three determinants relevant to assess 
training effectiveness and the extent to which transfer of training has occurred. All of the above-
cited authors concur that a responsive oriented curriculum content reactive to the  participants 
working environments plays a pivotal role in igniting the transfer of training by participants.
<<<PAGE=102>>>
101 
 
This is due to the fact that, if trainees perceive the curriculum content to be unconnected to 
their working environments, then, it  is less likely  that the transfer of training will occur. In 
other words, the course curriculum content, the quality of facilitators and their facilitation style, 
facilitate learning which spills over to individual organisations.  
Thus far, it has been established that trainees of the DETPA programme pointed out  that the 
programme contributed to their skills and knowledge acquisitions. Additionally , the 
programme was also perceived to have augmented trainee’s evaluation technical , project 
management and cross-cultural competencies. These competencies are the enablers expected 
to be applied by the trainees  in doing their work. On a separate but related note, the line 
managers mentioned that the programme contributed to the improvement of their subordinates 
M&E skills and knowledge acquisition which led to improving their job performances. Below 
follows a response from one of the line managers: 
They are able to hold conversations with a government official at a high level. In turn, it 
improves the efficiency of the Twende Mbele programme. In the end, we hope the staff will 
become experts in the sector” (Line manager 2) 
Stevahn, King, Ghere and Minnema ’s (2005) study signified the importance of project 
management competencies, such as negotiating contracts and co- ordination of resources with 
various stakeholders and partners who are involved in the design or implementation of 
evaluations. Galport and Azzam (2017) synthesise this argument by assert ing that there are 
compelling reasons why employers fund their employees to attend the  training programme. 
Amongst others , these  training programmes are estimated to build or enhance employees ’ 
various skills such as interpersonal skills, report writing, project and team management skills.  
In sum, they are geared to ensure that their employees are capacitated with M&E skills, which 
will ultimately lead to improve ment within their respective organis ations. Further, 
organisations regard training initiatives as a strategic response to the pressures to adapt to the 
ever-changing and competitive environment. Therefore, in their own minds, leaders in 
organisations anticipate that the exercise to fund employees t o attend training will ultimately 
yield results for the overall  organisation (Methode et al., 2019). Using their economic lenses, 
Carton (2004) and Odor ( 2014) would categorise the contribution of the trained employees in 
the organisation as an ROI.   
Another important element, at this level, is the opportunity to apply what was learnt, aided by 
the support of participant’s line managers. A number of responses quoted that the support from
<<<PAGE=103>>>
102 
 
their line managers contributed towards improving their working environment. One of the 
respondents said: 
“My manager has promoted that I should be the lead facilitator in the workshops and 
training. It helps me to facilitate M&E training. I have been reviewing people’s work. 
I am looking at initiating my own Impact Evaluation consultancy” (Participant 7). 
Sarti et al. (2017) purport that the support of management in ensuring that the trainee’s newly 
learnt skills and knowledge are applied and enabled to contribute towards improving trainee’s 
job performance is important.  Sarti et al. (2017) further conclude the above assertion by stating 
that participants interact with the curriculum content with the objective to learn and apply their 
newly acquired skills and knowledge towards improving their jobs. This is despite the fact that 
the M&E vocabulary and theories  might still be  nascent post the delivery of a training 
programme. Besides the reality that trainees might still be new to the M&E materials and 
concepts, this is an indication of their willingness to apply new concepts if allowable by their 
managers.  Axtell et al. (1997) refer to this phenomenon as participants’ characteristics, which 
points to the agility of participants to transfer skills and knowledge  learnt.  In the case of this 
study, this agility was observed. Below follows a quote from one of the line managers: 
 “The programme has improved my subordinates work and their reporting and ability to 
interact with government officials (Line manager 2) 
 
Gravenhorst et al. (2003) elevate the support of management plays in  embedding learning. 
Axtell et al. (1997) concur that it is almost impossible to detach the role of management in the 
transferability of newly acquired skills and knowledge by the trainees into their working 
environment At this level, Kirkpatrick’s model put emphasis on the role of senior or line 
managers in availing opportunities to subordinates to experiment with the newly acquired skills 
and knowledge sets. This agrees  with Bansal and Thakur (2015) , who argue that the role of 
senior managers in an organisation has an effect on the effectiveness of skills and knowledge 
acquired by their subordinates. In the case of the respondents in this study, line managers led 
from the front via the creation of an enabling environment for participants to experiment with 
the newly acquired skills and knowledge sets.
<<<PAGE=104>>>
103 
 
In summary, t he findings on Level 1 (the overall reaction to the programme) sets a tone or 
influence how respondents respond to learning Level 2 (acquire skills and knowledge), which 
then affects their learning is applied to impr ove their job performances  Level 3 ( job 
performances). This study notes that  the majority of the t rainees indicated that they have 
acquired and improved their technical skills such as data collection and analysis, project 
management skills and cultural sens itive competencies which enabled them to improve their 
job performances. In addition, they credit their line managers for availing the opportunities  to 
apply their newly acquired M&E related skills. In sum, learning, the applicability of learning 
which led to improvements in their job performances occurred separately but in a systematic 
and sequential manner.  
 
5.5 Level 4 – Improved organisational performance 
This section discusses the research question focusing on participants’ perceptions regarding the 
impact of the training on their organisations, specifically in terms of improving their M&E 
practices. Due to the cyclical nature of Kirkpatrick’s model, this level is interlinked with the 
other three previously discussed levels (Level 1 – Level 3). Flowing from the previous section 
(the analysis of Level  3), the majority of the trainees and line managers both consented the 
transfer of training spearheaded by their line managers spilt over and improved their daily job 
performances. This section provides a systematic analysis of whether or not the anecdotal  
transfer of skills and knowledge witnessed in Level 3, has contributed to the improvement of 
respondent’s entire organisational performances. 
As per Kirkpatrick’s definition, this level assesses whether or not a training programme has led 
to the improvement of organisational performance. Traditionally, this level measures the three 
objectives collectively or selectively any of the three objectives separately; (i) higher revenue 
sales, (ii) higher productivity levels, (iii) improved quality of work and organisational culture.  
For t his study, an improvement in the quality of work and organisational culture wa s the  
applicable objective that will be assessed in an attempt to establish whether the DETPA training 
intervention has contributed to individual trainee’s organisational performances.  
In this study, a noticeable improvement in the quality of work was highlighted. Put differently, 
the majority of the respondents indicated that the quality of their organisational work has been 
improved. These improvements have direct positive effects on how individual organisations 
deliver services to their individual clients. This is not surprising considering that their
<<<PAGE=105>>>
104 
 
individual job performances were also perceived to have improved (see Level 3). Below is one 
of the responses attesting to the improvement in the quality of work: 
“Of course, I learnt, I got the opportunity to evaluate a programme in 2018. I 
encouraged the client to move beyond secondary data and start to plan for impact 
evaluation from the start of their programmes. This was because it was difficult to 
attribute the impact, as it was not planned. From such an institution, their staff have 
registered for an M&E course” (Participant 3) 
Boyle and Fogarty (2016)  present social identity theory. This theory contends that human 
beings (individuals) as employees constitute organisations. These organisations then assign 
their employees (as individual or groups ) with the duties and responsibilities to implement 
work-related activities  on their behalf. These individuals or groups are comprised of  
employees, management or shareholders with a common quest for a sense of belonging. Their 
sense of belonging influence how these individual employees or groups (team) discharge their 
duties on behalf of their organisations. This has to do with how individual employees or groups 
(team) feel about their organisations.  In summary, achieving organisational performance is 
dependent on the performance of individual employees or groups and their feelings towards an 
organisation in the process of executing their duties. Nyasha (2011) expands this argument by 
stating that organisational performance can only be attained  when there is a tripartite alliance  
of individual employees, their groupings (team) and organisations as entities.  
Furthermore, there needs to be an appreciation of the psychological quest to belong pursued by 
these individual employees or teams. Boyle and Fogarty (2016) supplement his submission by 
asserting that  this sense of belonging breeds a sense of pride and loyalty from individual 
employees or teams in discharging their duties on behalf of their organisations. Consequently, 
in the process when individual employees or teams are discharging their duties with a sense of 
pride and loyalty, a new path on how the organisation behaves gets to be instilled 
(organisational behaviour). It follows that organisational behaviour contribute s towards 
improving the quality of work executed by individual employees and teams on behalf of their 
organisations.  
In this study, respondents confirmed that the quality of their individual work including their 
organisations have been improved. As evidenced in the previous paragraphs, there has been 
an evident behavioural change witnessed by different organisations. Beyond the perceived  
behavioural change, a large number of interviewees mentioned that new ways of doing things
<<<PAGE=106>>>
105 
 
or new culture has been elevated in their respective organisations. Below follows some of the  
responses from the participants and line managers attesting to a new organisational culture: 
“In turn, it improved the efficiency of the Twende Mbele programme.  Some of the staff 
gained interest to pursue a career in M&E. In the end, we hope the staff will become experts 
in the sector” (Line manager 2). 
“However, post the delivery, he made my own job easier because he became part of the core 
training team. As a result, we managed to reach more training participants because he was a 
party to training participants in terms of developing indicators” (Line Manager 1). 
“Most notably, DETPA has given me leverage because international organisations 
looking at local partners are receptive to my CV. I have done work with Amnesty 
International. Oxfam appointed me as a local partner as a result of my participation 
during the DETPA training programme” (Participant 3). 
It is undeniable that measuring this level is complex. Gravenhorst et al. (2003) appreciate that 
it is not sufficient that organisations are agile to learning and enable the applicability of these 
learnings. Added to agility and applicability of learning, organisations are duty- bound to 
shadow initiatives aimed at ensuring that a new behavioural culture is embedded and sustained 
in their organisations. In other words, the new behavioural change in an organisation should 
yield tangible results. These results include enhancements in dealing with its business clientele, 
newness to stakeholders and shareholders engagements (Gravenhorst et al., 2003). Odor (2014) 
supports this argument by stating that the sustainability of a new culture in an organisation is 
influenced by the recognition of external and internal factors. This study notes that the change 
in the organisation is not linear and multifaceted. This explains why participants and their line 
managers were independently interviewed to solicit their independent views in relation to 
organisational be haviour. In the case of this study the new organisational behaviour was 
witnessed in the increase in the efficiencies of various organisations, including partnering with 
various stakeholders towards achieving organisational productivity.  
Although the majority of the interviewees agreed that the DETPA programme contributed to 
the improvement in their organisational performance, a minimal number expressed some form 
of uncertainties. These  respondents were uncertain about whether or not the improvement 
witnessed in the performances of their organisations was solely because of the DETPA training 
intervention. They further mentioned that it is too early for the training to effect change already.
<<<PAGE=107>>>
106 
 
Below follows some of the responses expressed by the minority of the participants and their 
line managers: 
“Yes somewhat. Growing drive towards an evidence-based decision – making, but not only 
because of DETPA” (Participant 4). 
“DETPA is a resource for capacity building. So far, limited impact, but over time as they 
apply to learn, this may be possible” (Line manager 3) 
Gravenhorst et al. (2003) propose that organisational change is a process and not an event. 
These authors additionally state that it is not sufficient that organisations intend to improve the 
efficiencies of their work. Besides the commitment to pursue a change in organisations by all 
stakeholders (employees, shareholders and management), instilling a new culture ( new ways 
of doing things) is complex. Chang (2010) and Thackeray (2016) corroborate this argument by 
stating that linking employee performances on the job and the organisational impact remains 
difficult. Mbabaali (2015) and Tarsilla (2014)  assert that measuring training effectiveness 
involves a long- term approach that moves beyond the individual capacities and training 
interventions and the appreciation of external factors suc h as the working environment . 
Tarsilla’s (2017) moves the debate further by recommending that suppliers of training should 
consider comprehensive approaches that move beyond piecemeal interventions, such as short-
course training programmes and offer long term and multipronged interventions guided by the 
ECD principles.  
A study by Chang (2010)  provided some insight s on how to account for organisational 
performance when there are inconsistencies as well as deepening OD scholarly  debates.  This 
study investigated whether the first three levels of Kirkpatrick’s model (Level 1- Level 3) are 
proficient to forecast the findings in Level 4. Using a regression model, Chang (2010)  study 
concluded that the trainees of a hospitality- training programme increased their knowledge of 
the content and the skills to handle calls (Level 2).   A mixed result in assessing Level 3 was 
recorded. Put in details, there was a substantive improvement in the call conversion but a non-
substantive improvement in terms of the time it took to make the calls. In the end, the findings 
of the Level 1 - Level 3 was used to partly forecast the results of Level 4 . Even though there 
were mixed results in assessing Level 3, the study supported Kirkpatrick’s theory that in order 
to witness the organisational performance, learning (Level 2) and job performance (Level 2) 
should be recorded. Therefore, in spite of the inconsistencies in assessing some of the levels of 
Kirkpatrick’s model, conclusions on the improvement witnessed in organisational performance
<<<PAGE=108>>>
107 
 
can still be drawn. Tonhäuser and Büker (2016)  commented in support of Chang (2010) 
narrative by saying that training effectiveness is  accomplished when the learnt concepts  are 
continually applied in practical settings and an enabling working environment is available.  
5.6 Areas of improvement  
 
In this section, the analysis of the research question aimed at elucidating participant perceptions 
regarding the gaps of programmes , such as the DETPA 2017, in building skills, knowledge, 
individual performance and organisational improvement and the possibility of enhancing them 
is presented.  
This section has the intention to identify the weaknesses and failures of a training programme 
such as DETPA . This aligns with Hamblin (1974) model, as cited by Sharma (2017) which 
presents that the Kirkpatrick’s model is  also adapted to identify weaknesses (areas of 
improvement) of a training programme.  In this study, the areas of improvement  could be 
summed into two categories, namely, the alignment and standa rdisation of the curriculum as 
well as time allocation. Identifying these areas  was intended to contribute towards improving 
the conceptualisation and implementation of training programmes such as DETPA.  
Using the above -presented findings sections (Level 1 – Level 4); it is almost impossible to 
ignore the fact that  the programme was positively received . In sum and  in the eyes of the 
majority of those interview ed, the programme enabled trainees to obtain M&E skills and 
knowledge, which were applied to improve their job and organisational performances. Having 
said that, this positive glance over the programme should not discount the importance of the 
areas which were deemed to require improvements. Below follow the discussions focusing on 
these areas.  
5.6.1 Alignment throughout the curriculum design process 
In principle, the curriculum content of the programme was mentioned as one of the contributing 
features accountable for the positive feedback awarded to the programme. On the same token, 
it must be noted that  respondents noted that there was some misalignment in the curriculum 
design which led to the inconsistencies in the actual content and its delivery modes . Below 
follows some of the related comments expressed by the interviewees: 
“It would be very interesting for the program to address specific topics adapted to the 
participants' needs such as the elaboration of the terms of reference, how to communicate on 
the results of the evaluation in the African context, etc.” (Participant 8).
<<<PAGE=109>>>
108 
 
“CLEAR should consider using a case study approach whereby participants are able to do 
RCT’s because it is perceived as a scientific method. This allowed participants to conduct 
calculations and practice in preparation of real-life cases” (Line manager 3). 
The majority of the respondents (see section 5.2) commended the facilitators and their 
facilitation styles; however, there were perceptible inconsistencies in the teaching approaches 
from one facilitator to another.  It seems like the decision to apply case studies and practice -
oriented approaches were left at the discretion of each facilitator and their one-to-one sessions. 
In addressing these inconsistencies, Tonhäuser and Büker (2016, p. 139 ) recommend that 
approaches such as practice-oriented examples, use of case studies and real -world examples, 
should be enshrined in the design of the curriculum content. This is because, if these practice-
oriented approaches are applied systematically in the delivery of training, they have a higher 
retention rate towards cementing transfer of training within organisations . The notion of a 
practice-oriented and led curriculum design was earlier accentuated by Punia and Kant (2013) 
in some of the recommendations addressed to the  training providers. These authors 
unashamedly argued that  standardising the use of work- related examples  as a process of 
delivering the programme is an important element that should be intermingled into the delivery 
of training by the training providers.  
Based on the above paragraph, this study argues that t he perceived lack of standardised 
curriculum design could have led to the incoherence and inconsistencies witnessed by the 
respondents concerning some of the sessions. A minimal number of respondents observed some 
duplications. Below follows the comment regarding duplication from one of the facilitators: 
“This will ensure that the participants will build rapport and reduce duplication”  
(Facilitator 1) 
Machin (2002) asserts that the sequencing of course curriculum and the time allocated per 
session forms an integral part of mapping out a  training intervention. In support of this 
argument, a research finding from a study piloted by Sánchez-Romero and Prskawetz, (2019) 
concluded that there is a correlation between the sequencing of activities and the increase of 
outputs achieved per training session.  This could infer that module allocation and curriculum 
sequencing or alignment should be premised on a well thought. This is because as evidenced 
by the Sánchez-Romero and Prskawetz, (2019) , time allocation predetermines the plays a 
pivotal role in training as well as increases the prospects of achieving results.
<<<PAGE=110>>>
109 
 
Prior to the commencement of a training programme, there is a call  to dedicate time in 
scrutinising the design elements of the curriculum. Except for  the appreciation of the 
incorporation of ‘Made in Africa ’ evaluation (MAE) agenda in the curriculum , there was a 
sense that the MAE approach was yet to be assimilated  into the totality of the  programme. 
Below, a comment regarding integrating the MAE approach is presented. 
“Basically, international NGO perspectives traditionally inform most programmes, such as 
DETPA. However, practical M&E in government is neither understood nor properly taught 
by most programmes. Hence, the DETPA programme should consider integrating the MAE 
approach in all the modules” (Line manager 4). 
Not in so many words but yet profound, Mehlape (2017) posits that localised and contextually 
relevant capacity building interventions have high probabilities of contributing towards 
improving individual and organisational performances. Linked to the previous section (refer to 
section 5.6.1) on the use practice-oriented examples, such as case studies and real -world 
examples argument, Punia and Kant (2013), recommends that the proposed practice -oriented 
and led examples should make use an endogenous or localised lens  to evaluation . Their 
argument is that using examples, which appreciate participant’s indigent  experiences as well 
as t heir professional practices, will potentially accelerate the acquisition, sustainability and 
usability of skills and knowledge opportunities within organisations. Chilisa and Tsheko (2014) 
support this narrative by stating that an Afrocentric lens should underpin the ECB interventions 
implemented in Africa . Morkel and Mangwiro (2018)  specify that the development of 
evaluation-oriented curriculum design and its teaching approaches should reflect the context in 
which evaluation is practi sed in the region. This will possibly contribute to the improvement 
of the broader evaluation ecosystem and inherently the organisations operating in Africa. 
 Gaotlhobogwe et al.  (2018), escalate Punia and Kant (2013) contentions on practice-oriented 
curriculum by purporting that African ways of knowing and doing should be incorporated into 
capacity building initiatives as well as the broader evaluation profession. These African based 
authors assert that values, culture, and beliefs are the domains that should be intertwined with 
the capacity building interventions. In support of this narrative, respondents earlier indicated 
that the MAE approach (see section 4.2) might serve as a distinguishing factor between DETPA 
programme and other training initiatives implemented outside the region such as the Global 
North. Turab and Casimir (2015), recommends that training providers should think about  
commissioning trainee’s needs assessments, followed by the curriculum design adapted to the
<<<PAGE=111>>>
110 
 
skills and knowledge needs emerging from the findings of the needs assessments. As a result , 
the DETPA project team should consider the inclusion of the use of needs assessments so that 
the real-life examples and the MAE approach are interwoven into the programme.  
5.6.2 Implementation period and logistics 
The respondents signified factors outside the training room. Although there were noticeable  
differences from the responses elucidated from the respondents, the issues raised are should be 
factored in the design of the future training programmes such as DETPA. Specifically, these  
disparities related to the implementation period (two weeks versus one week). Below follows 
some of the responses from the interviewees  expressing their voices regarding the 
implementation period:  
“I found 2 weeks short considering this is supposed to be a hands-on course. Can we also 
think of having two sessions in a year?” (Participant 4). 
“Consider course duration of 1 week - it appears target participants are busy people may 
struggle with being away for 2 weeks” (Line manager 2). 
Tonhäuser and Büker (2016) warned that training implementation factors (beyond the training 
intervention itself)  influence the classsroom as well as the extent to which learning and transfer 
of training occurs. The wave of using technology for teaching and learning has grown in the 
past decade. African s cholars such as Boitshwarelo  (2009) have argued tha t the use of 
technology and the availability of Internet is an innovative  catalyst in the conceptualisation of 
training programmes in the near future . This author argues that if used correctly, technology 
can create a blend between traditional face -to-face (f2f) and an online delivery modes. This 
implies that a new teaching and learning approach should be forged which will facilitate a  
content, context and technology in order to deliver relevant and useful M&E curricula. All in 
all, the use of technology as argued by Boitshwarelo (2009) can possibly cater for the needs of 
participants who are proponents of a longer or shorter implementation period.  
 
5.7 Conclusion 
 
In summary, the analysis chapter documents the following conclusions. This study illustrated 
that applying Kirkpatrick’s model in this study illuminate  that the model is sequential or 
chronological. In other words, each level serves as a building block of the next level. As an
<<<PAGE=112>>>
111 
 
illustration, the findings that are recorded in Level 1 have  the potential to influence Level 2. 
However, it looks like  this phenomenon does not automatically apply in the application or 
analysis of Level 3 and Level 4. At these levels, issues of complexities arise. In other words, it 
is not given that the results witnessed in Level 3 will inherently be the same as Level 4. This is 
because linking organisation performance with a training programme entails recognition of 
multifaceted factors beyond the training intervention.  
 In the case of the current study, a combination of factors positively influenced the reactions of 
the participants. These include; the coordination of the programme, the curriculum content and 
the context in which it was delivered including the role of the facilitators and their facilitation 
styles. The cited factors enabled an environment where M&E skills, knowledge were acquired 
by the participation. various factors namely; the coordination of the was generally perceived to 
be of good quality. According to respondents, it has contributed towards skills and knowledge 
acquisitions, led to the transfer of skills. In addition, to some degree , the programme has 
contributed towards improving organisational performance. However, respondents cautioned 
argued that these improvements could not solely be attributed to the contribution played by the 
DETPA programme.  Significantly, a contextually fitted curriculum content such as ‘ Made in 
Africa’ (MAE) was cited as one of the key findings that determined the perceived contribution 
of the DETPA programme. Respondents posited that this enabled them to acquire  then apply 
the acquired skills and knowledge as well as improve their job and organisational 
performances.  
This was followed by the contribution of the role of experienced local facilitators and the use 
of case studies, which  were also perceived as factor s that contributed towards learning. This 
empowered participants with skills and knowledge to navigate the relationship between theory 
and practice. Therefore, a combination of these elements (contextually fitted curriculum, local 
experienced facilitators, use of case studies) motivated participants to apply the skills and 
knowledge acquired towards improving their job performance.  
As a way forward, areas of improvement were recommended. The study found that the use of 
case studies and  the integration of the MAE approach should be applied. It  is anticipated 
implementing this approach will contribute towards unveiling uniformity across the entire 
programme. As an illustration, respondents argued that the MAE approach and the use of case 
studies, cs as teaching approaches should be interlinked across the programme. It is  believed 
that these approaches will improve the planning and delivery of the programme.
<<<PAGE=113>>>
112 
 
CHAPTER 6 CONCLUSION AND RECOMMENDATIONS  
This chapter present s a summary of the research reports  as well as  recommendations and 
prospective research areas.  
6.1 Summary of the research report 
 
This research study was conducted at a time when  very few studies aimed at assessing the 
effectiveness of training programmes in Africa have been conducted. Usually, w hen 
assessments are conducted, they are perceived to be linear, incomplete and inclined to measure 
the effectiveness of training skills haphazardly  (Morkel  & Ramasobana, 2017; Wao et al., 
2017). These assessments are predominantly using a positivist research paradigm . 
Consequently, participant perceptions of programmes such as DETPA are unknown. Hence, 
the primary research problem focused on soliciting participant perceptions using the DETPA 
2017 programme as a case study in terms of its effectiveness as postulated by Kirkpatrick’s 
(1959) model of  training effectiveness . Essentially, this  research study document ed 
participants’ perceptions on the effectiveness of the DETPA programme to highlight an 
important concept of HRD, which looks to assess the impact of training on individuals, their 
job performance  and organisations. This includes understanding concepts such as 
organisational performance and social theory  followed by ECD and more especially ECB as 
some of the key importan concepts in grounding  studies focusing on training interventions.  
A qualitative case study research approach was used  to address the primary research question 
posed in chapter one, which was : What are participant’s perceptions regarding the DETPA 
2017 programme in terms of its effectiveness in respect of Kirkpatrick’s four levels of training 
effectiveness: reaction, learning, transfer and organisational impact. The subquestions included 
questions seeking participants perception on (i) the overall perceptions and reactions to the 
DETPA 2017   programme?, (ii) whether or not they have acquired new knowledge, skills and 
learning from participating in the delivery of the DETPA 2017 programme. This was followed 
by (iii)  their perceptions regarding their  job- related performance in M&E after their 
participation in the DETPA 2017 as well as (iv) their perceptions regarding the impact of the 
training on their organisations, specifically in terms of M&E practice? (v) . Finally, a sub-
question beyond Kirkpatrick’s training effectiveness aimed at identi fying the gaps  of
<<<PAGE=114>>>
113 
 
programmes such as the DETPA 2017 in building skills, knowledge, individual performance 
and organisational improvement and how they can be enhanced was posed. 
 An in -depth li terature review aimed at perusing similar and previous research studies  was 
conducted by the researcher. This provided the conceptual underpinning and situate d the 
knowledge gaps for the study. This gap w as to provide insights on the African experience of 
ECB in particular through the lens of the DETPA programme. 
The findings from the study are:  
(i) the  co-ordination of the programme, the curriculum content and the context in 
which it was implemented, the facilitators role and their facilitation sytles and 
trainees attitudes/attributes are the characteristics which were cited to have 
contributed to the majority of participat’ s positive reaction to the overall 
programme. 
(ii) Furthermore, the majority of the respondents indicated that they acquired M&E 
skills and knowledge and increased their evaluation technical and cross -cultural 
competencies. Thus, it has been concluded that  learning was attained during the  
DETPA programme.  
(iii)  Respondents further mentioned that the combination of their noticeable learnings 
and management support improved and sustained transfer of skills and knowledge  
(iv) In addition, respondents acknowledged that a new culture of doing things was 
observed in their respective organisations. Despite some inconsistencies on the 
responses, participants were largely convinced that there were improvement s in 
organisational performances.  
(v) Standardising some elements of the programme such as the use of case studies and 
facilitaton styles and integrating the ‘Made in Africa’ evaluation approach (because 
of context matters) in the design programme, were cited as the two main areas of 
improvement.  
The study also revealed that improving organisational performance requires a multifaceted and 
comprehensive approach beyond a singular training intervention such as the DETPA 
programme.
<<<PAGE=115>>>
114 
 
Overall, the limitations of this study include; a) the use of purposive and convenience sampling, 
b) the involvement of the researcher in the planning and delivery of the programme, c) research 
time frames and the use of technological ly advanced communication platforms. First of all, 
purposive sampling uses self-reporting as an approach to gathering data. Respondents in similar 
studies that have used self-reporting (such as the current study) have been criticised for 
elucidating responses which are framed in a positive tone . As previously stated in the 
methodology section, the resear cher was involved in the planni ng and implementation of the 
programme. Therefore, the researcher develop ed professional relationships with the 
respondents which might have influenced them to provide overly positive responses. As a way 
of mitigating this limitation, research principles which empowered the researcher to separate 
the reseach from daily work concerning the DETPA programme were applied. On a different 
note,the timeframes on which the researcher was expected to comp lete the Master 's study 
limited the scope of the research.  In other words, the researcher had to comply with the 
timeframes spelt out in the research workplan as well as the university guideline to com plete 
the research within a year. The use of technologically advanced communication platforms such 
as Skype and Whatsapp calls utilised as the mode of data collection in this study was impaired 
by the limited network coverage in some instances. This caused delays in the data collection 
process as well  as required additional administ rative work (back and forth) between the 
researcher and the respondents.  The presented limitations explain why the case study cannot 
be generalised although the findings are useful in informing planning, delivery and assessment 
of training programmes implemented in Africa.  
 6.2 Recommendations and future research  
 
In summary, the literature reviewed pursuant M&E practitioners and scholars in Africa to 
acknowledge the historical past responsible for the regional underdevelopment. This was 
elucidated by the literature that persuaded the researcher to acknowledge the fact that context 
matters. The reviewed literature further recognised that  very few studies assessing training 
effectiveness conducted in the region. In cases where such studies are commissioned, the 
majority of these studies used positivist research methods to assess training effectiveness.  
This study makes the following recommendations:
<<<PAGE=116>>>
115 
 
(i) Further studies exploring the linkages between participants reactions, learning, job 
performance and organisational performances, particularly on how they contribute 
towards growing the evaluation practice in the region should be commissioned 
(ii) Research focusing on identifying and addressing the gaps on programmes such as 
the DETPA programme should be undertaken 
(iii) The DETPA programme should be informed by the findings of this study 
(iv) More studies focusing on interlinking the role of training programmes and 
organisational performance should be conducted, 
(v) Training programmes which integrate and standardise MAE approach, case studies 
and facilitation styles in the design of the curriculum  should be explored or 
increased, 
(vi) Lastly, the M&E ecosystem including stakeholders such as donor funders, and 
suppliers of training programmes should jointly provide resources  geared to 
replicate programmes such as the DETPA programme across the continent.
<<<PAGE=117>>>
116 
 
 
REFERENCES  
 
Abdulwahed, M., & Nagy, Z. K. (2009). Applying Kolb’s Experiential Learning Cycle for 
Laboratory Education. Journal of Engineering Education, 98(3), 283–294. 
https://doi.org/10.1002/j.2168-9830.2009.tb01025.x  
Abrahams, M. A. (2015). A review of the growth of monitoring and evaluation in South Africa: 
Monitoring and evaluation as a profession, an industry and a governance tool. African 
Evaluation Journal, 3(1), 1–8. https://doi.org/10.4102/aej.v3i1.142  
African Capacity Building Foundation (ACBF). (2016). Capacity Requirements for the New 
African Vision. Retrieved from 
https://opendocs.ids.ac.uk/opendocs/bitstream/handle/123456789/12840/CR for Agenda 2063 
English.pdf   
Alawneh, M. K. (2008). Factors affecting training transfer: participants’ motivation to 
transfer training, literature review. Academy of Human Resource Development International 
Research Conference in the Americas, (c), 7–32. https://doi.org/10.1080/13803610701626127  
Aliaga, O. A., (2005). A study of Human Resource Practices in Minnesota Companies .Human 
Resource Development Centre , Retrieved from 
http://richardswanson.com/hrdrcreports/Aliaga-2005innovation.pdf
  
Alliger, G. M., & Janak, E. A. (1989). Kirkpatrick’s Levels of Training Cirteria: Thirty Years 
Later. Personnel Psychology , 42(2), 331–342. https://doi.org/10.1111/j.1744-
6570.1989.tb00661.x  
Aluko, F. R., & Shonubi, O. K. (2014). Going beyond Kirkpatrick’s Training Evaluation 
Model: The role of workplace factors in distance learning transfer. Africa Education Review, 
11(4), 638–657. https://doi.org/10.1080/18146627.2014.935007
  
Axtell, C., Maitlis, S., & Yearta, S. K. (1997). Predicting Immediate and Longer -Terms 
Transfer of Training. Personnel Review, 26(3), 201–213. 
Baldwin, Ti. T., & Ford, J. K. (1988). Transfer of Training: a Review and Directions for Future 
Research. Personnel Psychology , 41(1), 63–105. https://doi.org/10.1111/j.1744-
6570.1988.tb00632.x
<<<PAGE=118>>>
117 
 
Bansal, A., & Thakur, M. (2013). The Impact of Perception of Organizational Transfer Climate 
Factors and Trainees’ Characteristics on Training Transfer : The Context of Mergers and 
Acquisitions. Journal of International Business and Economics, 1(1), 50–66. 
Basheka, B. C., & Byamugisha, A. (2015). The state of Monitoring and Evaluation (M & E ) 
as a discipline in Africa From infancy to adulthood ? African Journal of Public Affairs , 8(3), 
75–95. 
Gravenhorst, B, Kilian, M., Werkman, R. A., & Boonstra, J. J. (2003). The change capacity of 
organisations: General assessment and five configurations. Applied Psychology, 52(1), 83–
105. https://doi.org/10.1111/1464-0597.00125 
Blaser Mapitsa, C., & Chirau, T. J. (2019). Institutionalising the evaluation function: A South 
African study of impartiality, use and cost. Evaluation and Program Planning, 75(November), 
38–42. https://doi.org/10.1016/j.evalprogplan.2019.04.00 
Bellagio Report. (2012). African thought leaders’ forum on evaluation for development 
expanding thought leadership in Africa (Vol. 39). Retrieved from http://www.clear -
aa.co.za/wp-content/uploads/2013/09/Bellagio-Report-Vs-Apr-14.pdf  
Boyle, D., & Harris, M. (2009). The challenge of Co-Production - How equal partnerships 
between professionals and the public are crucial to improving public services. Public 
Administration, 28. Retrieved from 
http://b.3cdn.net/nefoundation/312ac8ce93a00d5973_3im6i6t0e.pdf%5Cnhttp://s.bsd.net/nef
oundation/default/page/file/312ac8ce93a00d5973_3im6i6t0e.pdf.  
Brown, R. E., Reed, C, S. (2002). An Integral Approach to Evaluating Outcome Evaluation 
Training. American Journal of Evaluation, 23(1), 1–17. 
https://doi.org/10.1177/109821400202300102.  
Brown, L., Lafond, A., & Macintyre, K. (2001). Measuring Capacity Building. (March), 919–
966. Retrieved from www.cpc.unc.edu/measure.  
Browning, P. (1970). Evaluation of Short -Term Training in Oregon Studies in the 
Rehabilitation Retarded. Retrieved from 
https://files.eric.ed.gov/fulltext/ED057208.pdf#page=41  
Bryman, A. (2012). Social research methods. Oxford: Oxford University Press.
<<<PAGE=119>>>
118 
 
Cameron, R. (2009). New Public Management Reforms in the South African Public Service: 
1999-2009. Journal of Public Administration, 44(1), 897–909. Retrieved from 
https://open.uct.ac.za/handle/11427/22033.  
Carter, B. (2010). Evaluation Capacity Development. In IEG World Bank - IFC - MIGA (Vol. 
23). 
Chang, Y.- H. E. (2010). An Empirical Study of Kirkpatrick’s Evaluation Model in the 
Hospitality Industry. FIU Electronic Theses and Dissertations. 325. 
https://doi.org/10.25148/etd.FI10120807. 
Chilisa, B. & Kawulich, B. (2012). Selecting a Research Approach: Paradigm, Methodology, 
and Methods. Doing Social Research A Global Context, (October), 51–61. 
Chilisa, B. (2017). Decolonising transdisciplinary research approaches : an African perspective 
for enhancing knowledge integration in sustainability science. Sustainability Science , 12(5), 
813–827. https://doi.org/10.1007/s11625-017-0461-1. 
Chilisa, B., & Tsheko, G. N. (2014). Mixed Methods in Indigenous Research: Building 
Relationships for Sustainable Intervention Outcomes. Journal of Mixed Methods Research, 
8(3), 222–233. https://doi.org/10.1177/1558689814527878 . 
Chipkin, I., & Lipietz, B. (2012). Transforming South Africa’s racial bureaucracy : New 
Public Management and public sector reform in contemporary South Africa. 1–27. Retrieved 
from https://wiser.wits.ac.za/system/files/seminar/Chipkin2012.pdf  
CLEAR- AA. (2017). ETDP SETA Phase 2 Programmes Tracer Study. JHB. 
CLEAR-AA Report. (2018). Development Evaluation Training Programme in Africa Concept 
Note (Vol. 27). 
Cloete, F. (2016). Developing an Africa -rooted programme evaluation approach. African 
Journal of Public Affairs, 9(4), 55–70. 
Cooley, S. J., Cumming, J., Holland, M. J. G., & Burns, V. E. (2015). Developing the model 
for optimal learning and transfer (MOLT) following an evaluation of outdoor groupwork skills 
programmes. European Journal of Training and Development, 39(2), 104–121. 
https://doi.org/10.1108/EJTD-06-2014-0046.  
Creswell, J. W. (2007). Qualitative Inquiry & Research Design : Chosing Among Five 
Approaches (2nd Ed.). Thousand Oaks, CA.: Sage Publication Inc.
<<<PAGE=120>>>
119 
 
Dzimbiri, L. B. (2008). Experiences in New Public Management in Africa : The Case of 
Performance Management Systems in Botswana. Africa Development, 33(4), 43–58. 
Falola, H. O., Osibanjo,  a O., & Ojo, S. I. (2014). Effectiveness of Training and Development 
on Employees ’ Performance and Organisation Competitiveness in the Nigerian Banking 
Industry. Bulletin of the Transilvania University of Brasov Series V: Economi c Sciences, 
7(56), 161–170. 
Foxon, M. (1989). Evaluation of Taining and Development Programs: A Review of the 
Literature. Australian Journal of Educational Technology , 5(2), 89–104. Retrieved from 
http://www.ascilite.org.au/ajet/ajet5/foxon.html  
Galport, Nicole, & Azzam, T. (2017). Evaluator Training Needs and Competencies: A Gap 
Analysis. American Journal of Evaluation, 38(1), 80–100. 
https://doi.org/10.1177/1098214016643183
   
Gaotlhobogwe, M., Major, T. E., Koloi -Keaikitse, S., & Chilisa, B. (2018). Conceptualizing 
Evaluation in African Contexts. New Directions for Evaluation, 2018(159), 47–62. 
https://doi.org/10.1002/ev.20332  
Carton, R. (2004). Measuring organizational performance: an exploratory study by Robert B. 
Carton (Under the Direction of Charles W. Hofer). 11(2), 2–4  
Castro, M. P., Fragapane, S., & Rinaldi, F. M. (2016). Professionalization and evaluation : A 
European analysis in the digital era. https://doi.org/10.1177/1356389016667887 
Ghere, G., King, J. A., Stevahn, L., & Minnema, J. (2006). Reflecting on Program Evaluator 
Competencies. 27(1), 108–123. https://doi.org/10.1177/1098214005284974  
Giangreco, A., Carugati, A., & Sebastiano, A. (2010). Are we doing the right thing?: Food for 
thought on training evaluation and its context. Personnel Review, 39(2), 162–177. 
https://doi.org/10.1108/00483481011017390
  
Gjers, M. C. A model of the antecedents of training transfer. , (2015). 
Gravenhorst, B, Kilian M. Werkman, R. A., & Boonstra, J. J. (2003). The change capacity of 
organisations: General assessment and five configurations. Applied Psychology, 52(1), 83–
105. https://doi.org/10.1111/1464-0597.00125 
Gupta, P. D., & Sinha, V. (2012). A study on return on investment of Training Programme in 
a government enterprise in India. Vikalpa; The Journal of Decision Makers 31(1), 31-48
<<<PAGE=121>>>
120 
 
Haddadi, F., & Yaghoobi, T. (2014). Key indicators for organizational performance 
measurement. Management Science Letters, 4(9), 2021 –2030. 
https://doi.org/10.5267/j.msl.2014.8.019 
Healey, M., & Jenkins, A. (2000). Kolb’s experiential learning theory and its application in 
geography in higher education. Journal of Geography , 99(5), 185–195. 
https://doi.org/10.1080/00221340008978967.  
Itika, S. (2011). Fundamentals of human resource management . African Studies Centre / 
University of Groningen / Mzumbe University. Retrieved from 
http://rozenbergquarterly.com/itika-josephat-stephen-fundamentals-of-human-resource-
management-emerging-experiences-from-africa/  
Jasson, C. C., & Govender, C. M. (2016). Measuring return on investment and risk in training 
– A business training evaluation model for managers and leaders. Acta Commercii. 17(1),  
1–9. 
Keane, M., Khupe, C., & Muza, B. (2016). It matters who you are: Indigenous knowledge 
research and researchers. Education as Change , 20(2), 163–183. 
https://doi.org/10.17159/1947-9417/2016/913.  
Kumpikaite, V., & Sakalas, A. (2011). The Model of Human Resource Development System’s 
Evaluation. International Conference on E-Business, Management and Economics, 25, 46–50. 
Lakshmi, K. S., & Agyeman, C. M. (2014). Exploring the effectiveness of training and 
development exploring the effectiveness of training and development programmes : a study on 
banks. 
Levin, R. M. (2017). Professionalising monitoring and evaluation for improved performance 
and integrity : opportunities and unintended consequences. Journal of Public Administration, 
Volume 52, 136–149. Retrieved from https://hdl.handle.net/10520/EJC-990e4853e
  
Lusthaus, C., Adrien, M., & Perstinger, M. (1999). Capacity development: definitions, issues 
and implications for planning, monitoring and evaluation. Universalia Occasional Paper, 
35(35), 1–21. Retrieved from http://preval.org/documentos/2034.pdf  
Lingham, T., Richley, B., & Rezania, D. (2006). An evaluation system for training programs: 
a case study using a four‐phase approach. Career Development International, 11(4), 334–351. 
https://doi.org/10.1108/13620430610672540
<<<PAGE=122>>>
121 
 
Machin, M. A. (2002). Planning, managing, and optimizing transfer of training. Creating, 
Implementing, and Managing Effective Training and Development , 263–301. Retrieved from 
http://www.wiley.com/WileyCDA/WileyTitle/productCd- 
0787953962.html%5Cnhttp://eprints.usq.edu.au/2284/1/Machin,_2002.PDF%5Cnhttps://epri
nts.usq.edu.au/2284/.   
Mbabaali, R. (2015). Strengthening Capacity for Monitoring and Evaluation In Uganda: A 
Results Based Management Perspective. Innovative Higher Education, 8(3), 191–202. 
https://doi.org/10.3390/educsci8010034.  
Mbava, N. P., & Dahler -Larsen, P. (2019). Evaluation in African contexts: The promises of 
participatory approaches in theory- based evaluations. African Evaluation Journal, 7(1), 1–9. 
https://doi.org/10.4102/aej.v7i1.383  
Martin, B. O., Kolomitro, K., Lam, (2014). The change Capacity of Organizations : General 
The Change Capacity of Organisations : General Assessment and Five Configurations. 
4(November 2014), 2021–2030. https://doi.org/10.1177/1534484313497947  
Mehlape, M. M. (2017). The Role of Human Resource Development in Improving Municipal 
Service in South Africa. Journal of Public Administration, (July), 106–112. 
Methode, K., Osunsan, O. K., Florence, I., Augustine, W., Abiria, P., & Innocent, B. (2019). 
Effect of Organizational Change on Employee Performance among selected Commercial 
Banks in Bujumbura, Burundi . (May). 
Morell, J., & Flaherty, E. W. (1978). The development of evaluation as a profession : Current 
status and some predictions Hahnemann Medical College and Hospital. 1, 11– 
Morkel, C. & Mangwiro, N. (2018). Implications of Evaluation Trends for Evaluation Capacity 
Building. In Evaluation Landscape in Africa. Johannesburg: Unpublished. 
Morkel, C & Ramasobana, M. (2017). Measuring the effect of Evaluation Capacity Building 
Initiatives in Africa : A review . 1–12. Retrieved from 
https://aejonline.org/index.php/aej/article/view/187.  
Martin, B. O., Kolomitro, K., & Lam, T. C. M. (2014). Training Methods: A Review and 
Analysis. Human Resource Development Review, 13(1), 11–35. 
https://doi.org/10.1177/1534484313497947
  
Matyesha, M. E. (2011). The Impact of Organisational Change on Employee Motivation and 
Performance Levels. UNIVERSITY OF KWAZULU-NATAL
<<<PAGE=123>>>
122 
 
Ndashimye, G. (2015). Rwanda capacity development country outlook report : national CD 
Expert Rwanda [RGB Nyagatare and Gatsibo Capacity Building Coach in Plan M&E 2014-
2015. 
Nhlabathi, S. (2016). Implementation of skills development programmes in the education 
training and development sector . Masters research report. Johannesburg: University of the 
Witwatersrand. 
Nickols, F. (2011). Leveraging the Kirkpatrick Model Validation vs Evaluation. 
Noe, R. A, & Schmitt, N. (1986). The influence of trainee attitudes on training effectiveness : 
test of a model. Personnel psychology, 39(3), 497-523. 
Noe, R. A. (1986). Trainees’ Attributes and Attitudes : Neglected Influences on Training 
Effectiveness. Academy of Management Review, 11(4), 736–749. 
Nyasha, T. (2011). The Impact of Organisational Change: A study of the Gauteng Provincial 
Department of Infrastructure Development. Graduate School of Business Leadership 
University of South Africa. 
Odor, H. O. (2014). Organisational Change and Development. SSRN Electronic Journal, 
(April). https://doi.org/10.2139/ssrn.670601
  
Ogbu, J. M. (2017). Impact of Employee Training on Organizational Performance: A Study of 
Selected Insurance Firms in Abuja- Nigeria. European Journal of Business and Management 
Www.Iiste.Org ISSN, 9(14), 64–72. Retrieved from www.iiste.org
  
Pestoff, V. (2016). New Public Governance and Accountability – Some Jewels in a Treasure 
Chest.  Guest lecture on May 3, 2011 at the CIES program on Corporate Social Responsibility 
and Social Enterprise in Atlanta, Georgia. 
Phawe, K. (2016). The Perceptions of Adolescent Males on their Involvement in Teenage 
Pregnancy Prevention in Kliptown. Bachelor degree dissertation, Johannesburg: University of 
the Witwatersrand. Retrieved from https://hdl.handle.net/10539/25592   
Podems, D. (2014). Evaluator competencies and professionalizing the field: Where are we 
now? The Canadian Journal of Program Evaluation, 28(3), 127–136   
Preskill, H. & Boyle, S. (2008). A Multidisciplinary Model of Evaluation Capacity Building. 
American Journal of Evaluation, (909), 443–459. https://doi.org/10.1177/1098214008324182
<<<PAGE=124>>>
123 
 
Przybylska, E. (2016). Professionalization in Evaluation. European Journal of Internal 
Medicine, 16(1), 1–42. https://doi.org/10.3138/cjpe.29.3.1 
Punia, B., & Kant, S. (2013). A Review of Factors Affecting Training Effectiveness Vis-À-Vis 
Managerial Implications and Future Research Directions. International Journal of Advanced 
Research in Management and Social Sciences, 2(1), 151–164.  
Ramasobana, M., & Kone, N. N. (2019). Elements of a successful evaluation capacity building 
intervention in Africa. Retrieved from Blog website: 
https://www.google.com/search?q=Nagnouma+Nanou+Kone&rlz=1C1GCEU_enZA839ZA8
39&oq=Nagnouma+Nanou+Kone&aqs=chrome..69i57.642j0j7&sourceid=chrome&ie=UTF-
8  
Ross, S., & Hopson, R. (2006). Building Evaluation Capacity: 72 Activities for Teaching and 
Training. In Evaluation Models, approaches and designs  (Vol. 27). 
https://doi.org/10.4135/9781412983549  
Sánchez-Romero, M., & Prskawetz, A. (2019). Optimal time allocation in active retirement. 
Central European Journal of Operations Research. https://doi.org/10.1007/s10100-019-00663-
8  
Sharma, D. (2016). Assessment of evaluation theory : Kirkpatrick model in opposition to 
Hamblin model. International Journal of Science Technology and Management, 1, 194–204. 
Subramanian, K.S., Sinha, V., & Gupta, P.D. (2012), A Study on Return on Investment of 
Training Programme in a Government Enterprise in India. Vikalpa, 37(1), 31-48. 
Sujatha, S., Lakshmi, K.S., Agyeman, C.M. & Kumar, N.S. (1989). Exploring the effectiveness 
of training and development programmes: a study on banks. Pensee, 76(4),313-324. 
Sydhagen, K., Cunningham, P., Sydhagen, K., & Cunningham, P. (2007). Human Resource 
Development in Sub- Saharan Africa. Human Resource Development , 10(2), 121-
135.https://doi.org/10.1080/13678860701347156.  
Tarsilla, M. (2014). Evaluation capacity development in Africa: Current landscape of 
international partners’ initiatives, lessons learned and the way forward. African Evaluation 
Journal, 2(1), 1–13. https://doi.org/10.4102/aej.v2i1.89.  
Tarsilla, M. (2017). From Evaluation Capacity Building to Evaluation Capacity Development 
- A Paradigm Shift.  Evaluation capacity building: key issues and limitations . Retrieved from 
https://ideas-global.org/wp-content/uploads//2017/12/Chapter-6.pdf.
<<<PAGE=125>>>
124 
 
Thackeray, R. (2016). A New Conceptual Framework for the Evaluation of L & D 
Programmes. D.Prof. thesis. Middlesex University. 
Tirivanhu, P,, Robertson, H., Waller, C & Chirau. T (2018). Assessing evaluation education in 
african tertiary education institutions : opportunities and reflections. South African Journal of 
Higher Education, 32(4), 229–245.  
Tracey, J. B., Tannenbaum, S. I., & Kavanagh, M. J. (1995). Applying Trained Skills on the 
Job : The Importance of the Work Environment. Journal of Applied Psychology , 80(2), 239–
252.  
Turab, G.M. & Casimir, G. (2015). A model of the antecedents of training transfer, 
International Journal of Training Research, 13(1), 82- 95, DOI: 
10.1080/14480220.2015.1051352.   
Ulum, Ö. G. (2015). Program Evaluation through Kirkpatrick ’ s Framework. 8(1). 
Van der Westhuizen, G., Chilisa, B., Major, T. E., Gaotlhobogwe, M., & Mokgolodi, H. (2016). 
Decolonizing and indigenizing evaluation practice in Africa: Toward African relational 
evaluation approaches. African Evaluation Journal , 1(1), 313–328. 
https://doi.org/10.4102/aej.v1i1.44.  
Vince, R. (1998). Behind and beyond Kolb’s learning cycle. Journal of Management 
Education, 22(3), 304–319. https://doi.org/10.1177/105256299802200304.  
Wao, H., Onyango, R., Kisio, E., Njatha, M., & Onyango, N. O. (2017). Strengthening capacity 
for monitoring and evaluation through short course training in Kenya. African Evaluation 
Journal  Vol 5, No 1 | a192 | DOI: https://doi.org/10.4102/aej.v5i1.192   
Weinberger, L. A. (1998). Commonly held theories of human resource development. Human 
Resource Development International, 1(1), 75–93. 
https://doi.org/10.1080/13678869800000009.  
World Bank Report. (2005). Building Country Capacity for Monitoring and Evaluation in the 
Public Sector: Selected Lessons of International Experience. In ECD Working Paper Series ♦ 
13 (Vol. 3). https://doi.org/10.1002/ev.  
Wotela, K. (2016). Towards an outcomes-based approach to a ‘research approaches, procedure 
and methods chapter’ for business and management. Journal of Public Administration, 52(1), 
223-246.
<<<PAGE=126>>>
125 
 
Wotela, K. (2017). A proposed monitoring and evaluation curriculum based on a model that 
institutionalises monitoring and evaluation. African Evaluation Journal, 5(1), 1–8. 
Yeung, R. (2009). Are school uniforms a good fit?: Results from the ECLS -K and the NELS. 
Educational Policy, 23(6), 847–874. https://doi.org/10.1177/0895904808330170.  
Zaciewski, R. (2001). Measuring Training's Effectiveness. Quality Progress, 34(6), 104.
<<<PAGE=127>>>
126 
 
APPENDICES 
Appendices 1.1: Project information sheet  
 
Part I: Project Information Statement 
Participant’s perceptions of the effectiveness of CLEAR -AA’s Development Training 
Programme in Africa 
I, Mokgophana Ramasobana, am a Masters student at the University of the Witwatersrand 
(Wits). I am conducting research on the effectiveness of M&E short course pieces of training. 
This study meets the requirements of the Research Ethics Committee (Human) of the Wits.   
 
Aims of the Research 
The Primary research aim: 
To assess participant's’ perceptions of whether or not the CLEAR -AA DETPA 2017 
programme contributed towards improving their M&E skills and knowledge, reaction to the 
programme, assessment of the content, job performance and organisational impact.  
The secondary research aims: 
1. To assess participant’s perceptions and reactions of the 2017 DETPA course 
programme in terms of contribution to improving the skills and knowledge? 
2. To assess participant’s perceptions regarding their job related performance in M&E 
post the delivery of the DETPA 2017? 
3. To assess participants’ perceptions regarding the impact of the training on their 
organizations, specifically in terms of M&E practice? 
4. To assess participants’ perceptions regarding their organizations behavioural change, 
specifically in terms of M&E practice and whether it has improved their organisational 
performance? 
5. To assess participant’s perceptions regarding the gaps of programmes such as the 
DETPA 2017 in building skills, knowledge, individual performance and organisational 
improvement and how they can be enhanced  
Significance of the Research Project 
The research is significant in the following way: 
Studies, particularly in Africa have not sufficiently assessed participant’s perception of the 
effectiveness of M&E short course training programmes. Thus, the contribution of the DETPA 
programme towards building participant’s individual capacities (skills  and knowledge), their 
individual behavioural change and organisational behavioural change is unknown, whilst 
CLEAR-AA plans to continue implementing the programme on an annual basis.  In the limited 
cases where assessments were conducted in Africa, studies indicate that the curriculum of the 
training programmes offered is densely theoretical without practical components. In other 
words, the training are not tailored to respond to the participant’s skills and knowledge needs.
<<<PAGE=128>>>
127 
 
Therefore, it might be contended that the courses are observed to be irrelevant and not useful. 
This study contributes towards deepening scholars an understanding on how to assess the 
effectiveness of short -course training programmes in M&E from the perspe ctive of 
participants, which may contribute to understanding how to better design and implement such 
programmes with the target audience’s needs in mind 
Benefits of the Research to ECDB ecosystem 
1. This study contributes towards deepening scholars an understanding on how to assess 
the effectiveness of short -course training programmes in M&E from the perspective of 
participants, which may contribute to understanding how to better design and implement such 
programmes with the target audience’s needs in mind. 
2. The findings from the study will assist ECB scholars in general and CLEAR -AA in 
particular to improve the conceptualisation, implementation and assessment of training 
programme interventions 
. 
Research Plan and Method 
My research includes: 
• interviews with DETPA participants and their line managers; 
• interviews with DETPA facilitators and moderators; and 
• an in-depth document study.   
Purposive and convenience sampling will be used in all cases.   
In terms of the DETPA participants, line managers, facilitators and moderators: 
A minimum of 10 and a maximum of fifteen interviews will be conducted.  The researcher will 
ask all interviewees whether they are aware of their line managers who would have insights to 
share, and who could be contacted for an interview. Stratified sampling will be used to identify 
the first ten DETPA participants in accordance with their country of origin to participate in the 
study. A contact list of all DETPA participant, facilitators and moderators representing fifty -
five participants, eight facilitators and three moderators have been obtained from CLEAR-AA 
database by the researcher. This contact list represents the entire population from which the 
sample size will be drawn. 
Permission will be sought from each respondent prior to his or her participation in the research. 
Only those who consent will participate.   I, Mokgophana Ramasobana, will administer the 
interview, which will take approximately forty -five minutes to comple te.  All information 
collected will be treated in the strictest confidence and individuals will not be identifiable in 
any reports that are written. The role of all participants is voluntary and invited participants 
may decide to withdraw their participation at any time without penalty. The nature of the data 
to be collected is not sensitive and will pose no harm or embarrassment to the individuals 
participating in the study. 
Procedure 
Once I have received an individuals’ informed consent to participate in the study, I, 
Mokgophana Ramasobana, will arrange a time with each individual for the interview to take 
place.
<<<PAGE=129>>>
128 
 
Further Information 
Attached is a letter assuring you of the research ethics considerations that have been undertaken 
in the preparation of this study, as well as a Consent Form./ 
 
Thank you for taking the time to read this information. 
Mokgophana Ramasobana     
MM Candidate/Researcher   
 
Part II: Information to ensure you of the research ethics considerations of this study 
You are being asked to provide consent to participate in a DETPA research study.  We will 
provide you with the necessary information to assist you to understand the study and explain 
what would be expected of participants. These guidelines would include the risks, benefits, and 
your rights as a study subject.  Please feel free to ask the researcher to clarify anything that is 
not clear to you.   
To participate, it will be required of you to provide a written consent that will include your 
signature, date and initials to verify that you understand and agree to the conditions. 
You have the right to query concerns regarding the study at any time. Immediately report any 
new problems during the study, to the researcher.  The telephone number of the researcher is 
(mobile): +27 72 936 5645 or email mokgophana@gmail.com .  Please feel  free to call or 
email.    
Furthermore, it is important that you are aware of the fact that the Research Ethics Committee 
(REC) of the university will submit the ethical integrity of the study for approval. The REC 
consists of a group of independent experts that has the responsibility to ensure that the rights 
and welfare of participants in research are protected and that studies are conducted in an ethical 
manner.  Studies cannot be conducted without REC approval.  I have received REC approval 
for this study, attached is the approva l letter. Queries with regard to your rights as a research 
subject can be directed to University of the Witwatersrand, Wits School of Governance, PO 
Box 601, Wits, 2050. 
If no one could assist you, you may email or contact the Research Office: The Research 
Director, Prof Pundy Pillay at pundy.pillay@wits.ac.za
    or +27 11 717 3808 or Ms. Sithembile 
Xaba at sithembile.xaba@wits.ac.za  or +27 11 717 3133. 
Participation in research is voluntary.  You are not obliged to take part in any research.  If you 
do partake, you have the right to withdraw at any given time, during the study without penalty.   
The study may be terminated at any time by the researcher, the sponsor or the Research Ethics 
Committee.  
Although your identity will at all times remain confidential, the results of the research study 
may be presented at scientific conferences or in specialist publications.  This informed consent 
statement has been prepared in compliance with current statutory guidelines.
<<<PAGE=130>>>
129 
 
 
Appendices 1.2: Informed consent form 
 
Participants Consent Form 
DETPA 2017 participant 
I,…………………………………………………………………………………………………
………………………..(name and surname ) give my consent to my participation in the 
Masters research of Mr. Mokgophana Ramasobana, entitled: “Participants Perceptions of the 
Effectiveness of CLEAR-AA’s Development Evaluation Training Programme in Africa”  
I have read the Project Information Statement explaining the purpose of the research project 
and understand that: 
• My role as a participant of the DETPA programme is voluntary. 
• Participants may withdraw from the study at any time without penalty. 
• I have been invited to participate and permission had been sought from me.  
• Only participant who consent will participate in the project. 
• All information obtained will be treated in strictest confidence.  
• Participant’s names will not be used and will not be identifiable in any written reports about 
the study.  
• A report of the findings will be made available to the participant. 
• I may seek further information on the project from Mokgophana Ramasobana on +27 
72 936 564545 or mokgophana@gmail.com
  
• I consent to the interviews being recorded by voice and/or note taking and transcribed, and 
the material used in the report. 
__________________________   ___________________________ 
Print Name     Signature 
 
__________________________ 
Designation/Position 
______________________    
Date
<<<PAGE=131>>>
130 
 
 
Appendices 1.3: Interview Guide 
 
 
INTERVIEW SCHEDULE  
 
Dear Participant 
Thank you for taking the time to participate in this interview.  It will take approximately one hour to complete, but I may discover some insights 
as I write up the findings, and would like to ask whether you would be willing to participate in a second, possibly shorter, interview at a later 
stage?  
I will be starting off with a few biographical questions. 
All your responses are strictly confidential, and you will not be identifiable in the final report.  I will not be providing the names of the 
organisations or persons interviewed, in order to ensure that you remain anonymous. 
I am more interested in your personal opinions, perceptions and views on the effectiveness of the DETPA programme, so please feel free to 
speak your mind, as that is what I am most interested in, and which will be of most value to this study. 
Thank you once again for your time. 
SECTION A   
Name of participant:  
Country:  
Position /Title:
<<<PAGE=132>>>
131 
 
Contact Details:   
(for follow-up, if needs be) 
SECTION A       Please place an “x” in the appropriate block 
OCCUPATION   
Coding: OCC  
 
NO OF YEARS IN M&E OCCUPATION Less than 1 year  1-5 years  6-10 years  11 -15  16 - 20  21 or more  
Coding: OCCYRS 1 2 3 4 5 6 
 
HAVE YOU PARTICIPATED IN ANY OTHER M&E 
PROGRAMME BESIDES DETPA 
No 
never 
Less than 
1 year 
 1-5 
years 
 6-10 
years 
 11 -
15 
 16 - 
20 
 21 or 
more 
 
Coding: OSCTP  1 2 3 4 5 6 
 
GENDER  MALE  FEMALE  OTHER (PLEASE SPECIFY, IF WILLING) 
……………………………………………………………………………………. 
 
 
Coding: G 1 2 3 
 
HIGHEST 
QUALIFICATION 
COMPLETED 
Phd  Masters  Honours or 
4th Year 
Equivalent 
 Postgraduate 
Diploma 
 Degree  National 
Diploma 
 Grade 
12  
 High 
School 
 Below 
High 
School 
 
Coding: QUAL 1 2 3 4 5 6 7 8 9
<<<PAGE=133>>>
132 
 
HIGHEST M&E 
RELATED 
QUALIFICATION 
COMPLETED  
Phd  Masters  Honours 
or 4th Year 
Equivalent 
 Postgraduate 
Diploma/Certificate 
 Degree  National 
Diploma 
 Grade 
12  
 High 
School 
 Below 
High 
School 
 
Coding: QUAL 1 2 3 4 5 6 7 8 9 
 
 
         
 
SECTION B 
Components of 
the conceptual 
framework 
Research Questions Comments 
(You are 
encouraged to 
provide in-
depth 
explanations) 
Reactions 1. Overall, what is your reaction to the 2017 DETPA programme delivered by CLEAR-AA?   
2. What is your perception on whether OR not you liked the training and would you consider it 
relevant?  
 
3. Do you perceive the DETPA training to have been good OR not good use of your time?   
4. What is your perception on the level of effort required to make most of the learning during the 
delivery of the training? 
 
5. How has it improved OR not improved your skills and knowledge?   
6. What is you perception regarding the facilitators and instructional design used during the 2017 
DETPA? Please elaborate why the facilitators and instructional design have contributed OR 
not contributed to your skills and knowledge acquisition) 
 
Learning 1. What is your perception on whether OR not you have learnt from participating during the 
delivery of the DETPA programme?  
 
2. What is your perception on whether OR not you have learnt what was intended to be learnt?
<<<PAGE=134>>>
133 
 
 3. What is your perception regarding whether or not your skills and knowledge were advanced as 
intended prior participating in the DETPA programme? 
 
Transfer 1. What is your perception regarding whether OR not the DETPA programme has contributed 
towards improving your job performance in M&E? (Please elaborate) 
 
2. What is your perception regarding whether OR not the learning from the DETPA programme 
improved how you do your work? (Please elaborate)  
 
 3. What is your perception regarding whether OR not your change in behaviour and new 
knowledge was sustained? 
 
 4. What is your perception on whether OR not you can be able to transfer the skills and 
knowledge learnt through the DETPA programme to fellow colleagues 
 
Organisation  1. What is your perception regarding whether OR not the DETPA programme has impact on 
your overall organisational performance (change in behaviour)?  (Please elaborate why the  
2017 DETPA programme have OR not have impact on your organisational performance) 
 
Others 1. What is your perception regarding the gaps of programmes such as the 2017 DETPA in 
building skills, knowledge, individual performance and organisational improvement (change 
in behaviour) and how they can be enhanced? (Please elaborate on the gaps and how they 
can be enhanced)