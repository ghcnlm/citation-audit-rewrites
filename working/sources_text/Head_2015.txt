<<<PAGE=1>>>
Brian W. Head is professor of policy 
analysis at The University of Queensland, 
Australia. He has also held several senior 
roles in government. He has published 
widely on public policy, public management, 
social issues, and environmental policy. He 
is undertaking projects on research utiliza-
tion, research impacts, wicked problems, 
policy innovation, natural resources issues, 
and social program evaluation.
E-mail: brian.head@uq.edu.au
472 
Public Administration Review, 
Vol. 76, Iss. 3, pp. 472–484. © 2015 
The Authors. Public Administration Review 
published by Wiley Periodicals, Inc. on 
behalf of The American Society for Public 
Administration. 
DOI: 10.1111/puar.12475.
The copyright line for this article was 
changed on April 5, 2016 after original 
online publication.
This is an open access article under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, 
the use is non-commercial and no modiﬁ  cations or adaptations are made.
 Brian W. Head
The University of Queensland, Australia
Abstract: Th e quality of public decision making depends signiﬁ cantly on the quality of analysis and advice provided 
through public organizations. Champions of “evidence-informed” policy making claim that rigorous evaluation 
practices can signiﬁ cantly improve attainment of cost-eﬀ ective outcomes. After decades of experience, performance 
information is more sophisticated, but evaluation practices and capabilities vary enormously. Public agencies gather 
and process vast amounts of information, but there has been little analysis of how this information is actually utilized 
for policy and program improvement. Th is article examines how government agencies use evidence about policy and 
program eﬀ ectiveness, with attention to four themes: (1) the prospects for improving “evidence-informed” policy mak-
ing, (2) the diversity of practices concerning evidence utilization and evaluation across types of public agencies and 
policy arenas, (3) recent attempts to “institutionalize” evaluation as a core feature of policy development and budget 
approval, and (4) the relationships between public agencies and nongovernmental sources of expertise.
Practitioner Points
• Although most practitioners claim to support the use of evidence relevant to their roles, their use of the best 
available evidence is patchy.
• Commitment to funding and using evidence from evaluations is essential.
• Political and ideological factors often undermine evidence-informed practices.
• Interaction and brokering across organizational boundaries are crucial.
(Bogenschneider and Corbett 2010; Davies, Nutley, 
and Smith 2000; Meltsner 1976; Nutley, Walter, 
and Davies 2007; Radin 2000; Weiss 1980). Within 
public policy discussions, it is axiomatic that reliable 
information and expert knowledge are integral to 
sound processes for formulating and implementing 
policy (Radaelli 1995); however, the processing of 
this information and expert knowledge is problem-
atic and highly variable across organizations. Th e 
potential for close linkage between good informa-
tion and “good policy making” is routinely under-
mined by two important mechanisms: political and 
organizational.
First, the policy process is inescapably anchored 
in political values, persuasion, and negotiation 
(Majone 1989). In this politicized context, some 
kinds of evidence are inevitably seen as more rel-
evant than others for underpinning policy positions. 
Th ese political dynamics are expressed through the 
preferences and agenda setting of political leaders, 
legislators, lobbyists, and stakeholders, mediated 
through media communication and public opin-
ion. Policy scholarship has clearly demonstrated 
that the neutral and objective evidence of scientiﬁ  c 
T oward More “Evidence-Informed” Policy Making?
C
oncerns to make better use of evidence in pol-
icy making are closely linked to widespread 
pressures for improved eﬀ ectiveness in service 
delivery and accountability in democratic countries. 
Th is focus on better design of policies and programs 
for improved eﬀ ectiveness has been most evident 
within domestic policy issues but has also attracted 
recent concerns about better design and delivery of 
overseas aid programs. Evidence-informed decision-
making processes, relying on transparent use of sound 
evidence and appropriate consultation processes, are 
seen as contributing to balanced policies and legiti-
mate governance. Th e eﬃ  ciency and eﬀ ectiveness 
goals are complemented by wider concerns to improve 
the perceived legitimacy of policy-making processes 
and civic trust in decision makers.
Th e “evidence-based policy” movement developed 
early momentum in the 1970s (Aaron 1978; Bulmer 
1982, 1987; Rivlin 1971) and enjoyed renewed 
strength beginning in the late 1990s. It sought to 
promote rigorous analysis of policy and program 
options, with the intention of providing useful inputs 
for policy makers in their ongoing consideration 
of policy development and program improvement
<<<PAGE=2>>>
Toward More “Evidence-Informed” Policy Making? 473
Four key themes and challenges for evidence use by government 
agencies emerged from this literature and serve as the structure for 
the article:
1. Th  e prospects of improving “evidence-informed” policy 
making
2. Th  e continuing diversity of practices concerning evidence 
use in diﬀ erent policy arenas and diﬀ erent types of public 
agencies
3. Recent attempts to “institutionalize” evaluation as a 
core feature of policy development and budget approval 
processes
4. Th  e variable relationships between public agencies and 
external (nongovernment) sources of expertise
Th ese issues are analyzed in the following sections. Th e purpose of 
the article is to contribute to a better understanding of how govern-
ment agencies are involved in generating, considering, and using 
reliable evidence from various sources and how the commissioning 
and communication of research is creating new platforms for col-
laboration in evidence use, as well as to suggest directions for further 
research, including the need for comparative analysis of trends and 
outcomes.
Under What Conditions Is Evidence-Informed Policy 
Making Possible?
Th e research literature on improving policy making through bet-
ter use of evidence encompasses a wide range of viewpoints, but 
two main camps can be identiﬁ ed among those who endorse the 
importance of good evidence in the policy process. Th e ﬁ rst camp 
believes that evidence-based approaches are possible but require a 
signiﬁ cant commitment to rigorous methodologies for program 
evaluation (e.g., Banks 2009; Boruch and Rui 2008; Campbell 
1969; Coalition for Evidence-Based Policy 2015; Davies 2004; 
Donaldson, Christie, and Mark 2009; Mosteller and Boruch 2002; 
Nussle and Orszag 2014; Petrosino et al. 2001; Rivlin 1971). Th e 
scholars in this group believe it is both feasible and highly desir-
able to strengthen the capacity of public institutions to use rigorous 
methods. Th ey claim that reliable information about “what works” 
has been inadequate and that improvements depend on public agen-
cies endorsing program evaluations based on randomized controlled 
trials (RCTs). Maynard (2006) has called for careful identiﬁ cation of 
well-grounded sources of evidence and research synthesis on which 
practitioners can rely but notes that personal and political inter-
pretations of evidence continue to play a signiﬁ cant role in policy 
making.
Among the champions of rigorous evaluation for evidence-informed 
policy making, Haskins and Margolis (2014) speak for many 
proponents when they claim that President Barack Obama’s support 
for promoting social programs validated by rigorous evaluation 
evidence “has the potential to become the most eﬀ ective strategy yet 
for attacking the nation’s social problems” (2014, 238). Th e ambi-
tion of building a more evidence-informed public sector, whether 
in the United States or any other country, requires institutionaliza-
tion through government support for long-term investment in data 
collection and analysis (on key social, economic, and environmental 
matters), as well as investment in technical and managerial skills for 
interpreting and utilizing information from multiple sources (Head 
knowledge does not, and cannot, drive policy in a democratic 
political system. Evidence is harnessed to competing arguments 
about ends and means, and the political decision-making process 
is inherently marked by conﬂ  icts, trade-oﬀ s, and compromises 
(Lindblom 1979). In light of this political context, the early 
ambitions of “evidence-based policy” have recently been heav-
ily qualiﬁ ed, with many writers now adopting the more modest 
phrase “evidence-informed policy.”
Second, in diﬀ erent public organizations, the information needs 
and practices of senior managers will vary considerably. While 
access to accurate information is very important in all agencies, the 
speciﬁ c administrative practices and procedural rules governing 
information selection and use in each type of organization are cru-
cial for the way evidence is identiﬁ ed and utilized. Th us, patterns 
of evidence use and information management vary across policy 
domains (e.g., social policy, economic development, environmental 
regulation) and across organizational types associated with diﬀ erent 
public sector functions (e.g., service delivery, regulatory oversight, 
and policy development). In the research literature on public 
agencies, organizational types have been diﬀ erentiated in various 
ways.
1 For the present discussion, it is suﬃ  cient to distinguish 
between the key functional roles of policy development, regulatory 
oversight, and service delivery and to suggest that public agencies 
undertaking such functions are likely to have diﬀ erent information 
requirements.
Scope and Method
Th is article focuses on evidence utilization by government agencies 
in their quest for eﬃ  cient management of programs and regulatory 
regimes, improvement of program outcomes, and provision of pol-
icy-relevant advice. Th e literature on research utilization has grown 
rapidly in the OECD (Organisation for Economic Co-operation 
and Development) group of countries, especially in North America 
and Western Europe, where the debate on the potentialities of 
“evidence-informed” policy has been most concentrated. My focus 
is on the national level of government, while acknowledging that 
decentralized levels of public authority play very important roles in 
many countries, especially for the delivery of human services and 
urban infrastructure. Given the enormous breadth of governmental 
activities and policy issues, the focus of this article is restricted to 
human services, including education, social security, public health 
care, and policing services. (Th us, some areas in which scientiﬁ c 
information has been mandated in decision making—such as food 
safety standards, and environmental pollution standards—are not 
included in this article.)
Key themes were selected through extensive searches of major 
journals in public administration, public policy, and organizational 
studies to identify signiﬁ cant analyses of the evolving interface 
between science (or, more broadly, expert knowledge relevant to 
policy) and the policy-making processes of public agencies. Reviews 
of research utilization, research translation, and policy evaluation 
practices provided guidance on major ﬁ ndings and themes, includ-
ing the links between expertise, evaluation practices, and program 
improvement. Networks of research experts also provided direct 
advice concerning cutting-edge examples of government utilization 
of rigorous evidence and assisted the author in identifying signiﬁ -
cant reports and practitioner analyses.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=3>>>
474 Public Administration Review  May | June 2016
Th us, problem-solving capacity might not proceed apace with the 
expansion of knowledge. As new knowledge becomes available, 
science experts become aware of related knowledge gaps and limita-
tions (Lindblom and Cohen 1979), raising new uncertainties and 
complexities about causal linkages. Hence there will be continued 
uncertainties about the relationships between research ﬁ ndings, 
policy options, and implementation pathways. Second, there has 
been concern that policy problems have been loosely deﬁ ned, mak-
ing appropriate solutions harder to ascertain. Close attention to 
clearly deﬁ ning the underlying policy problems is widely recom-
mended (Dery 1984; Rochefort and Cobb 1994). Th e assumption 
is that tighter deﬁ nition and rigorous evaluation will allow relevant 
knowledge and experience to be assembled to address issues and 
improve outcomes (Glennerster 2012; Orszag 2009). Clear deﬁ ni-
tion of the problem is easier said than done, however, especially in 
relation to complex or “wicked” social problems in which conﬂ ict-
ing perspectives are deeply embedded.
Leaders who might wish to create more systematic linkages 
between the realm of rigorous research-based analysis and the 
politicized realm of policy design and deliberation face some 
serious obstacles. Th e advocates of science-based and technocratic 
policy making face diﬃ  cult challenges inherent in the democratic 
political process (Head 2013; Tseng and Nutley 2014). Even if 
sound evidence that is useful for policy anal-
ysis continues to expand, the political nature 
of policy debate and decision making is 
generally unfavorable to science-driven per-
spectives. Government oﬃ  cials and political 
leaders are often motivated by sociopolitical 
factors other than research evidence (Boswell 
2008; Head 2010; Howlett 2009; Shillabeer, 
Buss, and Rousseau 2011). Political lead-
ers in a democracy may be more focused 
on political argumentation, maintaining 
stakeholder support, engaging with media-framed debates, and 
managing risks. Evidence can inform and enrich these political 
debates in a democracy but does not drive the outcome (Majone 
1989; Shulock 1999). Given the multiple interests, perspec-
tives, and problem frames mobilized by policy actors, the link-
ages between evidence and policy are deeply mediated by diverse 
evolving contexts, interpretations, negotiations, and organizational 
practices (Fischer and Gottweis 2012; Hajer and Wagenaar 2003; 
Hammersley 2013).
Th ere are millions of policy and program documents produced 
annually by government oﬃ  cials. But there has been surprisingly lit-
tle research concerning how policy bureaucrats actually make deci-
sions informed by available evidence and what sources of evidence 
are actually deployed in this process (Halligan 1995; Mandell and 
Sauter 1984). While there is a large literature on program imple-
mentation and program evaluation, relatively little attention has 
been given to how evidence is used within public bureaucracies in 
the policy development work of public employees. In some agencies, 
there are dedicated units concerned with policy analysis, research, 
and review. However, relatively little research has been undertaken 
to explore the practices, skills, and capacities of these policy workers: 
how they undertake their policy design and review roles, how they 
perceive their tasks, how they use information, what sources they 
2013; Solinis and Baya-Laﬃ  te 2011). Th e process goal is to extend 
the use of evaluation and review mechanisms (as discussed later in 
this article), with clear procedures for assessing the impact of various 
programs, regulations, and interventions and with feedback into the 
policy development process. Th ese developments would most likely 
occur in a system in which legislators, and the political culture more 
generally, are supportive of transparency and knowledge sharing.
By contrast, the second camp believes that although improvements 
are highly desirable, there is no prospect of constructing a public 
policy decision-making system shaped mainly by research and 
evaluation ﬁ ndings. Th is group argues that good decision making 
should be informed by a range of relevant “best available” evidence, 
accepting a broad conception of usable knowledge and recognizing 
the value of relevant professional expertise (Head 2008; Lindblom 
and Cohen 1979; Nesta 2011; Pawson 2006; Shillabeer, Buss, and 
Rousseau 2011). Th is group also accepts that conﬂ ict and bar-
gaining are ongoing features of a democratic political system and 
acknowledges the intrinsic role of values, ideologies, and economic 
interests in shaping policy making (e.g., Lindblom 1979; Majone 
1989; Radin 2006; Shulock 1999; Weiss 1979, 1999). In particular, 
for addressing complex policy and program areas, these scholars 
accept that collaborative approaches to knowledge sharing and 
adaptive management in light of experience will be necessary (Head 
and Alford 2015; Schorr and Auspos 2003; 
Schorr and Farrow 2011, 2014; Weber and 
Khademian 2008).
Th e early hopes of the evidence-based policy 
movement for large and rapid improvements 
in policies and programs through better use 
of rigorous research were not rapidly fulﬁ lled 
(Aaron 1978; Bulmer 1987; U.K. Cabinet 
Oﬃ  ce 1999; U.K. T reasury 2007). Th e 
reasons given for this lack of progress reveal 
some underlying diﬀ erences in perspective about the relationship 
between science, policy, and politics. For the advocates of scientiﬁ c 
evaluations and the use of RCTs, increasing the supply of high-
quality evaluations is critical. Th is supply-side solution depends, in 
turn, on increased investment in program evaluation and analytical 
skills and a willingness to learn from pilot programs (Sanderson 
2002). However, the demand-side issues are also important: who 
will pay to underwrite these investments, and who will actually use 
the high-quality information in their decision making? Political 
and legislative leaders are sometimes seen as unreliable allies in 
supporting social research in an era deeply aﬀ ected by partisan 
ideology, pressure group politics, and issue-based media campaigns 
(Moynihan and Roberts 2010). For example, it has been shown that 
the analytical resources utilized by the U.S. executive and legislative 
branches of government since the 1960s have ﬂ uctuated substan-
tially over time and seem to have been undermined by periodic 
waves of partisan politics (Baumgartner and Jones 2015, chap. 4; 
Joyce 2011; Williams 1998).
Th e puzzles about how to strengthen evidence-informed pro-
cesses and how to improve the outcomes of social programs have 
also attracted other lines of explanation in the research literature. 
One strong argument is that perhaps the problems are tougher to 
understand and more intractable to resolve than initially thought. 
Even if sound evidence that is 
useful for policy analysis con-
tinues to expand, the political 
nature of policy debate and 
decision making is generally 
unfavorable to science-driven 
perspectives.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=4>>>
Toward More “Evidence-Informed” Policy Making? 475
situations, partisans are likely to “cherry-pick” evidence that seems 
to support their existing positions (Weiss 1979) rather than take a 
balanced view of the available evidence. Th e partisan use of evi-
dence (“policy-driven” evidence) is an inevitable part of democratic 
debate. Handling these value-based conﬂ icts is the responsibility of 
political leaders, usually through stakeholder dialogue, rather than 
the domain of science itself. Th e production of “more” research 
is unlikely to settle the underlying issues. Th e ﬁ ndings of social 
research are focused on the analysis of social phenomena rather than 
the illumination of the policy and governance frameworks within 
which the debate is conducted.
It has been claimed that evidence-informed processes are more likely 
to develop in policy areas in which a policy approach or paradigm 
has become relatively “settled” and in which ideological disputa-
tion has diminished (Head 2010; Mulgan 2009). Th is stability and 
continuity allow for an iterative process of reﬁ nement, evaluation, 
and continuous improvement over a number of years. However, in 
some policy areas in which extant approaches are no longer seen to 
be delivering expected results, support for innovation and policy 
change may emerge. Th e source of inﬂ uential alternative policy 
ideas may well be located outside the government sector, requiring a 
more pluralist approach to developing new solutions and new ways 
to work with nongovernmental organizations (NGOs) (Mulgan 
2006; Osborne and Brown 2013). Disruptions in policy direction 
also regularly occur as a result of political change (for example, 
when a new conservative government has diﬀ erent commitments 
and goals from its social-democratic predecessor or vice versa). Calls 
for evidence-based approaches in the United Kingdom after 1997, 
following the election of the “New Labour” government, had some 
of these characteristics. Th e performance and evaluation database 
built around previous programs may become less germane in the 
search for new directions.
In policy areas that are widely seen as amenable to the ﬁ ndings of 
objective analysis, such as public health programs, the quality, acces-
sibility, and transparency of information is generally seen to promote 
a public perception of fair and legitimate decision making (Niessen 
et al. 2011). Studies in public health indicate rich information 
and processes for assessing and implementing evidence-informed 
systems and practices (e.g., Commission on Social Determinants of 
Health 2008; Lavis et al. 2003; Lavis et al. 
2008; Lemay and Sá 2014; National Research 
Council 2009). Assessing the cost-eﬀ ectiveness 
of pharmaceutical products and other health 
therapies has been a major focus of health 
regulators and program managers (Fox 2010). 
For example, the Drug Eﬀ ectiveness Review 
Project is a collaborative venture in which 
state Medicaid oﬃ  cials from 13 U.S. states 
pool information about the beneﬁ ts of speciﬁ c 
drugs used in health programs (Hall and 
Jennings 2012). More generally, the systematic reviews of health 
interventions commissioned by the Cochrane Collaboration now 
constitute a major library of knowledge about evidence-based health 
care (see http://community.cochrane.org/cochrane-reviews).
However, there are also policy areas in which systematic research is 
hard to ﬁ nd or professional experience and intuition are preferred 
trust, and how they process the feedback from political leaders and 
key stakeholders (Hall and Jennings 2008, 2010; Head et al. 2014; 
Jennings and Hall 2012; Wagenaar 2004).
Moreover, the literature on the policy process distinguishes between 
phases such as “problem deﬁ nition,” “data analysis,” “policy design,” 
“policy implementation,” and “program review” (Sabatier and 
Weible 2014). Th is diﬀ erentiation suggests some intriguing conse-
quences that are deserving of further exploration. First, it is possible 
that these roles are performed by very diﬀ erent sets of profession-
als who work in their own “silos” (Howlett and Wellstead 2011). 
In this scenario, perhaps only a very small minority of senior staﬀ  
are well positioned to understand and inﬂ uence the “big picture,” 
including interorganizational relationships and the changing infor-
mation requirements across various elements of the policy process. 
Second, it is possible that diﬀ erent groups of disciplinary knowledge 
are deployed in some of these roles. For example, it is likely that 
economic cost–beneﬁ t analysis will be vital in the policy design and 
program evaluation phases, but legal reasoning may be central for 
governance processes, and social analysis may be central for concep-
tualizing needs and social outcomes. For all of these reasons, and 
others discussed later, the search for evidence-informed policy and 
practice will be a long and arduous journey.
Diversity in Policy Arenas
Th e research literature on public organizations distinguishes among 
the key functional roles in policy development, regulatory oversight, 
and service delivery across a range of policy and administrative 
responsibilities. Agencies undertaking policy, regulatory, and service 
functions have speciﬁ c and divergent information requirements. Th e 
frameworks and practices for managing policy, regulation, and ser-
vice issues across diverse policy arenas are correspondingly diverse. 
Governments occasionally seek to impose generic or standardized 
requirements on all agencies—most notably, in relation to ﬁ nancial 
systems, reporting systems, personnel management systems, and 
obligations under public law (such as access to information, admin-
istrative appeal rights, and so on). But in regard to how knowledge 
is mobilized and how policy and program decisions are actually 
made, diversity is to be expected. It is also likely that agencies at dif-
ferent levels of government (federal, state, local) will reﬂ ect distinc-
tive patterns of stakeholder engagement, use of expert evidence, and 
capacity to deploy policy resources.
One of the most diﬃ  cult challenges is how 
to make better use of sound research within 
controversial or conﬂ ictual policy areas, 
which are characterized by highly publicized 
value diﬀ erences. In complex, value-laden 
areas—such as biotechnology applications 
in health care (e.g., Mintrom and Bollard 
2009), sociolegal policy responses to gun 
violence (e.g., Edwards and Sheptycki 2009), 
juvenile oﬀ ending (e.g., Petrosino, T urpin-Petrosino, and Buehler 
2002), or refugees and illicit immigration (e.g., Boswell 2009, 
2012)—rational and reasonable deliberative processes can become 
sidetracked by media-driven controversy. T o the extent that research 
ﬁ ndings are widely used as ammunition within strongly emotive 
debates, it may be only a short step to accusations that research 
on these matters is inherently biased and lacks objectivity. In such 
One of the most diﬃ  cult chal-
lenges is how to make better use 
of sound research within con-
troversial or conﬂ ictual policy 
areas, which are characterized 
by highly publicized value 
diﬀ erences.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=5>>>
476 Public Administration Review  May | June 2016
the politics of policy debate and the impact of stakeholders than the 
use of scientiﬁ c information by public agencies. T o overcome the 
diﬃ  culty of learning from discrete or unique cases, analysts have 
called for more comparative studies of how evidence use might vary 
among agencies across national boundaries and across policy areas. 
For example, a recent symposium presented case studies from six 
European countries (Nutley et al. 2010). It was apparent that some 
countries, and some speciﬁ c agencies, have become more advanced 
in championing evidence-informed approaches. While government 
investment in science-related research is one important dimension, 
the level of science and evaluation spending did not explain the vari-
ations in agency behavior. Nutley et al. developed a broad explana-
tory framework linking several knowledge factors with institutional 
context factors that interacted in diﬀ erent ways:
[W]e worked with a similar framework of research supply 
(knowledge creation), policy and practice demand (knowl-
edge application) and the linkages between supply and 
demand (knowledge mediation). We also asked participants 
to  comment on how these arrangements are shaped by the 
cultural, political and administrative context of their country. 
(2010, 134)
Th us, there is merit in pursuing comparative work to explain vari-
ations in evidence use, by understanding the complex dynamics of 
knowledge supply and demand in a variety of institutional contexts. 
Th ere has been only limited documentation of comparative experi-
ence concerning evidence-informed policy 
processes, despite recent eﬀ orts by the OECD 
to stress the importance of evidence-based 
approaches (OECD 2015).
Institutionalizing the Importance of 
Evidence and Evaluation
Th e champions of evidence-informed policy 
and administration have long argued that 
the key task is to institutionalize rigorous 
processes for appraisal and evaluation at the 
heart of public ﬁ nances (Rist 1990). Th is would require both supply-
side capacity, that is, skills and systems for producing good-quality 
analysis from organizations both inside and outside government, 
and demand-side facilitation, that is, the formal system requirements 
and inducements for using such analyses (Mayne et al. 1992).
As institutional processes for analysis and evaluation became 
professionalized and routinized, it was clear there were two impor-
tant purposes and functions underlying the institutionalization of 
evidence use. I term these the program accountability agenda and 
the policy eﬀ ectiveness and innovation agenda. Th e “accountability” 
agenda is long-standing in public administration and is concerned 
with the eﬃ  cient and eﬀ ective management of publicly funded 
programs. Here, the pressure is on leaders and managers in public 
organizations to demonstrate ongoing accountability for expend-
ing resources in optimal ways to meet performance targets. Th ese 
targets are usually linked to a system of key performance indicators 
and veriﬁ ed through multiple layers of independent auditing and 
public scrutiny (Frederickson and Frederickson 2006; Heinrich 
2007; Radin 2006; Van Dooren, Bouckaert, and Halligan 2010). 
By contrast, the “eﬀ ectiveness and innovation” agenda goes beyond 
to academic research as the basis for decision making. According 
to Jennings and Hall (2012), in a wide-ranging study of informa-
tion use in U.S. state agencies, many agencies paid only symbolic 
lip service to rigorous use of evidence. Jennings and Hall suggest a 
simple 2 × 2 typology of government agencies, based on two sets of 
key variables: (1) the degree of conﬂ ict concerning the core issues of 
the agency and (2) the level of scientiﬁ c capacity at the disposal of 
the agency (availability, relevance, and credibility of evidence). Th is 
heuristic suggests four types of government agency, as outlined in 
table 1.
Evidence-based initiatives are more advanced in particular policy 
sectors. In social policy, these sectors include health care services, 
child and youth development, education and vocational skills, crime 
control and corrections, family services, social care for vulnerable 
groups, and technology-assisted innovations in service delivery. 
Systematic reviews have been conducted in many of these areas 
under the auspices of the Campbell Collaboration (http://www.
campbellcollaboration.org/lib ). Several research centers have also 
been active in providing estimates of return on investment in crime 
prevention programs, emphasizing the avoided costs of incarcera-
tion and court processes (e.g., Clear 2010; France and Homel 2007; 
Jones et al. 2008; Tilley 2010). Prevention-based orientations to 
social policy design have been fruitful in recent years (Puttick 2012). 
For example, in youth oﬀ ending policy, the concept of “justice 
reinvestment” envisages a redirection of resources currently spent on 
incarceration (policing, prisons) toward tackling the upstream causes 
of criminal behavior—“family breakdown, 
poverty, mental illness, drug and alcohol 
dependence” (IPPR 2011, 4). However, while 
the principles of prevention-based approaches 
have achieved widespread rhetorical support, 
in practice, they have been vulnerable to 
populist “law and order” campaigns by politi-
cal leaders. In other areas such as school edu-
cation, the implementation of standardized 
skills testing has become a core mechanism for 
assessing school performance, and controlled 
trials have been conducted to assess the eﬀ ectiveness of various 
learning regimes and school governance arrangements (Mosteller 
and Boruch 2002). Analysts have also begun to reconsider the 
distinctive forms of interaction—in education, health, and social 
care—between the national system and decentralized or local-level 
systems (Best and Holmes 2010).
Th e policy literature has been focused mainly on individual case 
studies (i.e., single issues in single countries, such as Monaghan 
[2011] on U.K. drug policy; Vifell and Sjögren [2011] on Swedish 
pharmaceuticals policy; and Boswell [2012] on U.K. immigration 
policy). Much of this case study literature is more concerned with 
 Table 1 Expected Use of Evidence-Based Processes in Government Agencies, by 
Degree of Conﬂ ict and Level of Scientiﬁ c Capacity
Level of Conﬂ ict
Low High
Level of Scientiﬁ c 
Capacity
High 1. Evidence-based 
agency
2. “Challenged” evidence-
based agency
Low 3. Experiential agency 4. Symbolic agency
Source: Jennings and Hall (2012, 261, table 5).
While the principles of pre-
vention-based approaches have 
achieved widespread rhetorical 
support, in practice, they have 
been vulnerable to populist 
“law and order” campaigns by 
 political leaders.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=6>>>
Toward More “Evidence-Informed” Policy Making? 477
they might be equally satisﬁ ed by a combination of evidence types, 
including expert consensus processes (e.g., Prato 2007; Schorr and 
Auspos 2003), to ascertain program eﬃ  cacy and consider best value 
options.
Another important ﬁ eld in which public agencies may wish to uti-
lize systematic evidence is the development of “impact statements” 
for regulatory reform proposals and project assessments (T urnpenny 
et al. 2009). Many jurisdictions have introduced a requirement to 
identify and measure likely impacts in two situations: (1) when 
proposed changes in regulatory regimes may have impacts on busi-
ness and (2) when proposed major development projects may have 
environmental impacts. In such cases, it is often mandated that an 
analysis be undertaken to assess the likely social, economic, and 
environmental impacts of proposed changes. Th e OECD (2009) 
has taken steps to collect experience about “best-practice regulation” 
and has promoted thorough models for regulatory assessment that 
aim to protect business while achieving social or other objectives. 
Evidence for the appraisal of prospective risks and impacts (ex ante 
analysis) is not seen as “scientiﬁ c” in exactly the same sense as evi-
dence from RCTs, which assess actual interventions. Nevertheless, 
some agencies clearly make use of scientiﬁ c and other expert knowl-
edge in their work of justifying regulatory changes (Desmarais and 
Hird 2014).
A potential additional source of evidence for the development of 
policy and program ideas is learning from the experience of other 
jurisdictions. Th e adaptation of policy frame-
works previously implemented elsewhere 
has become more common as policy net-
works expand and communication channels 
improve. Policy borrowing and diﬀ usion have 
been widely undertaken across many ﬁ elds, 
but many adoption decisions are politically 
driven rather than evidence based in design 
and implementation. Th e inherent problems 
and pitfalls of policy transfer and diﬀ usion 
are well documented, and making appropriate 
choices can be fraught with risks (Benson and Jordan 2011; Shipan 
and Volden 2012).
Among the most signiﬁ cant types of frameworks and programs that 
have been diﬀ used across jurisdictions are performance report-
ing systems. In the United States, for example, federal and state 
legislatures and public agencies have been involved in major waves 
of performance management reforms and reviews (e.g., Ellig, 
McTigue, and Wray 2012; Heinrich 2012; Shillabeer, Buss, and 
Rousseau 2011). Th e U.S. Government Accountability Oﬃ  ce 
has long advocated the use of program evaluations and has issued 
guidance on their conduct (GAO 2009). Th e U.S. Oﬃ  ce of 
Management and Budget has repeatedly stated its commitment to 
make “rigorous, independent program evaluations” a key tool for 
program eﬀ ectiveness and value for money (OMB 2010). Haskins 
and Margolis (2014) have demonstrated how several key social pro-
grams have been the subject of signiﬁ cant evaluation scrutiny as a 
result of this commitment to independent and rigorous review. Th e 
Congressional Research Service has a history of providing studies of 
federal programs on behalf of elected oﬃ  cials seeking policy-related 
performance information (Joyce 2011; Williams 1998).
issues of operational eﬃ  ciency, reliability, and ﬁ ne-tuning. It seeks 
to reconsider policy options and program design. In doing so, it 
seeks to identify the most eﬀ ective methods for achieving positive 
outcomes, taking into account diverse contexts and collaborative 
program requirements (Osborne and Brown 2013). As Behn (2003) 
observes, there are many possible purposes underlying performance 
evaluation and monitoring systems, and many varieties of review 
and evaluation have been instituted across both the accountability 
and eﬀ ectiveness agendas.
Program evaluations and policy reviews are either undertaken by 
government agencies themselves or contracted to various research 
centers, think tanks, and evaluation professionals. Because evaluation 
requires speciﬁ c skills, evaluation activity has become a professional-
ized area of work across the government and nongovernment sectors. 
Th e quality and independence of evaluation reports has gradually 
improved over some decades of experience. Nevertheless, even when 
public oﬃ  cials have access to good evaluations and expert knowl-
edge, there is no guarantee they will boldly “follow the evidence” 
rather than conform to the political signals of executive government 
or the cultural and organizational practices of their own agency. For 
example, Stevens (2011) found that policy oﬃ  cials in a major crimi-
nal justice agency in the United Kingdom tacitly relied on docu-
mentation and interpretation that reinforced existing policy stances 
or narratives. While the availability of evaluation reports is expected 
to enhance the potential quality of subsequent deliberation, such 
reports do not determine how government policy makers actually use 
evidence in their decision making (Sullivan 
2011). For example, in Norway, after the gov-
ernment commissioned a series of comprehen-
sive evidence-based reports on major issues, 
the quality of the research base was found not 
to have been a prominent factor in the devel-
opment of policy conclusions (Innvær 2009). 
A review of research on the uses of evaluation 
found that “engagement, interaction, and 
communication between evaluation clients 
and evaluators is key to maximizing the use of 
evaluation in the long run” (Johnson et al. 2009, 389). At the same 
time, some professional evaluators report they have been subjected 
to direct and indirect pressure concerning the shape of their ﬁ ndings 
and recommendations (Morris and Clark 2013).
In practice, governments have to deal with information gaps and 
make decisions under conditions of uncertainty. In many areas of 
policy making and program development, there are serious uncer-
tainties about “what works for whom” and under what conditions 
(Boaz et al. 2008). Hence, they will tend to use the “best available” 
evidence rather than wait for the rigorous ﬁ ndings from RCTs or 
other experimental assessment designs. Some government agen-
cies have taken a pragmatic view of relevant evidence by accepting 
the importance of qualitative evidence for program evaluation, 
such as the professional judgment of practitioners and the experi-
ence of program clients (Deaton 2010; Head 2008; Pawson 2006; 
Woolcock 2009). Th e U.K. government’s central agencies have 
indicated that although scientiﬁ cally rigorous studies are highly 
desirable, all forms of systematically appraised evidence are poten-
tially valuable (U.K. Cabinet Oﬃ  ce 2008; U.K. T reasury 2007). 
Government leaders might in principle prefer rigor, but in practice, 
Policy borrowing and  diﬀ usion 
have been widely under-
taken across many ﬁ elds, but 
many adoption decisions are 
 politically driven rather than 
evidence based in design and 
implementation.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=7>>>
478 Public Administration Review  May | June 2016
policy makers as to why they avoided or ignored academic external 
research. Th ese practitioners indicated that their own agencies did 
not always place a high value on research or its communication. It 
was generally found that
• Internally conducted research or speciﬁ cally commissioned 
consultancy research was more likely to be regarded as relevant 
than academic research
• External academic research was seen as not timely or as not 
closely relevant to users’ current needs
• Research was less likely to be used when ﬁ ndings were contro-
versial (British Academy, 2008, 27)
Th ese perceptions by U.K. policy staﬀ  suggest a number of implica-
tions concerning how research is identiﬁ ed, assessed, and utilized; 
how research ﬁ ndings are ﬁ ltered for compatibility with established 
policy assumptions; and how relationships with external sources of 
expertise are managed.
Even when reliable evidence has been documented, there is often a 
poor “ﬁ t” between how specialized information has been assembled 
by researchers (e.g., scientiﬁ c reports) and the practical needs of pol-
icy and program managers (Bochel and Duncan 2007; Commission 
on Social Sciences 2003; Fazekas 2012). Researchers may not be 
adept at packaging and communicating their ﬁ ndings for policy and 
media audiences (Mead 2015; Vaughan and Buss 1998). On the 
other hand, better communication by researchers might not close 
the gap, as the potential users of research are highly diverse, and 
their receptivity to evidence-based policy ideas is beyond the control 
of the researchers. Science communication scholars now claim that 
the attention of decision makers cannot be gained simply through 
the distribution or transmission of scientiﬁ c reports (Bielak et al. 
2008). Th e emphasis has switched toward various forms of interac-
tive relations between the research sector and potential end users in 
the policy and practice arenas. Several research teams internationally 
have been working to understand more clearly how the traditional 
views of science communication and the ﬂ ow of knowledge (e.g., 
from science producers to science consumers) are seriously ﬂ awed 
(e.g., Cherney and Head 2011; Davies, Nutley, and Walter 2008; 
Harvey et al. 2010; Meagher, Lyall, and Nutley 2008; Ouimet et al. 
2009). Lomas (2000) proposed a number of interactive methods for 
fostering linkage and exchange in public health, and this approach 
has been adopted and broadened in many spheres of research/policy 
interaction (e.g., Bowen and Zwi 2005; Lavis et al. 2003; Lomas 
2007).
One of the key issues is whether purpose-built networks and 
communication channels need to be created to bridge the gap 
between the so-called three cultures of research, policy, and practice 
(Shonkoﬀ  2000). Current thinking is that a wide range of such 
arrangements would need to be institutionalized (Walter, Nutley, 
and Davies 2005). A review of studies concerned with “knowledge 
transfer and exchange” identiﬁ ed eight main methods:
• Face-to-face exchange (consultation, regular meetings) between 
decision makers and researchers
• Education sessions for decision makers
• Networks and communities of practice
• Facilitated meetings between decision makers and researchers
In a recent survey of state legislation and accountability initia-
tives, the Pew-MacArthur Results First Initiative (Pew 2015a) 
found that several U.S. state legislatures have created specialized 
oﬃ  ces to oversee research studies and evaluations of state-level 
policies and programs. Th ese evaluation studies and performance 
audits consider whether agencies are properly managing public 
programs and identify ways to improve outcomes and control costs 
(National Conference of State Legislatures 2012). For example, the 
Washington State legislature has taken a serious interest in the qual-
ity and cost-eﬀ ectiveness of publicly funded social programs, estab-
lishing evaluation regimes on special topics such as crime prevention 
and family support. Since the late 1990s, the Washington State 
Institute for Public Policy (WSIPP), an independent body based at 
the state university, has been asked to supply evidence-based policy 
reports on many topics, including juvenile and adult crime and 
corrections, school education, early childhood education, mental 
health, substance abuse, child welfare, and public health issues. 
WSIPP has developed a cost–beneﬁ t model that uses the results of 
a meta-analysis of high-quality evaluations to generate comparative 
rankings of the eﬀ ectiveness of programs in these policy areas (Lee 
et al. 2012). Th e Pew-MacArthur Results First Initiative is working 
with 19 U.S. states and four counties to replicate and customize 
the approach used in Washington State and incorporate the results 
into these jurisdictions’ policy and budget processes (Pew 2015b). 
Th is particular linkage between state decision makers and academia 
is unusually robust and could provide one model for forging closer 
relationships (Vanlandingham and Drake 2012) in jurisdictions 
that have committed to pursuing evidence-informed policy making 
in key policy domains. It is too early to assess the impact of these 
models and their capacity to be widely adopted.
Relationships, Communication, and Brokering
Public agencies have a variety of relationships with external (non-
governmental) sources of expertise. Relevant external entities 
include private corporations, university research centers, think 
tanks, not-for-proﬁ t organizations, and professional associations. 
While these linkages reﬂ ect the diﬀ erent needs of public organi-
zations (as noted earlier), there are some widely shared concerns 
about how to improve the exchange of expert knowledge between 
governmental and other organizations. Increased attention is now 
being directed toward methods to overcome the wide institutional 
“gaps” between the government sector and other sectors (including 
universities, business, and community organizations) in order to 
enhance knowledge sharing and translate research ﬁ ndings for policy 
and practice audiences (Head 2010; Newman 2011; Nutley, Walter, 
and Davies 2007). For example, some government agencies have 
contributed funds for “rapid review” consultancy services to identify 
evidence about “what works” in speciﬁ c situations. While the mod-
els vary, the essential feature is that research experts who are familiar 
with speciﬁ c topics are contracted to provide evidence-based sum-
maries at the request of government departments (e.g., Lavis et al. 
2009; Redman, Jorm, and Haines 2008; Sax Institute 2013).
Public agencies are often skeptical about relying on external sources 
of expertise and may prefer to utilize analysis generated internally by 
the agencies themselves, although studies have shown some diﬀ er-
ences in preferred sources among social, economic, regulatory, and 
technology agencies (Hall and Jennings 2010; Lester 1993; Webber 
1984). Th e British Academy (2008) reported reasons given by U.K. 
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=8>>>
Toward More “Evidence-Informed” Policy Making? 479
of information, poor management of available information, weak 
senior commitment to analytical skills, and low ability to partner 
with external groups. Th e expectations of the research community 
also need to be realistic. Weiss has demonstrated that although 
social science expertise can make useful contributions to policy 
analysis and debate over an extended period of time, the ﬁ ndings of 
a speciﬁ c report or article seldom align with the immediate needs 
of policy makers, and so the impacts of research are indirect (Weiss 
1979, 1980, 1999).
Despite these challenges, it is clear that profes-
sional standards in analysis and evaluation have 
improved substantially, with greater attention 
to clear program goals and performance indica-
tors and greater investment in data collection 
and analytical skills. Central government 
leaders have encouraged the rigorous use of 
evidence for policy and program improvement, 
although in many cases the rhetoric is more 
impressive than the practical implementation. 
Controlled trials to assess policy innovations 
have also become more widespread, and a large 
number of program evaluations are now publicly available through 
open source research centers. In social policy, strategic investment in 
key sets of social and economic data, ongoing performance monitor-
ing, and longitudinal information on key client groups are making 
a substantial diﬀ erence to the capacity of social science analysts to 
provide well-informed assessments of trends, issues, and interventions 
(Graﬀ y 2008; National Research Council 2012).
Some government agencies are making good use of this material, 
and there is some indication they are learning from each other and 
from external partners. Public agencies gather and process vast 
amounts of information, from both internal and external sources, 
but we have found surprisingly little analysis of how such informa-
tion is actually utilized (Hemsley-Brown 2004). Public profession-
als generally agree that evidence-based improvements to policy 
and administrative systems are desirable and possible. Th ey are not 
always clear about standards of evidence and what they want or 
need from external sources (Avey and Desch 2014; Cherney et al. 
2015; Head et al. 2014; Talbot and Talbot 2014). Th e institutionali-
zation of evidence-informed practices has made some progress, but 
political leaders and legislators necessarily pay as much attention to 
stakeholders and public opinion as data about program performance 
and policy options.
Th is article has shown that there are major gaps in knowledge about 
what happens inside government agencies in relation to produc-
ing, assessing, and incorporating research-based evidence into their 
policy advice, service delivery, regulatory, and program evaluation 
activities. It also suggests that progress toward a more evidence-
informed policy and administrative system would require sustained 
investment and commitment across several focus levels—individual 
leaders and managers, organizational units, and cross-organizational 
relationships. Haskins and Margolis (2014) claim that rigorous 
program evaluation can enhance cost-eﬀ ective policy development 
and therefore should be more widely adopted by public agencies and 
legislatures. Taking up this challenge would entail the adoption and 
incorporation of evaluation processes within the standard operating 
• Interactive, multidisciplinary workshops
• Capacity building within health services and health delivery 
organizations
• Web-based information, electronic communications
• Steering committees (to integrate views of local experts into 
the design, conduct, and interpretation of research) (Mitton 
et al. 2007, 744)
One promising idea is knowledge brokering, a concept that 
describes a wide range of possible methods to promote knowledge 
sharing and mutual understanding across the 
boundaries of disciplines, professional occu-
pations, and organizations (Van Kammen, 
de Savigny, and Sewankambo 2006; Ward, 
House, and Hamer 2009; Williams 2012). 
Th e approaches selected should be adapted 
for the scale of the issue, the organizational 
contexts, and stakeholder needs (Michaels 
2009). Th e knowledge-brokering concept goes 
beyond simply “telling” others about research 
(e.g., publicity about newly available summa-
ries of scientiﬁ c ﬁ ndings); rather, knowledge 
brokering seeks to add value for end users of knowledge through 
various types of dialogue and coproduction of insights in new 
contexts (Bammer, Michaux, and Sanson 2010; Landry et al. 2006; 
Meagher, Lyall, and Nutley 2008).
Strong arguments have been made for building high-level sup-
port networks and specialized bodies to foster research translation 
(e.g., Kitagaw and Lightowler 2013). For example, in the United 
Kingdom since the 1990s, new organizations and partnership 
networks have been established to address the problems of poor 
communication, lack of mutual awareness, inconsistent advice, 
and the need to embed new knowledge in organizational processes 
and procedures (Mulgan and Puttick 2013). Examples include 
the National Institute for Clinical Excellence, which focuses on 
guidelines, standards, and cost-eﬀ ectiveness evaluation (Walshe and 
Davies 2010). It was given a broader role as the National Institute 
for Health and Social Care in 2013, with a mission to contribute 
to social care innovation with Nesta and other entities dedicated to 
research translation (Alliance for Useful Evidence 2014). A further 
stream of research has canvassed how working across the boundaries 
of professional groups and organizations is crucial for good program 
outcomes, whether in social care (e.g., Gray et al. 2013; Palinkas 
and Soydan 2012; Sullivan and Skelcher 2002) or in emergency 
management (McGuire and Silvia 2010).
Conclusions and Implications for Future Research
Th e research literature demonstrates that speciﬁ c contextual 
relationships matter as much as scientiﬁ c ﬁ ndings in processes of 
evidence use and knowledge translation. Diversity in institutional 
practices helps explain why the use of evidence within policy mak-
ing and professional-managerial practice has been patchy (Landry, 
Amara, and Lamari 2001) and is likely to remain quite challeng-
ing. Institutional studies have established a range of problems and 
hindrances. In the sphere of evidence supply or production, issues 
include adequacy of research funding, clarity of priorities and 
targets, availability of analytical skills, and so on. In the sphere of 
research use or demand, issues include low trust in external sources 
It is clear that professional 
standards in analysis and evalu-
ation have improved substan-
tially, with greater attention to 
clear program goals and perfor-
mance indicators and greater 
investment in data collection 
and analytical skills.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=9>>>
480 Public Administration Review  May | June 2016
Avey, Paul C., and Michael C. Desch. 2014. What Do Policymakers Want from Us? 
Results of a Survey of Current and Former Senior National Security Decision 
Makers. International Studies Quarterly 58(2): 227–46.
Bammer, Gabriele, Annette Michaux, and Ann Sanson, eds. 2010. Bridging the 
Know-Do Gap: Knowledge Brokering to Improve Child Wellbeing. Canberra: 
Australian National University e-Press.
Banks, Gary. 2009. Evidence-Based Policy-Making: What Is It? How Do We Get It? 
Speech delivered at the Australian and New Zealand School of Government/
Australian National University Lecture Series, February 4. http://www.pc.gov.au/
news-media/speeches/cs20090204 [accessed October 7, 2015].
Baumgartner, Frank R., and Bryan D. Jones. 2015. Th e Politics of Information: 
Problem Deﬁ nition and the Course of Public Policy in America. Chicago: 
University of Chicago Press.
Behn, Robert D. 2003. Why Measure Performance? Diﬀ erent Purposes Require 
Diﬀ erent Measures. Public Administration Review 63(5): 586–606.
Benson, David, and Andrew Jordan. 2011. What Have We Learned from Policy T ransfer 
Research? Dolowitz and Marsh Revisited. Political Studies Review 9(3): 366–78.
Best, Allan, and Bev Holmes. 2010. Systems Th inking, Knowledge and Action: 
T owards Better Models and Methods. Evidence & Policy 6(2): 145–59.
Bielak, Alex, Andrew Campbell, Shealagh Pope, Karl Schaefer, and Louise 
Shaxson. 2008. From Science Communication to Knowledge Brokering. In 
Communicating Science in Social Contexts: New Models, New Practices, edited by 
Donghong Cheng et al., 201–26. Berlin: Springer.
Boaz, Annette, Lesley Grayson, Ruth Levitt, and William Solesbury. 2008. Does 
Evidence-Based Policy Work? Learning from the U.K. Experience. Evidence & 
Policy 4(2): 233–53.
Bochel, Hugh, and Sue Duncan, eds. 2007. Making Policy in Th eory and Practice.  
Bristol, UK: Policy Press.
Bogenschneider, Karen, and Th omas J. Corbett. 2010. Evidence-Based Policymaking: 
Insights from Policy-Minded Researchers and Research-Minded Policymakers. 
London: Routledge.
Boruch, Robert, and Ning Rui. 2008. From Randomized Controlled T rials to 
Evidence Grading Schemes: Current State of Evidence-Based Practice in Social 
Sciences. Journal of Evidence-Based Medicine 1(1): 41–49.
Boswell, Christina. 2008. Th e Political Functions of Expert Knowledge: Knowledge 
and Legitimation in European Union Immigration Policy. Journal of European 
Public Policy 15(4): 471–88.
———. 2009. Knowledge, Legitimation and the Politics of Risk: Th e Functions of 
Research in Public Debates on Migration. Political Studies 57(1): 165–86.
———. 2012. How Information Scarcity Inﬂ uences the Policy Agenda: Evidence 
from U.K. Immigration Policy. Governance 25(3): 367–89.
Bowen, Shelley, and Anthony B. Zwi. 2005. Pathways to “Evidence-Informed” Policy 
and Practice: A Framework for Action. PLoS Medicine 2(7)e166: 100–105.
British Academy. 2008. Punching Our Weight: Th e Humanities and Social Sciences in 
Public Policy Making. London: British Academy.
Bulmer, Martin. 1982. Th e Uses of Social Research: Social Investigation in Public Policy-
Making. London: Allen and Unwin.
———, ed. 1987. Social Science Research and Government: Comparative Essays on 
Britain and the United States. Cambridge, UK: Cambridge University Press.
Campbell, Donald T . 1969. Reforms as Experiments. American Psychologist 24(4): 
409–29.
Cherney, Adrian, and Brian W . Head. 2011. Supporting the Knowledge to Action 
Process: A Systems-Th
 inking Approach.  Evidence & Policy 7(4): 471–88.
Cherney, Adrian, Brian W . Head, Jenny Povey, Michele Ferguson, and Paul 
Boreham. 2015. Use of Academic Social Research by Public Oﬃ  cials: Exploring 
Preferences and Constraints that Impact on Research Use. Evidence & Policy 
11(2): 169–88.
Clear, T odd R. 2010. Policy and Evidence: Th e Challenge to the American Society of 
Criminology. Criminology 48(1): 1–25.
procedures of policy units, regulatory bodies, and service delivery 
organizations. However, the debate about the reliability of various 
forms of evidence remains very much alive in policy and program 
circles. As Heinrich observes,
[D]espite advances in our analytical tools and capacity for 
assembling performance information and scientiﬁ c evidence, it 
has become increasingly clear that we are still far from a consen-
sus—intellectually or politically—regarding what should count 
as evidence, how it should be produced and validated, and how 
it should be used to inﬂ uence policy making. (2007, 259)
Given the institutional diﬀ erences in organizational roles and 
resources of public agencies and diﬀ erences across policy areas, the 
future research agenda will need to be wide ranging. Among the 
priority areas for research attention, it would be helpful to encour-
age more nuanced and comparative studies on the following:
• Sources of variation in the capacity of public agencies to access 
and use expert evidence and research-based studies
• Exemplary practices wherein public oﬃ  cials and leaders ap-
preciate the contribution of rigorous research and work closely 
with researchers in setting research agendas
• Th e capacity of researchers to give priority to key issues of 
interest to policy makers and better communicate the implica-
tions of their research through improved linkages to policy 
communities
• Th e mechanisms through which the political and governmental 
systems provide support for open circulation of ideas/ informa-
tion and public investment in rigorous research programs.
Acknowledgments
Th e author gratefully acknowledges research support from 
the Swedish National Board of Health and Welfare, ARC 
grant LP100100380 on research utilization, and ARC grant 
DP140101532 on complex policy problems. Sincere thanks also 
go to the expert reviewers for PAR and to numerous colleagues who 
shaped and assisted this analysis over many years, including Sandra 
Nutley, Michael Howlett, Adrian Cherney, Haluk Soydan, Annette 
Boaz, Ross Homel, James Perry, Beryl Radin, Paul Boreham, and 
Kieran Walshe.
Note
1. For example, diﬀ  erentiation in terms of underlying incentives for action (e.g., 
the role of material, solidary, or purposive incentives; see Wilson 1973, chap. 4); 
in terms of the outputs and outcomes of agency activities (e.g., Wilson’s distinc-
tion between production agencies, procedural agencies, craft agencies and coping 
agencies; see Wilson 1989, chap. 9); in terms of their legal-structural features 
(Wettenhall 2003); or in terms of the areas, clients, processes, and public pur-
poses they serve (e.g., Peters 2010, chap. 4).
References
Aaron, Henry J. 1978. Politics and the Professors: Th e Great Society in Perspective.  
Washington, DC: Brookings Institution.
Alliance for Useful Evidence. 2014. Th e NICE Way: Lessons for Social Policy and 
Practice. http://www.alliance4usefulevidence.org/publication/the-nice-way-les-
sons-for-social-policy-and-practices-from-the-national-institute-for-health-and-
care-excellence/ [accessed October 7, 2015].
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=10>>>
Toward More “Evidence-Informed” Policy Making? 481
———. 2012. Administrators’ Perspectives on Successful Interstate Collaboration: 
Th e Drug Eﬀ ectiveness Review Project. State and Local Government Review 
44(2): 127–36.
Halligan, John. 1995. Policy Advice and the Public Service. In Governance in a 
Changing Environment, edited by B. Guy Peters and Donald J. Savoie, 138–72. 
Montreal: McGill University Press.
Hammersley, Martin. 2013. Th e Myth of Research-Based Policy and Practice. London: 
Sage Publications.
Harvey, Gill, Pauline Jas, Kieran Walshe, and Chris Skelcher. 2010. Absorptive 
Capacity: How Organizations Assimilate and Apply Knowledge to Improve 
Performance. In Connecting Knowledge and Performance in Public Services: 
From Knowing to Doing, edited by Kieran Walshe, Gill Harvey, and Pauline Jas, 
226–50. Cambridge, UK: Cambridge University Press.
Haskins, Ron, and Greg Margolis. 2014. Show Me the Evidence: Obama’s Fight for 
Rigor and Results in Social Policy. Washington, DC: Brookings Institution Press.
Head, Brian W . 2008. Th ree Lenses of Evidence-Based Policy. Australian Journal of 
Public Administration 67(1): 1–11.
———. 2010. Reconsidering Evidence-Based Policy: Key Issues and Challenges. 
Policy & Society 29(2): 77–94.
———. 2013. How Do Government Agencies Use Evidence? Research report for the 
Swedish National Board of Health and Welfare. http://www.socialstyrelsen.se/
Lists/Artikelkatalog/Attachments/19163/2013-6-38.pdf [accessed October 7, 
2015].
Head, Brian W ., and John Alford. 2015. Wicked Problems: Implications for Public 
Policy and Management. Administration & Society 47(6): 711–39.
Head, Brian W ., Michele Ferguson, Adrian Cherney, and Paul Boreham. 2014. Are 
Policy-Makers Interested in Social Research? Exploring the Sources and Uses of 
Valued Information among Public Servants in Australia. Policy & Society 33(2): 
89–101.
Heinrich, Carolyn J. 2007. Evidence-Based Policy and Performance Management: 
Challenges and Prospects in T wo Parallel Movements. American Review of Public 
Administration 37(3): 255–77.
———. 2012. How Credible Is the Evidence, and Does It Matter? An Analysis of the 
Program Assessment Rating T ool. Public Administration Review 72(1): 123–34.
Hemsley-Brown, Jane. 2004. Facilitating Research Utilisation: A Cross-Sector Review 
of Research Evidence. International Journal of Public Sector Management 17(6): 
534–52.
Howlett, Michael. 2009. Policy Analytical Capacity and Evidence-Based Policy-
Making: Lessons from Canada. Canadian Public Administration 52(2): 153–75.
Howlett, Michael, and Adam Wellstead. 2011. Policy Analysts in the Bureaucracy 
Revisited: Th e Nature of Professional Policy Work in Contemporary 
Government. Politics & Policy 39(4): 613–33.
Innvær, Simon. 2009. Th e Use of Evidence in Public Governmental Reports on 
Health Policy: An Analysis of 17 Norwegian Oﬃ  cial Reports (NOU). BMC 
Health Services Research 9: 177.
Institute for Public Policy Research (IPPR). 2011. Redesigning Justice: Reducing Crime 
through Justice Reinvestment. London: IPPR.
Jennings, Edward T ., and Jeremy L. Hall. 2012. Evidence-Based Practice and 
the Use of Information in State Agency Decision Making. Journal of Public 
Administration Research and Th eory  22(2): 245–66.
Johnson, Kelli, Lija Greenseid, Stacie T oal, Jean King, Frances Lawrenz, and Boris 
Volkov. 2009. Research on Evaluation Use: A Review of the Empirical Literature 
from 1986 to 2005. American Journal of Evaluation 30(3): 377–410.
Jones, Damon, Brian K. Bumbarger, Mark T . Greenberg, Peter Greenwood, and 
Sandee Kyler. 2008. Th e Economic Return on PCCD’s Investment in Research-
Based Programs: A Cost–Beneﬁ t Assessment of Delinquency Prevention in 
Pennsylvania. Report, Prevention Research Center, Pennsylvania State University. 
http://prevention.psu.edu/pubs/docs/PCCD_Report2.pdf [accessed October 7, 
2015].
Coalition for Evidence-Based Policy. 2015. What Works in Social Policy? Findings 
from Well-Conducted Randomized Controlled T rials. http://www.evidence-
basedprograms.org/ [accessed October 7, 2015].
Commission on the Social Determinants of Health. 2008. Closing the Gap in a 
Generation: Health Equity through Action on the Social Determinants of Health. 
Geneva: World Health Organization.
Commission on the Social Sciences. 2003. Great Expectations: Th e Social Sciences in 
Britain. London: Academy of Learned Societies for the Social Sciences.
Davies, Huw, Susan Nutley, and Peter Smith, eds. 2000. What Works? Evidence-Based 
Policy and Practice in Public Services. Bristol, UK: Policy Press.
Davies, Huw, Sandra Nutley, and Isabel Walter. 2008. Why “Knowledge T ransfer” Is 
Misconceived for Applied Social Research. Journal of Health Services Research and 
Policy 13(3): 188–90.
Davies, Philip. 2004. Is Evidence-Based Policy Possible? Jerry Lee Lecture, Campbell 
Collaboration Colloquium, Washington, DC, February 19.
Deaton, Angus. 2010. Instruments, Randomization, and Learning about 
Development. Journal of Economic Literature 48(2): 424–55.
Dery, David. 1984. Problem Deﬁ nition in Policy Analysis. Lawrence: University Press 
of Kansas.
Desmarais, Bruce A., and John A. Hird. 2014. Public Policy’s Bibliography: Th e Use 
of Research in U.S. Regulatory Impact Analyses. Regulation and Governance 8(4): 
497–510.
Donaldson, Stewart I., Christina A. Christie, and Melvin M. Mark, eds. 2009. 
What Counts as Credible Evidence in Applied Research and Evaluation Practice? 
Th ousand Oaks, CA: Sage Publications.
Edwards, Adam, and James Sheptycki. 2009. Th ird Wave Criminology: Guns, Crime 
and Social Order. Criminology and Criminal Justice 9(3): 379–97.
Ellig, Jerry, Maurice McTigue, and Henry Wray. 2012. Government Performance 
and Results: An Evaluation of GPRA’s First Decade. Boca Raton, FL: CRC Press.
Fazekas, Mihaly. 2012. Exploring the Complex Interaction between Governance and 
Knowledge: Synthesis of the Literature. Working Paper no. 67, Organisation for 
Economic Co-operation and Development. http://www.oecd-ilibrary.org/
education/exploring-the-complex-interaction-between-governance-and-
knowledge-in-education_5k9ﬂ cx2l340-en [accessed October 7, 2015].
Fischer, Frank, and Herbert Gottweis, eds. 2012. Th e Argumentative Turn 
Revisited: Public Policy as Communicative Practice. Durham, NC: Duke 
University Press.
Fox, Daniel M. 2010. Th e Convergence of Science and Governance.  Berkeley: 
University of California Press.
France, Alan, and Ross Homel, eds. 2007. Pathways and Crime Prevention. Abingdon, 
UK: Willan Publishing.
Frederickson, David G., and H. George Frederickson. 2006. Measuring the 
Performance of the Hollow State. Washington, DC: Georgetown University Press.
Glennerster, Rachel. 2012. Th e Power of Evidence: Improving the Eﬀ ectiveness 
of Government by Investing in More Rigorous Evaluation. National Institute 
Economic Review 219(1): R4–14.
Graﬀ y, Elisabeth A. 2008. Meeting the Challenges of Policy-Relevant Science: 
Bridging Th eory and Practice. Public Administration Review 68(6): 1087–1100.
Gray, Mel, Elyssa Joy, Debbie Plath, and Stephen Webb. 2013. Implementing 
Evidence-Based Practice: A Review of the Empirical Research Literature. Research 
on Social Work Practice 23(2): 157–66.
Hajer, Maarten A., and Hendrik Wagenaar, eds. 2003. Deliberative Policy Analysis: 
Understanding Governance in the Network Society. Cambridge, UK: Cambridge 
University Press.
Hall, Jeremy L., and Edward T . Jennings. 2008. Taking Chances: Evaluating Risk 
as a Guide to Better Use of Best Practices. Public Administration Review 68(4): 
695–708.
———. 2010. Assessing the Use and Weight of Information and Evidence in U.S. 
State Policy Decisions. Policy & Society 29(2): 137–47.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=11>>>
482 Public Administration Review  May | June 2016
Meltsner, Arnold J. 1976. Policy Analysts in the Bureaucracy. Berkeley: University of 
California Press.
Michaels, Sarah. 2009. Matching Knowledge Brokering Strategies to Environmental 
Policy Problems and Settings. Environmental Science and Policy 12(7): 994–1011.
Mintrom, Michael, and Rebecca Bollard. 2009. Governing Controversial Science: 
Lessons from Stem Cell Research. Policy & Society 28(4): 301–14.
Mitton, Craig, Carol Adair, Emily McKenzie, Scott Patten, and Brenda Perry. 2007. 
Knowledge T ransfer and Exchange: Review and Synthesis of the Literature. 
Milbank Quarterly 85(4): 729–68.
Monaghan, Mark. 2011. Evidence versus Politics: Exploiting Research in U.K. Drug 
Policy Making? Bristol, UK: Policy Press.
Morris, Michael, and Brittany Clark. 2013. You Want Me to Do What? Evaluators 
and the Pressure to Misrepresent Findings. American Journal of Evaluation 34(1): 
57–70.
Mosteller, Frederick, and Robert Boruch, eds. 2002. Evidence Matters: Randomized 
Trials in Education Research. Washington, DC: Brookings Institution Press.
Moynihan, Donald P ., and Alasdair S. Roberts. 2010. Th e T riumph of Loyalty over 
Competence: Th e Bush Administration and the Exhaustion of the Politicized 
Presidency. Public Administration Review 70(4): 572–81.
Mulgan, Geoﬀ . 2006. Th e Process of Social Innovation. Innovations 1(2): 145–62.
———. 2009. Th e Art of Public Strategy: Mobilizing Power and Knowledge for the 
Common Good. Oxford, UK: Oxford University Press.
Mulgan, Geoﬀ , and Ruth Puttick. 2013. Making Evidence Useful. London: Nesta. 
http://www.nesta.org.uk/library/documents/MakingEvidenceUseful.pdf 
[accessed October 7, 2015].
National Conference of State Legislatures. 2012. Ensuring the Public T rust 2012: 
Program Policy Evaluation’s Role in Serving State Legislatures. http://www.ncsl.
org/documents/nlpes/EPT2012.pdf [accessed October 7, 2015].
National Research Council. 2009. Preventing Mental, Emotional and Behavioral 
Disorders among Young People: Progress and Possibilities. Washington, DC: 
National Academies Press.
———. 2012. Using Science as Evidence in Public Policy. Washington, DC: National 
Academies Press.
Nesta. 2011. Evidence for Social Policy and Practice. London: Nesta.
Newman, Janet. 2011. Boundary T roubles: Working the Academic–Policy Interface. 
Policy and Politics 39(4): 473–84.
Niessen, Louis W ., John Bridges, Brandyn Lau, Renee Wilson, Ritu Sharma, Damian 
Walker, Kevin Frick, and Eric Bass. 2011. Assessing the Impact of Economic 
Evidence on Policymakers in Health Care—A Systematic Review. Rockville, MD: 
Agency for Healthcare Research and Quality.
Nussle, Jim, and Peter Orszag, eds. 2014. Moneyball for Government. New York: 
Disruption Books.
Nutley, Sandra M., Sarah Morton, T obias Jung, and Annette Boaz. 2010. Evidence 
and Policy in Six European Countries: Diverse Approaches and Common 
Challenges. Evidence & Policy 6(2): 131–44.
Nutley, Sandra M., Isabel Walter, and Huw Davies. 2007. Using Evidence: How 
Research Can Inform Public Services. Bristol, UK: Policy Press.
Organisation for Economic Co-operation and Development (OECD). 2009. 
Improving the Quality of Regulations: Policy Brief. http://www.oecd.org/gov/
regulatory-policy/Policy%20Brief%20-%20Improving%20the%20Quality%20
of%20Regulations.pdf [accessed October 7, 2015].
———. 2015. Government at a Glance 2015. http://www.oecd-ilibrary.org/
governance/government-at-a-glance-2015_gov_glance-2015-en 
[accessed October 7, 2015].
Orszag, Peter. 2009. Building Rigorous Evidence to Drive Policy. June 8. http://www.
whitehouse.gov/omb/blog/09/06/08/BuildingRigorousEvidencetoDrivePolicy/ 
[accessed October 7, 2015].
Osborne, Stephen P ., and Louise Brown, eds. 2013. Handbook of Innovation in Public 
Services. Cheltenham, UK: Edward Elgar.
Joyce, Philip G. 2011. Th
 e Congressional Budget Oﬃ  ce: Honest Numbers, Power, and 
Policymaking. Washington, DC: Georgetown University Press.
Kitagaw, Fumi, and Claire Lightowler. 2013. Knowledge Exchange: A Comparison 
of Policies, Strategies, and Funding Incentives in English and Scottish Higher 
Education. Research Evaluation 22(1): 1–14.
Landry, Réjean, Nabil Amara, and Moktar Lamari. 2001. Utilization of Social 
Science Research Knowledge in Canada. Research Policy 30(2): 333–49.
Landry, Réjean, Nabil Amara, Ariel Pablos-Mendes, Ramesh Shademani, and 
Irving Gold. 2006. Th e Knowledge-Value Chain: A Conceptual Framework 
for Knowledge T ranslation in Health. Bulletin of the World Health Organization 
84(8): 597–602.
Lavis, John N., Elizabeth Paulsen, Andrew Oxman, and Ray Moynihan. 2008. 
Evidence-Informed Health Policy 2: Survey of Organizations Th at Support the 
Use of Research Evidence. Implementation Science 3: 54.
Lavis, John N., Govin Permanand, Andrew Oxman, Simon Lewin, and Atle 
Fretheim. 2009. SUPPORT T ools for Evidence-Informed Health Policymaking 
STP 13. Supplement, Health Research Policy and Systems 7: S13.
Lavis, John N., Dave Robertson, Jennifer Woodside, Christopher McLeod, 
and Julia Abelson. 2003. How Can Research Organizations More Eﬀ ectively 
T ransfer Research Knowledge to Decision Makers? Milbank Quarterly 81(2): 
221–48.
Lee, Stephanie, Steve Aos, Elizabeth Drake, Annie Pennucci, Marna Miller, and 
Laurie Anderson. 2012. Return on Investment: Evidence-Based Options to Improve 
Statewide Outcomes, April 2012 Update. Olympia: Washington State Institute for 
Public Policy. http://www.wsipp.wa.gov/ReportFile/1102/Wsipp_Return-on-
Investment-Evidence-Based-Options-to-Improve-Statewide-Outcomes-April-
2012-Update_Full-Report.pdf [accessed October 7, 2015].
Lemay, Margaret, and Cresco Sá. 2014. Th e Use of Academic Research in Public 
Health Policy and Practice. Research Evaluation 23(1): 79–88.
Lester, James P . 1993. Th e Utilization of Policy Analysis by State Agency Oﬃ  cials. 
Knowledge: Creating, Diﬀ usion, Utilization 14(3): 267–90.
Lindblom, Charles E. 1979. Still Muddling, Not Yet Th rough. Public Administration 
Review 39(6): 517–26.
Lindblom, Charles E., and David K. Cohen. 1979. Usable Knowledge: Social Science 
and Social Problem Solving. New Haven, CT: Yale University Press.
Lomas, Jonathan. 2000. Using “Linkage and Exchange” to Move Research into 
Policy at a Canadian Foundation. Health Aﬀ airs 19(3): 236–40.
———. 2007. Th e In-Between World of Knowledge Brokering. British Medical 
Journal 334: 129–32.
Majone, Giandomenico. 1989. Evidence, Argument, and Persuasion in the Policy 
Process. New Haven, CT: Yale University Press.
Mandell, Marvin, and Vicki Sauter. 1984. Approaches to the Study of Information 
Utilization in Public Agencies: Problems and Pitfalls. Science Communication 
6(2): 145–64.
Maynard, Rebecca A. 2006. Presidential Address: Evidence-Based Decision Making: 
What Will It Take for the Decision Makers to Care? Journal of Policy Analysis and 
Management 25(2): 249–66.
Mayne, John, Marie-Louise Bemelmans-Videc, Joe Hudson, and Ross Conner, 
eds. 1992. Advancing Public Policy Evaluation: Learning from International 
Experiences. Amsterdam: North-Holland.
McGuire, Michael, and Chris Silvia. 2010. Th e Eﬀ ect of Problem Severity, 
Managerial and Organizational Capacity, and Agency Structure on 
Intergovernmental Collaboration: Evidence from Local Emergency 
Management. Public Administration Review 70(2): 279–88.
Mead, Lawrence M. 2015. Only Connect: Why Government Often Ignores 
Research. Policy Sciences 48(2): 257–72.
Meagher, Laura, Catherine Lyall, and Sandra Nutley. 2008. Flows of Knowledge, 
Expertise and Inﬂ uence: A Method for Assessing Policy and Practice Impacts 
from Social Science Research. Research Evaluation 17(3): 163–73.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=12>>>
Toward More “Evidence-Informed” Policy Making? 483
Shipan, Charles R., and Craig Volden. 2012. Policy Diﬀ usion: Seven Lessons for 
Scholars and Practitioners. Public Administration Review 72(6): 788–96.
Shonkoﬀ , Jack P . 2000. Science, Policy and Practice: Th ree Cultures in Search of a 
Shared Mission. Child Development 71(1): 181–87.
Shulock, Nancy. 1999. Th e Paradox of Policy Analysis: If It Is Not Used, Why Do We 
Produce So Much of It? Journal of Policy Analysis and Management 18(2): 226–44.
Solinis, Germán, and Nicolas Baya-Laﬃ  te, eds. 2011. Mapping Out the Research-
Policy Matrix. Paris: UNESCO.
Stevens, Alex. 2011. T elling Policy Stories: An Ethnographic Study of the Use of 
Evidence in Policy-Making in the U.K. Journal of Social Policy 40(2): 237–55.
Sullivan, Helen. 2011. “T ruth” Junkies: Using Evaluation in U.K. Public Policy. 
Policy & Politics 39(4): 499–512.
Sullivan, Helen, and Chris Skelcher. 2002. Working across Boundaries: Collaboration 
in Public Services. London: Palgrave Macmillan.
Talbot, Colin, and Carole Talbot. 2014. Sir Humphrey and the Professors: What 
Does Whitehall Want from Academics? Discussion paper, University of 
Manchester. http://www.policy.manchester.ac.uk/media/projects/policyman-
chester/1008_Policy@Manchester_Senior_Civil_Servants_Survey_v4(1).pdf 
[accessed October 7, 2015].
Tilley, Nick, ed. 2010. Analysis for Crime Prevention. Boulder, CO: Lynne Rienner.
Tseng, Vivian, and Sandra Nutley. 2014. Building the Infrastructure to Improve 
the Use and Usefulness of Research in Education. In Using Research Evidence in 
Education: From the Schoolhouse Door to Capitol Hill, edited by Kara Finnigan 
and Alan Daly, 163–75. New York: Springer.
T urnpenny, John, Claudio Radaelli, Andrew Jordan, and Klaus Jacob. 2009. Th e 
Policy and Politics of Policy Appraisal: Emerging T rends and New Directions. 
Journal of European Public Policy 16(4): 640–53.
U.K. Cabinet Oﬃ  ce. 1999. Professional Policy Making for the T wenty First Century. 
London: Cabinet Oﬃ  ce.
———. 2008. Th ink Research: Using Research Evidence to Inform Service 
Development for Vulnerable Groups. London: Social Exclusion Taskforce, 
Cabinet Oﬃ  ce.
U.K. T reasury. 2007. Analysis for Policy: Evidence-Based Policy in Practice. London: 
Government Social Research Unit, T reasury.
U.S. Government Accountability Oﬃ  ce (GAO). 2009. Program Evaluation: A 
Variety of Rigorous Methods Can Help Identify Eﬀ ective Interventions. 
Washington, DC: U.S. Government Printing Oﬃ  ce. GAO-10-30.
U.S. Oﬃ  ce of Management and Budget (OMB). 2010. Evaluating Programs 
for Eﬃ  cacy and Cost-Eﬃ  ciency. Memorandum for Heads of Executive 
Departments, July 29. https://www.whitehouse.gov/sites/default/ﬁ les/omb/
memoranda/2010/m10-32.pdf [accessed October 7, 2015].
Van Dooren, Wouter, Geert Bouckaert, and John Halligan. 2010. Performance 
Management in the Public Sector. Abingdon, UK: Routledge.
Van Kammen, Jessika, Don de Savigny, and Nelson Sewankambo. 2006. Using 
Knowledge-Brokering to Promote Evidence-Based Policy-Making: Th e Need for 
Support Structures. Bulletin of the World Health Organization 84(8): 608–12.
Vanlandingham, Gary R., and Elizabeth Drake. 2012. Results First: Using Evidence-
Based Policy Models in State Policymaking. Public Performance and Management 
Review 35(3): 550–63.
Vaughan, Roger D., and T erry F . Buss. 1998. Communicating Social Science Research 
to Policy Makers. Th ousand Oaks, CA: Sage Publications.
Vifell, Åsa C., and Ebba Sjögren. 2011. More Knowledge, Better Government? 
Consequences of Knowledge-Based Decision-Making in Swedish Pharmaceutical 
Beneﬁ ts. Governance 24(1): 85–110.
Wagenaar, Hendrik. 2004. “Knowing” the Rules: Administrative Work as Practice. 
Public Administration Review 64(6): 643–56.
Walshe, Kieran, and Huw Davies. 2010. Research, Inﬂ uence and Impact: 
Deconstructing the Norms of Health Services Research Commissioning. Policy 
& Society 29(2): 103–11.
Ouimet, Mathieu, Réjean Landry, Saliha Ziam, and Pierre-Olivier Bédard. 2009. Th e 
Absorption of Research Knowledge by Public Civil Servants. Evidence and Policy 
5(4): 331–50.
Palinkas, Lawrence, and Haluk Soydan. 2012. Translation and Implementation of 
Evidence-Based Practice. Oxford, UK: Oxford University Press.
Pawson, Ray. 2006. Evidence-Based Policy: A Realist Perspective. London: Sage 
Publications.
Peters, B. Guy. 2010. Th e Politics of Bureaucracy. 6th ed. New York: Routledge.
Petrosino, Anthony, Robert Boruch, Haluk Soydan, Lorna Duggan, and Julio 
Sanchez-Meca. 2001. Meeting the Challenges of Evidence-Based Policy: Th e 
Campbell Collaboration. Annals of the American Academy of Political and Social 
Science 578: 14–34.
Petrosino, Anthony, Carolyn T urpin-Petrosino, and John Buehler. 2002. “Scared 
Straight” and Other Juvenile Awareness Programs for Preventing Juvenile 
Delinquency. Cochrane Database of Systematic Reviews, no. 2: article no. 
CD002796.
Pew Charitable T rusts. 2015a. Legislating Evidence-Based Policymaking: A Look 
at State Laws that Support Data-Driven Decision-Making. Issue brief, Pew-
MacArthur Results First Initiative. http://www.pewtrusts.org/en/research-and-
analysis/issue-briefs/2015/03/legislating-evidence-based-policymaking [accessed 
October 7, 2015].
———. 2015b. Pew-MacArthur Results First Initiative. http://www.pewtrusts.
org/en/projects/pew-macarthur-results-ﬁ  rst-initiative [accessed October 7, 
2015]
Prato, T ony. 2007. Evaluating Land Use Plans under Uncertainty. Land Use Policy 
24(1): 165–74.
Puttick, Ruth. 2012. Innovations in Prevention. London: Nesta.
Radaelli, Claudio M. 1995. Th e Role of Knowledge in the Policy Process. Journal of 
European Public Policy 2(2): 159–83.
Radin, Beryl A. 2000. Beyond Machiavelli: Policy Analysis Comes of Age. Washington, 
DC: Georgetown University Press.
———. 2006. Challenging the Performance Movement: Accountability, Complexity, and 
Democratic Values. Washington, DC: Georgetown University Press.
Redman, Sally, Louisa Jorm, and Mary Haines. 2008. Increasing the Use of Research 
in Health Policy: Th e Sax Institute Model. Australasian Epidemiologist 15(3): 
15–18.
Rist, Ray C., ed. 1990. Program Evaluation and the Management of Government. New 
Brunswick, NJ: T ransaction.
Rivlin, Alice M. 1971. Systematic Th inking for Social Action.  Washington, DC: 
Brookings Institution.
Rochefort, David, and Roger Cobb, eds. 1994. Th e Politics of Problem Deﬁ nition: 
Shaping the Policy Agenda. Lawrence: University Press of Kansas.
Sabatier, Paul A., and Christopher M. Weible, eds. 2014. Th eories of the Policy Process.  
3rd ed. Boulder, CO: Westview Press.
Sanderson, Ian. 2002. Evaluation, Policy Learning and Evidence-Based Policy 
Making. Public Administration 80(1): 1–22.
Sax Institute. 2013. Evidence Check Library. https://www.saxinstitute.org.au/
category/publications/evidence-check-library/ [accessed October 18, 2015].
Schorr, Lisbeth B., and Patricia Auspos. 2003. Usable Information about What 
Works: Building a Broader and Deeper Knowledge Base. Journal of Policy 
Analysis and Management 22(4): 669–76.
Schorr, Lisbeth B., and Frank Farrow. 2011. Expanding the Evidence Universe. Paper 
presented at the Harold Richman Public Policy Symposium, Washington, DC, 
Center for the Study of Social Policy.
———. 2014. An Evidence Framework to Improve Results. Paper presented at the 
Harold Richman Public Policy Symposium, Washington, DC, Center for the 
Study of Social Policy.
Shillabeer, Anna, T erry F . Buss, and Denise M. Rousseau, eds. 2011. 
Evidence-Based 
Public Management: Practices, Issues, and Prospects. Armonk, NY: M. E. Sharpe.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License
<<<PAGE=13>>>
484 Public Administration Review  May | June 2016
———. 1999. Th e Interface between Evaluation and Public Policy. Evaluation 5(4): 
468–86.
Wettenhall, Roger. 2003. Exploring Types of Public Sector Organizations. Public 
Organization Review 3(3): 219–45.
Williams, Paul A. 2012. Collaboration in Public Policy and Practice: Perspectives on 
Boundary Spanners. Bristol, UK: Policy Press.
Williams, Walter. 1998. Honest Numbers and Democracy: Social Policy Analysis in the 
White House, Congress, and the Federal Agencies. Washington, DC: Georgetown 
University Press.
Wilson, James Q. 1973. Political Organizations. New York: Basic Books.
———. 1989. Bureaucracy: What Government Agencies Do and Why Th ey Do It. New 
York: Basic Books.
Woolcock, Michael. 2009. T oward a Plurality of Methods in Project Evaluation. 
Journal of Development Eﬀ ectiveness 1(1): 1–14.
Walter, Isabel, Sandra Nutley, and Huw Davies. 2005. What Works to Promote 
Evidence-Based Practice? A Cross-Sector Review. Evidence & Policy 1(3): 335–64.
Ward, Vicky, Allan House, and Susan Hamer. 2009. Knowledge Brokering: 
Th e Missing Link in the Evidence to Action Chain? Evidence & Policy 5(3): 
267–79.
Webber, David J. 1984. Political Conditions Motivating Legislators’ Use of Policy 
Information. Policy Studies Review 4(1): 110–18.
Weber, Edward P ., and Anne M. Khademian. 2008. Wicked Problems, Knowledge 
Challenges, and Collaborative Capacity Builders in Network Settings. Public 
Administration Review 68(2): 334–49.
Weiss, Carol H. 1979. Th e Many Meanings of Research Utilization. Public 
Administration Review 39(5): 426–31.
———. 1980. Knowledge Creep and Decision Accretion. Knowledge: Creation, 
Diﬀ usion, Creation 1(3): 381–404.
 15406210, 2016, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/puar.12475 by Test, Wiley Online Library on [17/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License