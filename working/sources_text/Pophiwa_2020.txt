<<<PAGE=1>>>
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/342110867
Using evaluations to inform policy and practice in a government department
Chapter · June 2020
DOI: 10.4324/9781003007043-5
CITATIONS
8
READS
257
4 authors, including:
Nedson Pophiwa
University of Johannesburg
39 PUBLICATIONS   96 CITATIONS   
SEE PROFILE
Carol Nuga Deliwe
Stellenbosch University
3 PUBLICATIONS   31 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Carol Nuga Deliwe on 06 December 2022.
The user has requested enhancement of the downloaded file.
<<<PAGE=2>>>
5 Using evaluations to inform 
policy and practice in a 
government department
The case of the Department of 
Basic Education in South Africa
Nedson Pophiwa, Carol Nuga Deliwe,  
Jabulani Mathe and Stephen T aylor
Summary
South Africa’s education sector has been very problematic with educa-
tional outcomes being less than desired. It has had to overcome the legacy 
of apartheid, and has struggled to create an effective educational system. The 
Department of Basic Education has been a pioneer department in the use  
of evaluations, research and data. T wo of eight evaluations undertaken by the 
department are used as mini-cases of the use of evaluations: the Funza Lushaka 
Bursary Programme and the National School Nutrition Programme. A variety 
of instrumental, conceptual and process uses of the evaluations can be seen, sup-
ported by a range of use interventions undertaken internally by the department, 
supported through the mechanisms of the national evaluation system. The cases 
provide examples of evidence-informed policy and practice and how a govern-
ment department can undertake evaluations effectively. They demonstrate the 
importance of an internal knowledge broker who is involved in the strategic 
discussions to champion and support evidence, as well as the usefulness of a 
national evaluation system providing key elements that encourage use.
Background
In 1994 the new government, led by the African National Congress, was faced 
with overcoming the legacy of discriminatory apartheid policies that deliber -
ately provided poor quality education to black people.
The quality of basic education in South Africa has been a government prior-
ity since the advent of democracy in 1994. The new Department of Education 
(DoE) began crafting policy to transform, seek redress, and enable equity and 
quality education outcomes for all South Africans. Most people have partici -
pated in or experienced the education system and have strong views on the 
system’s deficiencies and how to improve it. Tackling problems has required 
juggling popular ideas with scientific and evidence-informed approaches.
In 2010, the DoE was reorganised into two departments, the Department of Basic 
Education (DBE) and the Department of Higher Education and Training (DHET),
<<<PAGE=3>>>
76 Nedson Pophiwa et al.
to focus on the schooling and post-schooling sectors, respectively. The DBE is one 
of the stakeholders involved in establishing South Africa’s national evaluation system 
(NES), and a pioneer in using evidence for policy and decision making.
This chapter examines the DBE’s journey and looks at two mini-cases of the 
use of evaluations, the Funza Lushaka Bursary Programme (FLBP) for teachers, 
and the National School Nutrition Programme (NSNP), which were selected 
based on the importance of the programmes and the DBE’s intention to use the 
products and outcomes of the evaluations in strengthening policy support and 
implementation. The chapter sets out lessons for the use of evaluations, and the 
factors enabling or hindering it in the DBE.
The methodology was guided by an analytical framework in 
Chapter 3 that 
informed the selection of research questions. Data collection methods included 
the review of published and unpublished documents such as annual reports, 
peer-reviewed journal articles, and evaluation reports. Semi-structured inter -
views were conducted between November 2018 and March 2019 with seven 
DBE officials, both senior policy makers, focusing on those managing the pro-
grammes in the two cases chosen (NSNP and FLBP), as well as monitoring and 
evaluation (M&E) officials from DBE and the Department of Planning, Moni-
toring and Evaluation (DPME).The involvement of key players in the process 
as co-authors provided the richness of participant observation. The chapter was 
drafted by a researcher, with key historical, structural and substantive contribu-
tions from the co-authors in DBE and DPME.
Overview of the sector and its evidence journey
Country context
South Africa is a constitutional democracy with a three-tier system of govern-
ment: national, provincial and local. Education is shared between the national 
and provincial levels, with provincial having the responsibility for running the 
school system and national responsible for policy and functions such as teacher 
training and universities. There have been four education ministers since 1994, 
each bringing significant changes ( Motala, 2015). The incumbent, Minister 
Motshekga, who has been in post since 2009, had oversight of the creation of 
the DBE and DHET from the original DoE.
South Africa’s school education successes have been in providing universal 
access to educational opportunities for the majority of learners (97% participa-
tion for 7- to 15-year-olds, and 83% for 16- to 18-year olds); improving infra-
structure; equalising resource allocation; providing free education to learners 
from poor households; and expanding the nutrition programme to about nine 
million learners (DBE, 2018b).
Despite South Africa’s middle-income status and a large proportion of 
government spending on education, the major shortcoming is the quality of 
education outcomes, which can be seen in relation to our Southern African 
Development Community (SADC) neighbours. The main contributory factors 
to these deficiencies include: the lasting effects of intergenerational poverty;
<<<PAGE=4>>>
Using evaluations to inform policy 77
low levels of language and cognitive skills of learners coming into the system; 
structural and accountability weaknesses in teaching, management and school 
support at district level; and low efficiency of conversion of resources into qual-
ity in government. Attempts to address school education challenges have been 
characterised by blame and a lack of accountability (National Planning Com -
mission (NPC), 2012, p. 302).  Despite a history of poor learning outcomes and 
performance, recent regional and international assessments for learners from 
poor households have shown improvements in education outcomes and quality 
in the foundation phases of schooling, albeit off a low base (
Reddy et al., 2016).
The development of structures to use evidence in DBE
The DBE has a long tradition of using statistical evidence drawn from admin-
istrative data, official statistics on the population and special surveys. The edu-
cational planning system was developed from a need to understand the size and 
shape of the education system, and the first forays into evidence use were drawn 
from the first Schools Register of Needs, commissioned in 1996 to provide plan-
ning information on the distribution of resources and the extent of backlogs that 
the new government had to deal with. An Education Management Information 
System was created in 2001 to collect information on school-level resourcing, 
complementing information in the personnel administration system, followed 
by creation of a small Policy Support Unit to support system-wide planning, 
monitoring and evaluation and track medium- to long-range performance.
The Policy Support Unit set out to supplement the administrative data in the 
schooling system by motivating and advocating for education policy-relevant 
data in existing data collection, including those undertaken by Statistics South 
Africa. This allowed trends in provisioning of educational inputs, as captured in 
household surveys, to be analysed with provincial disaggregations from 2002 
onwards. Deeper analysis was done of education data collected in demographic 
data. In its first decade, the unit focused on generating policy-relevant analyses 
and trends using in-house data and specially commissioned surveys. Economet-
ric and other analyses of school performance were also possible using the end of 
school Grade 12
1 Senior Certificate Examination performance data and panel 
data provided by the National School Effectiveness Survey that was carried out 
over three years from 2007. This confirmed how little learning was happening 
in schools, even in the lower grades.
By 2010, the Policy Support Unit had been clustered with the unit respon-
sible for short- to medium-term planning and monitoring and renamed  
the Research Coordination, Monitoring and Evaluation (RCME) Unit in the 
Strategic Planning, Research and Coordination Chief Directorate, with the 
former policy support director as head of the Chief Directorate. The new unit 
retained the functions of the Policy Support Unit and was now also responsible 
for intergovernmental coordination, strategic planning, research coordination, 
monitoring and evaluation. Its briefings, reports and analyses on policy-relevant 
trends were adopted in policy circles. Presentations by the director general 
and senior managers to oversight bodies and stakeholders increasingly included
<<<PAGE=5>>>
78 Nedson Pophiwa et al.
reference to data and trends rather than a recital of expenditure patterns, pro-
gramme delivery and monitoring visits. Resolutions at ruling party conferences 
began to refer to this information.
By 2010, despite contestation around their use, national and international 
assessments of learning outcomes were used to identify the factors associated 
with the low levels of performance in schools. Between 2011 and 2014, Annual 
National Assessments (ANA) were implemented to measure learning outcomes 
at school level in maths and language from Grades 1 to 9. These created pres-
sure for schools to account for learning performance and indicated what was 
expected in terms of learning outcomes at each grade. However, the assess -
ments were abandoned in 2015 as burdensome, too frequent, and too focused 
on reporting and naming and shaming.
2 By 2018, despite the abrupt end of the 
ANA, long-standing participation in these assessments had illustrated progress 
in learning outcomes, albeit from a low base.
Despite resistance by unions, the tradition of tracking performance in the 
schooling system using evidence from different sources was strong and pro -
vided fertile ground for adopting an evaluative approach in policy analysis. 
By 2012, the National Development Plan (NDP) had been launched. The 
Basic Education Sector plan (developed in 2010) informed the education 
chapter of the NDP , along with a diagnostic review of the barriers to effec -
tive schooling and quality learning, and interventions designed to improve 
the quality of learning (NPC, 2011, 2012).
A textbook availability crisis in 2012 resulted in criticism of the political and 
administrative leadership of the national department and the minister responsi-
ble. The trauma of the crisis was felt in the whole system. There was heightened 
public and media scrutiny of the roles and responsibilities of provincial and 
national departments in service delivery, the monitoring systems and the data 
required to monitor progress. Information from household surveys was per -
ceived by political and union-aligned stakeholders and the public as more inde-
pendent than the education sector’s administrative data, and so more credible.
The crisis was a turning point in the schooling system and galvanised the 
national and provincial departments responsible for basic education to work 
with experts to develop and document national standards and a national system 
for improving process management, capacity and monitoring of textbook pro-
visioning, delivery and management.
Changes in the South African government-wide approaches to monitoring 
and evaluation were also critical in supporting DBE’s momentum. A National 
Evaluation Policy Framework (NEPF) was approved by Cabinet in Novem -
ber 2011, with experts from DBE as co-authors (Davids et al., 2015, p. 1; Phil-
lips et al., 2014). In its efforts to build a coalition to support the evaluation 
system, DPME established a cross-government Evaluation T echnical Working 
Group (ETWG) ‘as a sounding board and to be an advocate of the system’ 
(
Goldman et al., 2015, p. 3).  DBE was among the early adopters and members 
of this ETWG as they had already undertaken evaluations.
Following adoption of the NEPF , in 2012 the national evaluation system (NES) 
was being designed. However, the harrowing and very public events of the textbook
<<<PAGE=6>>>
Using evaluations to inform policy 79
crisis in 2012 were fresh in the minds of policy makers and the context of imple-
menting evaluations in the schooling system was politically charged. T oo critical or 
public, and unfavourable evaluation findings could be shelved and not used. T oo 
positive, and the media could dismiss the work as propaganda in a country with a 
healthy cynicism about service delivery. Faced with coordinating evaluations in the 
sector, the concerns in the DBE’s evaluation unit were not only technical.
The unit adopted an improvement support approach, providing technical 
support and closely partnering the evaluating programme managers, with sup-
port from DPME. The M&E unit communicated the utility of evaluations in 
improving programme quality, effectiveness and efficiency. Drawing from the 
events that followed the textbook crisis, this improvement narrative resonated 
with programme managers and was used during and after the evaluation in 
discussions and debates, as well as in strategic events and presentations.
A range of programmes was identified for evaluation, with all programmes 
selected receiving large amounts of funding, ranging from early childhood 
development programmes to nutrition, initial teacher education bursaries, and 
the best ways of teaching reading. Six of the eight evaluations were implemen-
tation evaluations, partly because impact could not be determined due to the 
lack of data.
The NES required a number of systems to be established including Evalu-
ation Steering Committees (ESCs), which included the custodian depart -
ment and DPME, a commitment to publishing the evaluations as a deliberate 
accountability mechanism, development of a management response and imple-
mentation of improvement planning, with reports on progress for two years 
following approval of the report.
Starting evaluations under the national evaluation system
Prior to 2011, ‘monitoring and evaluation’ activities in DBE and its precur -
sor had to a large extent been limited to monitoring and standard forms of 
reporting (
Samuels et al., 2015, p. 3). Table 5.1 provides a list of the research and 
evaluations carried out from 2011. The first evaluation under the NES was of 
Early Childhood Development (ECD), undertaken with the Departments of 
Social Development and Basic Education and Health. This evaluation, reported 
in 2012, recommended that further evaluations should be undertaken on two 
components of ECD, namely an additional reception year of schooling (Grade 
R) and on nutrition interventions for children under five. In 2012/2013, an 
impact evaluation of the introduction of Grade R was carried out by a team of 
researchers from Stellenbosch University, building on the relatively good data 
that DBE had on learning outcomes from ANAs, and administrative data on 
registrations for Grade R.
With the stopping of ANA due to contestation from the main teacher 
union, DBE no longer had good data on learning outcomes and focused 
instead on implementation or impact evaluations of large programmes. The 
data was used to generate lessons to assist programme managers to improve 
their programmes.
<<<PAGE=7>>>
80 Nedson Pophiwa et al.
Evaluation buy-in was critical and the idea of improving implementation 
through evaluation was attractive for policy and practical purposes (Respond -
ent 4). Once programme managers agreed to evaluations being conducted, it 
became easier for DBE to work with DPME to take evaluations forward.
T wo evaluations are scrutinised in some detail in this chapter, both imple -
Name T ype/purpose Year
School Monitoring Survey Survey of sector progress in achieving 2011/2012
education mandate
Independent Workbook and Formative evaluation of a sample of approved 2011/2012
T extbook Evaluation DBE workbooks and textbooks
The Impact of the Evaluation to estimate the effect of having 2012/2013
Introduction of Grade R* attended Grade R on learning outcomes 
later in primary school
The Mind the Gap Impact Randomised Control Trial to measure impact 2012/2013
Assessment of study guides on performance
The Funza Lushaka Bursary Implementation evaluation of FLBP 2014–2016
Programme* Programme
The Early Grade Reading Impact evaluation of three alternative teacher 2015–2018
Study I North West* training interventions in Setswana using a 
randomised controlled trial method
The Early Grade Reading Impact evaluation of two alternative teacher 2016–2018
Study II* training interventions in English First (2015 prep)
Additional Language using a randomised 
controlled trial method
CAPS* Implementation evaluation of the 2015–2017
Curriculum Assessment Policy Statement 
(CAPS) system
NSNP* Implementation evaluation of the National 2014–2016
School Nutrition Programme (NSNP)
Note: *Those in the National Evaluation Plan.
mented as part of the National Evaluation Plan, and so in partnership with 
DPME. The chapter also mentions other evaluations and evidence that help to 
understand the contextual, institutional and cultural enablers or barriers to use 
of evidence in the educational policy space.
The evaluation of Funza Lushaka Bursary Programme (FLBP)
The FLBP was established in 2007 and provides full-cost bursaries to high-
achieving students to undertake initial teacher education (ITE) programmes to 
become teachers in priority subjects such as maths, physical science and account-
ing and in foundation phase learning, and locations such as rural areas. It is a 
large-scale programme that reached 23,392 students during the period under 
evaluation (2007–2012), on average 15% of the total ITE enrolment over the 
period (DPME/DBE, 2016b). Owing to the importance of the programme and 
the need to motivate for its continued support by government, the head of the  
T able 5.1 List of DBE’s research and e valuations to date
<<<PAGE=8>>>
Using evaluations to inform policy 81
unit responsible for planning, monitoring, evaluation and research managed to 
convince key officials responsible for it to use the evaluation as an opportunity 
to improve implementation.
It was decided that the evaluation would be an implementation evaluation 
rather than an impact evaluation. The evaluation was procured by DPME and 
JET Educational Services was contracted to conduct it.
The overall findings were that the FLBP is performing well and is broadly 
effective (and cost-effective) in attracting high-achieving students who complete 
ITE programmes in good time and take up government-paid positions in public 
schools (DPME/DBE, 2016b). However, the evaluation identified inefficiencies 
in implementation with regard to supply and placement of educators at different 
levels of government. The final report, management response, and improvement 
plan to address the findings were approved by Cabinet in March 2017.
The improvement plan has been taken forward. ‘The programme manager 
agreed with the recommendations and so he was eager to see those things put 
in place. In some of the other programmes it was more of a burden to be seen 
to be implementing the improvement plan’ (Respondent 1).
There are different ways of understanding evidence use, and here we con -
sider instrumental, conceptual, symbolic and process use as used by Johnson 
et al. (2009) and Patton (1998). This differentiation was discussed in Chapter 1.
In terms of use of the findings and recommendations, a key area was rethink-
ing the selection criteria to target specific areas of teacher specialisation. Prior 
to the evaluation, students were simply told ‘if you want to be a teacher, we will 
give you a bursary’ (Respondent 2). The evaluation report recommended that 
DBE, with universities, should develop an effective system to monitor the pri-
ority areas that students have enrolled for and that subject areas should be fixed 
between application and selection (DPME/DBE, 2016
a, p. 36).  Since then, they 
have produced a set of guidelines and criteria for selection of students based 
on geographic and subject area and phases required by the FLBP policy, and 
become stricter as to who is selected as a beneficiary (instrumental use).
The ITE Directorate was able to use recommendations related to monitor -
ing, tracking and data management to motivate for funding to modernise the 
information management system and successfully approached government for 
funding an online system that has been in use since October 2018 and has been 
an important building block to effectively managing information concerning 
the programme (Respondent 2) (instrumental use).
The evaluation recommended that the DBE, in collaboration with provin -
cial education departments (PEDs), needs to strengthen methods for effective 
placement of graduating students. The ITE directorate has begun reporting not 
on administration of placement but on the utilisation of graduates in terms of 
where they are placed upon completion (Respondent 2) (instrumental use).
The process of undertaking the evaluation was very important in itself (pro-
cess use) and led to considerable learning. For example, the theory of change 
workshop brought together officials from higher education institutions, the 
National Student Financial Aid Scheme (NSFAS), civil society groups, and 
provincial and national department officials to gain an understanding of key
<<<PAGE=9>>>
82 Nedson Pophiwa et al.
components of the bursary programme (conceptual use). This was an eye-
opener for Respondent 2 because it was the first time he saw stakeholders 
with a common interest in implementation of the bursary programme come 
together to deliberate constructively on it.
Another example was the building of successful relationships. In Octo -
ber 2018, DBE held an indaba to open a dialogue on teacher professionalisa -
tion, teacher standards and school-based initial teacher education models. The 
success of this event was attributed to the collaborative nature of the Funza 
Lushaka evaluation: ‘it wouldn’t have happened smoothly if we hadn’t collabo-
rated in the Funza evaluation’ (Respondent 2). Relationships that were estab-
lished during the evaluation, especially the theory of change workshop, are said 
to have been a critical enabler for this event (DBE, 2018a) (process use).
There are examples of unintended use. Parliament became more interested in 
understanding how FLBP graduates are placed in specific targeted areas rather 
than the logistical, administrative data concerning how placement was managed 
(Respondent 2). The Directorate responsible for FLBP negotiated a partnership 
framework with roles and responsibilities/activities at all levels of government. 
The Directorate also used the findings on placement to obtain independent 
external advice on how the unit could improve its own efficiency, for example, 
a contract on how quickly graduates should be placed.
The evaluation of the National School Nutrition Programme
In a country with high levels of poverty and inequality such as South Africa 
where many children go to school without breakfast, the NSNP aims to 
improve the health and nutritional status of the poorest learners. It was initially 
a Primary School Nutrition Programme (PSNP) administered by the Depart -
ment of Health, which provided learners at primary schools with at least one 
meal per day. In 2002 it was decided that the programme should be migrated 
from the Department of Health to the Department of Education and expanded 
to cover beyond Grade 7.
Following a 2006 survey, the need to expand the programme to secondary 
schools was confirmed. Quintile 5 are the best-off public schools and quintile 
1 the poorest. The renamed National School Nutrition Programme was first 
implemented in quintile 1 secondary schools in April 2009, and was phased in 
to quintile 2 and 3 public secondary schools in April  2010 and 2011, respec-
tively (NSNP Annual Report, 2009). NSNP has involved a large financial com-
mitment from government (ZAR 5.3  billion by 2014) and reaches over nine 
million learners. Apart from feeding children at school, NSNP includes cam -
paigns raising awareness on healthy eating and lifestyles among learners.
In thinking about the type of evaluation to assess the NSNP , DBE/DPME 
commissioned a scoping study in 2012 that revealed insufficient data for the 
impact to be assessed. The Steering Committee of the evaluation decided to 
shelve the idea of an impact evaluation. However, in 2014 Cabinet requested the 
DBE to undertake an evaluation of the NSNP . It was agreed to resuscitate the 
evaluation as an implementation evaluation, achievable with the information
<<<PAGE=10>>>
Using evaluations to inform policy 83
at hand. The evaluation was commissioned by the DPME and DBE and was 
conducted by JET Education Services.
The main purpose of the evaluation was to assess whether the NSNP is 
being implemented in a way that is likely to result in significant health and 
educational benefits to primary school learners and establish how to improve 
programme effectiveness. The evaluation report was approved in October 2016.
In the management response, the DBE agreed with 80% of the recommen-
dations and indicated that some are already being implemented (DPME, 2017, 
p. 19).  An improvement plan was developed and the report and improvement 
plan were approved by Cabinet.
The programme managers have continued to implement the improvement plan 
and report on progress. The NSNP evaluation was used instrumentally in effect-
ing changes directly to the roll-out of the school nutrition programme. There also 
appears to be good buy-in from the PEDs to implement the recommendations, 
which ‘is evidence that the evaluation study has strong potential to shape and influ-
ence implementation of the NSNP in the near future’ (DPME, 2017
, p. 19).
The recommendations included introducing individual targeting in certain 
provinces/schools in which not all learners eat the NSNP meals regularly, and 
income and poverty levels are mixed. Task teams have been set up with their de 
facto terms of reference the NSNP evaluation recommendations relevant to the 
theme of the task team (Personal communication, Ms K. Maroba, Department 
of Basic Education, 8 October 2019). One task team, set up to determine the 
targeting criteria to be used in addressing learner opt-outs, recommended that 
there should be set criteria for targeting meals provision according to learner 
needs as long as it is affordable (Respondent 4) (instrumental use).
A recommendation was that the NSNP guidelines should specify who the 
meals are intended for, how leftover meals and stock should be dealt with, 
with monitoring of implementation. The guidelines indicate that if the meals 
are intended to encourage social cohesion and be eaten together by learners, 
volunteer food handlers, teaching and administrative staff, the guidelines should 
indicate this and concomitant funding be made available. Within the improve-
ment plan, DBE committed to revising its guidelines on meals and develop -
ing stock control and plans to manage learner food preferences, leftovers and 
wasted food. For example, the DBE had been required to make a submission 
to National Treasury for approval of soya from predetermined manufacturers 
through the centralised procurement system. In response, the DBE evaluated 
the quality of soya mince and developed a list of compliant manufacturers, 
which was then approved and circulated to PEDs at the end of 2017 to guide 
procurement decisions (instrumental use).
In addition, the Department set up a menu task team to consider alternatives 
to soya as a protein in meals, in consultation with nutrition experts. Finally, the 
DPME’s Quality Assessment Report emphasised that the evaluation process 
deepened stakeholders’ understanding of the NSNP activities, opportunities for 
better implementation, and utility (DPME, 2017
, p. 19) (conceptual use).
The evaluation provided the DBE with a robust understanding of suc -
cesses, barriers and inefficiencies in implementing the programme, and an
<<<PAGE=11>>>
84 Nedson Pophiwa et al.
overview of the perceptions, concerns and successes in its implementation. This 
information confirmed and strengthened the policy makers’ hand in putting 
forward a plan of action for the NSNP long after the improvement planning 
and reporting process had expired.
Some conclusions on use
In both evaluations we see considerable levels of use, including the different 
types we are focusing on. In one of the planning workshops that shaped the 
focus of this chapter, one comment that stood out was that DBE is among the 
few departments to have ‘institutionalised the use of evidence’ generated from 
research and evaluations.
Use interventions undertaken and the change mechanisms
Key for this book is understanding how evidence use happened and the inter-
ventions undertaken to promote use. Table 5.2 summarises and elaborates on 
some of these, including those undertaken through DBE systems, and features 
of the NES that assisted in ensuring use.
Overall we can see that the Chief Directorate responsible for Planning, Research, 
Monitoring and Evaluation played a key role in championing the use of evaluations 
and in knowledge brokering with programme managers, senior management of DBE, 
DPME and the evaluation service provider. This complemented the technical work 
done by the Monitoring and Evaluation Unit. In general the work produced by 
the research and evaluation directorate has been taken more seriously over the years. 
There is a recognisable shift in the attitude of senior management, which acknowl-
edges the importance of the evidence they are generating and using.
Different forums were important in widening awareness and ownership of 
the evaluation.
Some of the meetings with senior people.  . . .  when I pr esent they often 
rush, they have a massive agenda. They try to finish within a day. When 
you sit to present they say please try to summarise in five minutes. But 
recently they have been asking for more, I had lots of time. I was presenting 
in parliament yesterday. There is more of an interest in that kind of work 
on improving, what the research is saying. That’s quite encouraging to see.
(Respondent 1)
Ideally, organisations undertake evaluations at critical stages in the life cycle of 
interventions when important decisions need to be taken. Timing of the evalu-
ation is therefore critical to facilitate use, while delays in finalising evaluations 
may prove to be a challenge as findings may be too late to incorporate findings 
during policy and programme reviews. Notwithstanding some delays, most rec-
ommendations from the two evaluations were still relevant for immediate use at 
the completion of the evaluation processes.
Three-quarters of all evaluation recommendations in basic education since 
2013 have focused on promoting better internal operations rather than additional
<<<PAGE=12>>>
Using evaluations to inform policy 85
T able 5.2 Use interventions and how these influenced use
Intervention Effect and change mechanism activated
DBE systems
Knowledge brokering role The unit ‘marketed’ itself to programme managers to 
of Strategic Planning, help them see the value of improving implementation 
Research and Coordination through evaluations, and identifying possible topics. 
Chief Directorate It undertook internal communication to inform 
management and minister of the findings and 
recommendations
Working with programme managers helped to build 
awareness in the Department of evaluations and 
findings, trust in the credibility of findings, and to 
ensure the institutionalisation of mechanisms to respond 
to the evaluation
Unit having technically strong This allowed DBE to play a strong role in the technical 
members side of the evaluation, and increased the credibility 
and legitimacy of it within DBE, and so trust in the 
findings
Presenting and showcasing The evaluations were presented at the Council of 
evaluation findings in Education ministers, HeadCom of technical heads 
different forums of education departments in provinces with national 
government, various interprovincial subcommittees, 
e.g. on teacher development; curriculum; planning 
and M&E. This helped to build trust in the evaluation 
results
Elements of NES
T echnical Working Group TWG and ESC enabled co-development of all stages 
and Evaluation Steering of the evaluation from formulation to finalisation. 
Committee This facilitated agreement, ownership and trust between 
DPME and DBE and conviction in the usefulness of 
evaluation results
Developing theory of change Helped to build common understanding of how  
with stakeholders the programme worked, valuable in itself, and  
interest by stakeholders in being part of  
the process
Validation workship with This made stakeholders aware of the findings and then 
stakeholders recommendations were developed in an interactive 
manner with them. This allowed stakeholders an 
opportunity to reflect on the recommendations and 
thereby agree and own them, and trust the results
Simple evaluation report Improved accessibility helped with advocacy and 
dissemination of findings
Management response The management response provided a formal mechanism 
whereby different departments had to acknowledge 
the recommendations and indicate those they 
agreed/disagreed with and why. It provided a way of 
institutionalising them
(Continued)
<<<PAGE=13>>>
86 Nedson Pophiwa et al.
resources. In general, the evaluations completed in DBE, far from being compli-
ance exercises, have been used for operational improvement and policy review.
The contextual factors supporting or hindering the use of evidence
Factors enabling use
Table 5.1 shows the increase in research and evaluations commissioned by the 
department in recent years, which reflects the political will to support independ-
ent evaluation, information and data. The minister has over the years demon-
strated an appreciation for the need to use evidence in planning and policy 
making. The length of her tenure has ensured stability in the leadership of DBE 
as the macro-departmental focus has remained the same. This has allowed suf-
ficient time for the department to implement policy changes over time, unlike 
during the frequent changes in minister prior to 2009. Another critical aspect 
that was mentioned in the interviews was that it is not only political will at the 
level of the minister or Director General (DG) that matters but also the backing 
of project managers in the DBE. Project and programme managers who buy in 
to evaluations and use of evidence can commit resources and energy towards 
the realisation of activities outlined by the evaluation recommendations. They 
can even lobby the department to shift its approach towards a specific aspect as 
a result of lessons they have learned from an evaluation.
Crises related to delivery of educational services, such as the 2012 textbook 
crisis, provided the impulse for the department to be prepared to consider 
changes. This crisis threw into relief the need for integrated information for 
monitoring and evaluation in the sector.
Intervention Effect and change mechanism activated
Improvement plan This was developed for both evaluations and 
implemented closely with FLBP and NSNP . It also 
provided a formal mechanism for agreeing how to take 
forward and institutionalise recommendations
Quality assessment Both evaluations were checked by the DPME through 
assessment of government evaluation reports to ensure 
credibility and trust in evaluation findings
Report public on DPME Once approved by Cabinet the reports were made 
website available to the wider public on DBE and DPME 
websites. This helped in giving stakeholders access to 
the information, and awareness of the results
Approval by Cabinet The Cabinet process was effective in getting people to 
take the evaluation results seriously and in generating 
momentum for follow-up actions. It also promoted 
agreement by Cabinet and ownership of the results
Role of DPME evaluation Provided technical assistance, guidance and logistical 
director support for processes involved in evaluations, and a 
bridge to reporting to Cabinet
T able 5.2
 (Continued)
<<<PAGE=14>>>
Using evaluations to inform policy 87
The Chief Directorate: Strategic Planning, Research and Coordination, 
which includes the Research and Evaluations Directorate, have been champi-
ons for evidence generation and use in DBE. They have played an important 
role in helping their peers who manage programmes and projects to appreciate 
the value of lessons from evaluations. They have access to strategic discussions 
in DBE and also provide a technically strong partner for DPME to work with 
on the evaluations, and they have played the knowledge broker role in DBE to 
maximise the likelihood of use.
It’s a bit fragile in that it’s still a bit dependant on personalities. . . .  There are 
two or three very competent staff members in the directorate who have a 
good understanding of evaluations. I guess with them that’s a move slightly 
towards institutionalisation (as) those types of people would hopefully be 
retained in the Department, and may move to management over time. . . . 
part of our story has been the champions  . . .  without having to be pres -
surised into it or upskilled by DPME.  . . .  So it was fertile ground for the 
DPME to come and work in. But also maybe some of the juniors who 
are there now may become more senior in time. That’s maybe a move in a 
direction of institutionalisation.
(Respondent 1)
As champions, they have been able to convince programme managers not 
to worry about negative evaluation findings but rather use the recommen-
dations and lessons as to how they could strengthen the programme.
(Respondent 2)
Officials in the DBE also valued the facilitation of the DPME in driving and lead-
ing the evaluation system and providing technical advice as well as the presence 
of the NES, which emphasises evaluation quality and use.
Barriers to use
Outside these two specific evaluations, impediments have arisen in the application of  
improvement plans where proposed activities have negative political implica -
tions, are inconsistent with the law, too expensive to implement, impractical, 
lack management support or require policy amendment and therefore are not 
enforceable. T o ensure implementation, the improvement plans also need to 
be better linked to operational plans because that is how individuals are held 
to account (Respondent 1). The incorporation of improvement plan activities 
in the department’s annual performance and operational plans is important to 
ensure that they are budgeted for and therefore implemented.
Mohohlwane (2018) explains potential hesitation by managers in undertaking 
evaluations, associated with uncertainty of the value of evaluations, concerns 
about underperformance and repercussions. She gives an example:
A programme manager may be held accoun table for the programme 
being evaluated, however, they may not have control of all the underlying
<<<PAGE=15>>>
88 Nedson Pophiwa et al.
processes due to complexities in the structure, resourcing and the scale 
of programmes. These complexities include concurrent functions between 
national departments as well as the national and provincial education 
departments; funding that is received directly from National Treasury or 
Provincial Treasury to nine different Provincial Education Departments 
but accounted for nationally; and the number of schools in a programme.
Writing about the Grade R evaluation, 
Samuels et al. (2015) argue that one 
should not be naïve about the incentives facing government when conduct -
ing evaluations, because the results can point to significant problems and low 
impacts.
In an environment where the media are likely to pick up on this and create 
negative press for the implementing department, this creates an incentive 
for government officials to resent an evaluation rather than embrace it so 
as to learn from it.
Samuels et al. (2015, p. 9)
These observations suggest that DPME will need to find ways to assist partner-
ing departments in communicating findings to the public and in ensuring that 
the process is constructive. With the NSNP , the report was leaked and the main 
television station wanted to do a feature on it.
Lessons for the country going forward
How did the context and intervention influence the use  
of evidence in DBE?
The realist analysis discussed in Chapter 1 suggests that in different contexts, 
particular interventions will result in varying outcomes. In the context of evi-
dence use, interventions include an evidence generation process (e.g. an evalua-
tion), use interventions adopted to try and promote use (e.g. evaluation steering 
committee), which influence certain behavioural mechanisms (access to infor-
mation, building trust, etc.) and result in certain use outcomes (how evidence 
influences policy and practice). Understanding which mechanisms work within 
which context can help us understand conditions that increase the likelihood 
of research utilisation and therefore place us better to reproduce these.
The context in this case included the need for significant reforms and out -
comes in education after huge financial resources had been committed and 
there was a need to demonstrate effectiveness. Multiple stakeholders at national 
and provincial levels are involved in implementation of educational pro -
grammes but with some confusion in roles. There is a history of using evi -
dence in education, and there was a significant evidence champion, the Chief 
Directorate: Strategic Planning, Research and Coordination with a history of 
using evidence to support policy making. This was supported by a National
<<<PAGE=16>>>
Using evaluations to inform policy 89
Evaluation System giving recognition to evaluation, and systems and support 
for implementation.
In terms of evidence generation, the department had internal capacity to gener-
ate evidence and used a variety of sources, including evaluations, research and 
administrative data.
We see a number of use interventions being applied, including knowledge broker-
ing by the evidence champion Chief Directorate. This helped to identify areas 
to evaluate, and to maximise ownership of the findings and recommendations. 
The systems and technical support under the NES, with the focus on collabora -
tion, contributed to ownership and learning through the process. The quality 
assurance systems were important in building the credibility of the evaluations, 
even where, as in NSNP , there was no budget to do as extensive a survey as the 
DBE would have liked. The improvement plan was an important step in trying 
to ensure use.
We see examples of these leading to building of awareness, agreement/owner-
ship and trust in the findings and recommendations, and the institutionalisation of 
recommendations, which all help lead to individual, organisation and systems 
change. However, some managers remained sceptical and more institutionalisa-
tion of improvement plans in operational plans and departmental annual per -
formance plans and budgets was needed.
We see a range of outcomes being achieved. In terms of individual change, 
in both processes we see stakeholders becoming committed to change, build -
ing their motivation to use the results of the evaluation, and use evaluation 
more generally. Organisationally, we see a developing capability in DBE with 
the Research, M&E Directorate having significant expertise in evaluation and 
strongly motivated to use evaluation, and a range of organisational changes 
directly emanating from the two evaluations.
The main lessons that emerge
Some of the lessons that emerge are:
• Having the same leadership for a relatively long period of time provides 
stability, which allows time for evidence to be generated and used to drive 
change.
• Crises can provide an opportunity for use of evidence  – and developing 
an evidence base can provide the ability to respond quickly with evidence 
when need arises.
• With the increase in research and evaluations commissioned by the depart-
ment, continuing political will to support independent evaluation, informa-
tion and data is critical.
• Having an internal unit as evidence champions. In DBE they worked hard 
to promote appreciation of evidence, and to act as knowledge brokers 
linking evidence generation and use by policy makers and programme 
managers.
<<<PAGE=17>>>
90 Nedson Pophiwa et al.
• Evidence w as sometimes viewed negatively by programme managers and 
advocacy is required. The role of an internal champion is key in addressing this.
• The role of a national evaluation system and a national champion to drive, 
lead it and provide technical advice is necessary.
• The importance of an appr oach that supports involvement of stakeholders 
through the process, so that they own the product and process, for exam-
ple, developing the theory of change with stakeholders, or recommenda -
tions developed in an iterative manner in a broader stakeholder validation 
workshop.
• The importance of per ceived legitimacy of the messengers. In one evaluation, 
the choice of service provider was considered problematic as a renowned 
critic of government programmes was appointed in the competitive bid -
ding process, possibly compromising the legitimacy of the results and 
findings.
Conclusions
The cases presented in this chapter are evaluations where the evidence and 
recommendations from the evaluations were used. Although there were res -
ervations about evaluation initially, the knowledge broker role of the Chief 
Directorate was important in leading the use of evaluations. The constant need 
to reinforce the utility of the evaluations was a stumbling block but was over-
come by consistently communicating the benefits. Both evaluations eventually 
strengthened the hand of the programme managers in reviewing and strength-
ening policy implementation, despite low levels of understanding about the 
need for evaluation by programme managers and initial reservations about the 
evaluations being public.
Overall, this chapter provides a picture of how a government department can 
undertake evaluations effectively, and the importance of an internal knowledge 
broker to champion and support this. It also shows the usefulness of a national 
evaluation system providing key elements that encourage use. 
Notes
 1 Final year of schooling.
 2 For a history of the assessments, see Thulare (2018) and Nuga-Deliwe (2017).
References
Davids, M., Samuels, M-L., September, R., Moeng, T.L., Richter, L., Mabogoane, T.W ., et al. 
2015. The pilot evaluation for the National Evaluation System in South Africa – A diag-
nostic review of early childhood development. African Evaluation Journal, 3(1), Art. #141, 
7 pages. http://dx.doi.org/10.4102/aej.v3i1.141
DBE. 2009. National School Nutrition Programme (NSNP) 2009/10. Annual Report. 
Department of Basic Education.
DBE. 2010. Curriculum news. Department of Basic Education. Pretoria.
<<<PAGE=18>>>
Using evaluations to inform policy 91
DBE. 2018a, October. T eacher indaba evaluation report. Research Coordination, Monitor-
ing & Evaluation Directorate. Department of Basic Education.
DBE. 2018b. General Household Survey (GHS): Focus on Schooling 2017. Department of 
Basic Education.
DPME. 2014. Evaluation guideline 2.2.6. How to develop an improvement plan to address 
evaluation recommendations. Department of Planning, Monitoring and Evaluation.
DPME. 2017. Report on the assessment of government evaluations – Implementation e valuation of 
the national school nutrition programme. Department of Planning, Monitoring and Evaluation.
DPME/DBE. 2016a. Implementation evaluation of the national school nutrition pro -
gramme: Evaluation report. Department of Planning, Monitoring and Evaluation, and 
Department of Basic Education.
DPME/DBE. 2016b. Implementation evaluation of the Funza Lushaka Bursary Programme: 
Evaluation report. Department of Planning, Monitoring and Evaluation, and Department 
of Basic Education.
Goldman, I., Mathe, J.E., Jacob, C., Hercules, A., Amisi, M., Buthelezi, T., et al. 2015. Devel-
oping South Africa’s national evaluation policy and system: First lessons learned. African 
Evaluation Journal, 3(1), 107, 9 pages. http://dx.doi. org/10.4102/aej.v3i1.107
Johnson, K., Greenseid, L.O., T oal, S.A., King, J.A., Lawrenz, F . and Volkov, B. 2009. Research 
on evaluation use: A review of the empirical literature from 1986 to 2005. American Journal 
of Evaluation, 30, 377–410. https://doi.org/10.1177/1098214009341660
Mohohlwane, N. 2018. Implementing evaluations: Successes and challenges from a DBE 
perspective, available at: https://www.zenexfoundation.org.za/programme/thought-
leadership/m-e/item/363-implementing-evaluations-successes-and-challenges-from-a-
dbe-perspective
, accessed 15 March 2019
Motala, S. 2015. Equity, access and quality in basic education: A review. Journal of Education, 61.
National Planning Commission (NPC). 2012. National development plan 2030: Our future  – 
make it work. Pretoria: Presidency of South Africa.
National Planning Commission, Diagnostic Overview. 2011. Retrieved 20 September 2018, 
from www.nationalplanningcommission.org.za/Downloads/diagnosticoverview.pdf
Nuga Deliwe, C.O. 2017. An analysis of the measurement of the progress in learning out-
comes at country level: the case of South Africa. University of Wits, South Africa http://
wiredspace.wits.ac.za/handle/10539/25944
OECD. 2008. Reviews of national policies for education: South Africa. Reviews of National Pol-
icies for Education, OECD Publishing, Paris, https://doi.org/10.1787/9789264053526-en.
Patton, M.Q. 1998. Discovering process use. Evaluation, 4, 225–233. https://doi.org/10.  
1177/13563899822208437
Phillips, S., Goldman, I., Gasa, N., Akhalwaya, I. and Leon, B. 2014. A focus on M&E of 
results: An example from the Presidency, South Africa. Journal of Development Effectiveness, 
6, 392–406. https://doi.org/10.1080/19439342.2014.966453
Reddy, V ., Visser, M., Winnaar, L., Arends, F ., Juan, A. and Prinsloo, C.H. 2016. TIMSS 2015: 
Highlights of mathematics and science achievement of grade 9 South African learners . Pretoria: 
Human Sciences Research Council.
Samuels, M., Taylor, S., Shepherd, D., Van der Berg, S., Jacob, C., Deliwe, C.N., et al. 2015. 
Reflecting on an impact evaluation of the Grade R programme: Method, results and 
policy responses. African Evaluation Journal, 3b(1), Art. #139, 10 pages. http://dx.doi.
org/10.4102/aej.v3i1.139
Thulare, T.D. 2018. A policy analysis of the annual national assessments in South Africa. 
In Wiseman, A.W . and Davidson, P .M. (eds.), Cross-nationally comparative, evidence-based 
educational policymaking and reform  Emerald Publishing Limited, pp. 71–100. https://doi.
org/10.1108/S1479-367920180000035004 
View publication stats