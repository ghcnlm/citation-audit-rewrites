<<<PAGE=1>>>
Twende Mbele: Using M&E to improve performance and accountability of African governments.  
Hosted by CLEAR-AA at the University of Witwatersrand, Johannesburg, South Africa 
www.twendembele.org  / @TwendeMnE 
 
DIAGNOSTIC ON THE SUPPLY AND DEMAND OF EVALUATORS IN UGANDA 
 
 
DRAFT REPORT 
 
 
Dr Aggrey Niringiye 
 
 
April, 2018
<<<PAGE=2>>>
Page 2 
 
 
ACRONYMS 
AfrEA    African Evaluation Association 
AG    Auditor General 
APR    Annual Performance Report 
CIDA    Canadian International Development Agency 
CSO    Civil Society Organisation 
DAC    Development Assistance Committee 
EES    European Evaluation Society  
GoU    Government of Uganda (GoU) 
IDEAS    International Development Evaluation Association 
IOCE    International Organisation for Cooperation in Evaluation 
IPDET    International Program for Development Evaluation Training 
MDA     Ministries, Departments and Agencies 
M&E    Monitoring and Evaluation  
MEMS    Monitoring and Evaluation Management Services 
MISR    Makarere Institute for Social Research 
MSI    Management Systems International- 
NGO    Non-Governmental Organisation  
NIMES    National Integrated Monitoring and Evaluation Strategy 
NMETWG   National Monitoring and Evaluation Technical Working Group 
OAG    Office of the Auditor General 
OECD    Organisation for Economic Cooperation and Development 
OPM    Office of the Prime Minister 
PAF    Poverty Action Fund 
PD    Paris Declaration on Aid Effectiveness 
UEA    Ugandan Evaluation Association 
UMI    Uganda Management Institute
<<<PAGE=3>>>
Page 3 
USAID     United States Agency for International Development 
VFM    Value for money 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
EXECUTIVE SUMMARY
<<<PAGE=4>>>
Page 4 
 
This diagnostic study aimed at establishing the capacity and supply of evaluators in Uganda and what is required to 
strengthen this capacity and supply. On the supply side, the study established that there is a substantial potential pool 
of M&E consultants in Uganda. The study also established that there are   a number of universities, training 
institutions and national, regional and i nternational networks for evaluation capacity building in Uganda. 
Results from the survey of individual evaluators show that the majority of the evaluators self -taught(50%). It 
was also established that about half of the donor -led evaluations were conducte d by international evaluators 
only, while approximately the other half was conducted by mixed teams, i.e. international and national 
evaluators. The pricingof skills varies according to experience, education, or whether one is a local or 
international consultant. Results from the survey of local individual evaluators show that the average charge -
out rate per day was US$200. 
 
On the demand side,  the Government has in place guidelines for the public sector and UEA has developed and 
approved standard guidelin es that stipulate that evaluators and evaluation teams must be independent, 
trustworthy and transparently selected. The value of the current country demand by government as per the 
National Policy on Public Sector Monitoring and Evaluation (2013), is approximately 28% of the projects that are 
valued at over 70 billion shillings. All projects are supposed to allocate a percentage of not more than 4% of 
their budgets to evaluation, as determined by the Development Committee,. Survey results showed that the 
average cost of an evaluation in most CSOs is Ug Shs 30 million. The study also established the specific professional 
skills required of evaluators by both Government and CSOs include  minimum academic qualifications, 
experience, core skills and other qualities or attributes. The only source of information about available evaluators is 
the Uganda Evaluation Association (UEA) and the USAID funded Uganda Monitoring and Evaluation Management 
Services (MEMS) project that developed a roster of Ugandan institutions , firms and individual consultants for 
Monitoring and Evaluation Services . Findings from the survey of clients show that the majority of the 
commissioners of evaluations (71%) have a database of evaluation providers.  
The study also established a number of  opportunities for local evaluation capacity building in Uganda and in 
other communities of practice such as CSOs. The opportunities are in the form of training programs in 
monitoring and evaluation at the universities, training institutes and professional networks. Survey findings from 
most clients show that the supply of evaluators is not sufficient to meet the demand. To better match the supply
<<<PAGE=5>>>
Page 5 
to demand , it was established that there is need for a deeper analysis of the profession of evaluation in Uganda more 
especially organizational capacity gaps at MDAs level in Uganda. It was also established that local capacities for 
conducting high-quality evaluation are quite limited. Results from the survey of individual evaluators show that 
majority of the eval uators (43%) have experience in carrying out evaluations of between 1 and 3 years . The 
demand and supply evaluators continue to be driven by development partners with limited country ownership 
of the processes. 
To strengthen the evaluation capacity and supply of evaluators there is need to strengthen the   culture of management that 
understands values and uses evaluative information to achieve results and organizational performance. The 
NIMES Secretariat and the National Monitoring and Evaluation Technical Working Group (NMETWG) should 
ensure systematic dissemination of evaluation reports and sharing of good practices. There is need to create a 
Centre of Excellency in evaluation that would establish  an efficient Uganda information systems on evaluation 
findings and evaluator availability and opportunities . There is also need to  identify champions of evaluation to 
promote evaluations. There is also a need to expand the existing pool of evaluators, and enable emerging 
evaluators to enter the market with fewer restrictions. Persons engaged in designing, conducting and managing 
evaluation activities should possess core evaluation competencies which should be maintained through a 
regular programme of continuing professional development. In addition, there is need  to develop and maintain 
ongoing professional development for evaluation by GoU. There is also need for Government to interface with 
and support the development of professional evaluation associations such as the Ugandan Evaluation 
Association. There is also need to define very well the key competencies for evaluation that should form part of 
the functional organization of personnel requirements.
<<<PAGE=6>>>
Page 6 
 
 
TABLE OF CONTENTS 
Acronyms          i 
Executive Summary         ii 
1. Introduction         1 
1.1 Background         1 
1.2 Purpose of the Study        2 
1.3 Research Questions        3 
1.4 Methodology         3 
1.5 Structure of the Report        4 
2 Country Context         4 
3. A Root Cause Analysis         7 
3.1 Current Approach to Building the Evaluation Market in Uganda   7 
3.2 Supply Side Profile         8 
3.3Demand Side Profile        16 
3.4 Matching the Supply to Demand       18 
3.5 Efficiency of Uganda Information System     19 
3.6 Opportunities for Transformation and Uganda Specific Empowerment 20 
3.7 Obstacles to Evaluation Market Entry      20 
3.8 Citizens Based Monitoring and Empowerment     21 
4.Conclusion and Recommendations       22 
4.1 Conclusion         22 
4.2 Recommendations        24 
 
References         27 
 
 
1.Introduction
<<<PAGE=7>>>
Page 7 
1.1 Background 
Twende Mbele (TM) is a partnership programme between Benin, Uganda and South Africa, using a  
Peer-learning approach to build stronger national evaluation systems to improve governance and accountability to 
citizens. As more and more evaluations are being commissioned and undertaken in a number of African countries, 
it is clear that the current supply of good evaluators is constrained, with relatively small group of professional 
suppliers taking up most of the larger evaluation studies. In many instances, evaluations are done by foreign 
evaluators from the western Countries. As more Governments demand for more evaluative evidence, the 
number and quality of human resources required to meet this will grow. At the current level of demand, problems 
with the size of the pool of available evaluators, the quality of their deliverables and their ability to respond to 
Government terms of reference (ToRs), consolidates an already skewed evaluation market.  
 
There is, therefore, a need to expand  in terms of numbers and quality of the existing pool of evaluators, and to 
ensure that the representivity of evaluators in Uganda approximates that of the local population, as well as enabling 
emerging evaluators to enter the market with fewer restrictions which includes experience, qualifications, etc. Uganda 
will benefit from further understanding of the incentives and barriers that foster a more competitive demand and 
supply in the evaluation market.  
 
Within the framework provided by Twende Mbele, country-specific trends as well as cross-country comparisons are 
possible, with strong references to African and regional issues also possible. In political-economic terms, besides gender 
relations that are being fore grounded in DFID criteria, there is a growing recognition of the importance of broader 
poverty and social inequality as major determinants of current assessments of the economic winners and losers in the 
global social order. These critical considerations need to be factored into the thematic assessment of evaluator demand and 
supply, specifically issues of transformation, obstacles to evaluation market entry, and even more challenging, notion 
empowerment.  
 
A study was done in Uganda in June 2009 by Ian C. Davies and found out that demand for evaluative information in, and of 
the Government of Uganda, and resulting use, falls broadly into two categories; monitoring information for budget 
allocation and control purposes and; evaluation information to meet the accountability requirements. A key finding of the 
study was that there were significant gaps, and variances among MDAs, in organizational capacity for evaluation of GOU.
<<<PAGE=8>>>
Page 8 
 
The general approach considered involved Uganda-specific M&E system as foundation to provide the necessary system and 
context. System and country context provides the basis for study support and eventual ownership of study output. This 
study undertakes a diagnosti c on the  suppl y a nd de mand of evaluators in U ganda with the aim of providing a Uganda -specific 
demand-and-supply profile of evaluators. 
 
1.2 Purpose of the Diagnostic Study 
The diagnostic of supply and demand of evaluators sought to provide a country-specific demand-and-supply profile of 
evaluators in each of the three partner countries in the Twende Mbele project: Benin, Uganda and South Africa. The study 
answered questions around the capacity and supply of evaluators in Uganda, and what is required to strengthen this 
capacity and supply. 
 
On the supply side, current capacity was identified through: (i) size of current country supply of evaluation consultants 
(including government, academic, donors, business and civil society), (ii) skill levels, abilities and specializations, (iii) shape 
of the current pools of skills (age, gender, race and ethnicity, geography, language, organization or individual, etc.), (iv) 
pricing (relative cost) of skills and value-for-money considerations, (v) access to evaluators (ability and/or reach of 
national systems to procure required skills), (vi) other capacity considerations.  
 
On the demand side, considerations included (i) national government system requirements for eligibility to provide 
evaluation services, and other market entry determinants, (ii) value of current country demand (including 
government, academic, donors, business and civil society), (iii) specific professional skills required from evaluators, (iv) 
efficiency of country information systems on evaluator availability (supply), and opportunities (demand), (v) 
opportunities/possibilities for transformation and country- specific empowerment   
 
and, (vi) sustainability of the market.  
The outcomes of this diagnostic were used to design activities to improve the supply and quality of evaluations (and 
evaluators) in Uganda. These activities were designed during the national and regional workshops, and were 
supported by Uganda governments, UEA, academia and other relevant organizations. The report of this diagnostic is 
meant to provide the basis for planning specific TM programme interventions and activities, and to inform and enrich other
<<<PAGE=9>>>
Page 9 
planned activities.  
 
1.3 Research Questions 
The main research question that was answered through this study was: What do we know of the capacity and supply of 
evaluators on the continent, and what is required to strengthen this capacity and supply?  
 
Additional questions were:  
 
1. What is the size of the evaluation market (demand from donors, government, private sector)  
in each of three countries - and in response, how big is the supply (pool of skills available)?  
2. How do we better match the supply to demand (especially if we are trying to increase  
, demand)?  
3. To what extent has the current approach to building an evaluation market  
constrained/enabled that market in each country? 
4. What are the current patterns of evaluation implementation using external service providers  
versus the use of officials/public servants in government?  
 
1.4 Methodology 
The methodology incorporated an extensive review of documentation relevant to evaluation in Uganda. The 
collation of and analysis of secondary data covered policy, academic and grey literature relating to the demand and supply 
side of evaluation. Primary and secondary documentation was considered in order to substantiate the claims of the 
interviews and to expand the information base.   
 
Often desk study alone leaves many quest ions unanswered and much of the information obtained not verified. 
To understand the context and in addition to obtaining primary data from stakeholders, we carried out in—
depth interviews with a sample of Ministries, Departments and Agencies (MDAs) in the  GoU, of Parliamentary 
officers, committees and elected officials, of training and research institutions, of donors and of civil society 
organizations (CSOs)(see Annex1). In particular, issues of actual and latent demand and of evaluation capacities were 
explored through the interview process. Data collection took place in a semi-structured way that allowed people to narrate
<<<PAGE=10>>>
Page 10 
their story – with some probing taking place based upon the guiding supply and demand questions. The data from 
interviews was analyzed qualitatively basing on the emerging relevant themes and sub themes during the fieldwork, 
with emerging conclusions refined and subjected to validation during the last set of interviews.  
 
1.5 Structure of the Report 
Chapter two presents country context. C hapter three presents the root cause analysis and chapter four 
presents the conclusion and recommendations. 
 
2. Country Context 
This section presents an overview of Uganda's planning and budgeting process and how these are linked to 
monitoring and evaluati on framework in the country. In Uganda, over the past two decades, considerable 
efforts have been made to establish a strong and robust basis for assessing public spending, and its effects on 
development. In achieving this, Public Sector M&E was considered  as a means of Government measuring its 
development interventions. M&E was therefore enshrined in the National Development Plan and 
institutionalized in the governance systems and processes (National Development Plan, 2010/11- 2014/15).  
The Office of the Prime Minister (OPM) was given the constitutional mandate to oversee reforms and service 
delivery in all Government Ministries, Departments and Agencies and established an M&E function to support 
this role (National M&E Policy, 2013).It was set up to desig n, commission, conduct and disseminate evaluations 
on public policies and major public investments, as directed by the Uganda government, and to oversee 
improvements in the quality and utility of evaluations conducted across the government at a decentralis ed 
level.OPM works closely with evaluation networks, national and international partners and evaluation expert 
institutions to promote the use of evidence in policy and programme formulation and in decision-making. 
A National Strategy for Monitoring and Evaluation of Government programmes (NIMES) was  
developed with the aim of enhancing M&E capacity as well as ensuring that sound evidence 
-based data and information are available to inform decision -making (The Republic of Uganda, National 
Integrated Monitor ing and Evaluation Strategy, 2006).The National Policy for Public Sector Monitoring and 
Evaluation  was developed and approved in 2013 to provide a clear framework for strengthening the coverage, 
timeliness of assessment of public interventions. The policy  was meant to enhance the performance of Public 
Sector through strengthening coordination and cost -effective production and use of objective information in
<<<PAGE=11>>>
Page 11 
the implementation of national interventions. The policy enabled government, civil society organizat ions, 
development partners and corporations access credible evidence to inform policy and programmatic decisions, 
and hold the public sector accountable for its application of resources (National M&E Policy, 2013). 
 
The implementation of the National Polic y for Public Sector Monitoring and Evaluation saw government 
introduce a series of reforms to enhance accountability and transparency of the Ministries, Departments and 
Agencies (MDAs) and Local Governments (LGs). Significant effort went into introducing p lanning, results-based 
budgets, monitoring systems and developing the institutional capacity to design ministry strategy and plans to 
implement M&E arrangements to monitor results and provide a basis for performance improvement as 
provided for in the National Development Plan (Annual Performance Assessment Report, 2013/2014). 
 
In Uganda, the Office of the Prime Minister collates information from other departments and produce bi-annual 
and annual sector performance reports. There is a system of annual Cabinet retreats to review the performance 
of the government. The Prime Minister, ministers and top public servants attend the retreat. The retreats 
review reports and may issue recommendations to inform budgeting processes. In this way, there are 
mechanisms to institutionalize monitoring to feed into executive decision-making processes. For Parliament, the 
Office of the Auditor General has an established evaluative practice as it carries value —for-money audits on a 
regular and systematic basis since 2005. There is an evaluation practice in Civil Society Organizations (CSOs) 
although the practice appears under -resourced, ad -hoc and driven primarily by donor requirements and 
support. This shows a high level demand for M&E evidence. However, monitoring dominates the  M&E systems 
in all cases. 
In Uganda, there is a two -year rolling evaluation agenda, mainly donor funded and overseen by an M&E 
technical working group. The Government Evaluation Facility (GEF) is run by a  secretariat in the Office of the 
Prime Minister, which provides technical support for evaluations and the evaluation system. Evaluations are 
more focused on implementation and impact or summative forms of evaluations. The Department of M&E/UEA 
in Uganda is endeavoring to set standards across government f or evaluation and attempting to invoke demand 
for evaluation by introducing a range of tools to increase commitment by Cabinet, the Prime Minister and 
Sector Departments. The specific tools being applied to support this include mechanisms such as Departmen ts
<<<PAGE=12>>>
Page 12 
proposing evaluations, development of a national evaluation agenda or plan, and making the reports publicly 
available. 
Demand for evaluative information in, and of, the GoU, and resulting use, falls broadly into two categories: 
Monitoring information fo r budget allocation and control purposes and, evaluation information to meet the 
accountability requirements of donors. A majority of the evaluations are donor initiated, funded, commissioned, 
managed, conducted and used primarily to meet donor accountability requirements. 
In Uganda, the key challenges for implementing evaluation include invoking incentive for use and demand for 
use from politicians, and developing adapted endogenous system that can draw on in-country quality evaluation 
capacity. While some  limited capacity does exist for evaluation in the various public sector institutions in the 
country in terms of personnel and systems, the quality of practice is at variance with the standard norms that 
are agreed upon and used internationally. In addition, demand for evaluation comes primarily from donors who, 
in most cases, initiate, commission and manage evaluations with variable participation of GoU staff and/or of 
national evaluators. Although there is a robust supply of evaluations and production of evaluation reports 
generally in Uganda, one of the constraints for use by the GoU and the public sector in general is the lack of 
coordination and dissemination by donors of evaluation reports they commission. There is little in the way of 
organized social accountability mechanisms that could generate demand as well for evaluation. Although there 
is general demand for accountability information from Parliament and the public in general, with the former, 
mechanisms for implementing response and scrutiny requ ire strengthening and linking to clear instructions to 
actors that should respond. With the latter, mechanisms for social accountability are insufficiently developed 
and organized to exert effective and focused demand for evaluation and accountability information in general. 
There is however, institutionalized demand for evaluation at the national level as part of the National 
Development Plan (NDP) processes. As well, there is demand from Parliament, the Public Accounts Committee 
and the public in general for accountability information on GoU spending. This demand is addressed through 
the value for money audits of the Office of the Auditor General (OAG). The Office of the Auditor General does 
respond to the accountability demand of Parliament by conducting value for money audits on a systematic and 
regular basis.  
 
3. A Root Cause Analysis
<<<PAGE=13>>>
Page 13 
 
3.1 Current Approach to Building the Evaluation Market in Uganda 
Monitoringis the dominant part of Uganda Government monitoring and evaluation (M&E) system, indicating 
that there is weak demand from decision-makers for evidence. In addition, the supply of M&E in Uganda has to 
a large extent been influenced by donor demands that have stimulated the development of M&E practice, in 
face of limited the absence of national government demand. Even the available evaluators have been trained in 
a donor -orientated milieu, due to the strength of demand from donors and the limited government M&E 
system. The donor -driven orientation of M&E practice has been recognized by the African E valuation 
Association (AfrEA, 2007) and within the Paris Declaration on Aid Effectiveness (OECD, 2005). In Uganda there 
are multiple donor reporting systems at project level, and there is need to reduce, harmonize and minimize 
duplication of monitoring functions.  
 
In Uganda policy relating to the Monitoring and Evaluationis supported by the constitution thus enabling the 
evaluation market. As a result, the MDAs  have the institutional support to develop their organizational capacity 
for evaluation. International organizations like CLEAR, 3ie and donors (ie, GIZUSAID, World Bank,UNDP,etc) are 
actively supporting development of MDAs organizational capacity for evaluation. This is expected to reduce 
donor dominance, both in terms of concepts and instruments, h elp to reinforce in -country capacity to develop 
M&E systems, and build local confidence. There is evidence of emerging endogenous demand from Uganda for 
evidence. This demand is sometimes being filled by Uganda -led monitoring systems, and development of 
evaluations that supply deeper analysis. The Office of the Prime Minister goes beyond coordination, to 
information generation through evaluation.  
3.2 Uganda Supply Side Profile 
3.2.1  Size of the Current Supply of Evaluators  
The size of the evaluation market (demand from donors, government, private sector) in Uganda is considerable in terms of 
number of consultants and companies, for evaluation outside government. There is a substantial pool of M&E 
consultants in Uganda. The USAID funded Uganda Monitoring a nd Evaluation Management Services (MEMS) 
project developed a roster of Ugandan institutions, firms and individual consultants for Monitoring and 
Evaluation Services, in 2006. The roster identified 18 Ugandan based firms, 11 institutions and 75 individuals
<<<PAGE=14>>>
Page 14 
that provide M&E services. These have either served their respective clients satisfactorily on their consultancies 
or been assessed as having 1) experience, 2) qualifications, and 3) credibility, as in quality of product produced. 
The roster was updated in  2012 updated, under the management of the new contractor, the Mitchell Group. 
The updated roster has 158 Ugandan experts in evaluation. The MEMS project also conducted a series of 
training sessions in performance management to 302 people that have contrib uted immensely to the capacity 
of Ugandan evaluation staff to conduct their M&E activities more effectively.  
3ie and the Government of Uganda facilitated logistics for the impact evaluation technical training course 
provided by the Center for Learning and  Results (CLEAR) in Uganda in 2013. The course was attended by the 
staff from various  Government agencies as well as policymakers and researchers from other countries in sub -
Saharan Africa. 3ie has provided bursaries for OPM representatives to attend seve ral courses and conferences 
related to impact evaluation and rigorous evidence including for the seventh international AfrEA meeting in 
2014 and the Campbell Collaboration Colloquium in 2013. GIZ/GoU provided capacity building for the public 
sector, civil society, private sector in evaluation contributing to a pool of evaluators in 2012 to 2015. 
 
3.2.2  Evaluation Capacity Biulding Initiatives  
There are a number of opportunities for local evaluation capacity building in Uganda and in other communities 
of practice such as CSOs in the form of training programs in monitoring and evaluation at the universities and 
training institutes. Currently, the most well developed (and popular) training courses in M&E are provided by 
the Uganda Management Institute (UMI), UTAMI and the Makerere Institute of Social Research (MISR), but 
M&E is in most cases part of Project Planning & Management course curricula at Ugandan universities across 
the country. As well there exist donor supported training events in Uganda as well as  regional and international 
resources for evaluation. 
UMI offers a two -week course and post graduate diploma on project monitoring and evaluation. In developing 
the curriculum for the postgraduate programme, UMI consulted a few government agencies, donors and CSOs, 
which emphasized that the course should be relevant to the M&E needs within government, be based on 
programme theory rather than project thinking, and also address M&E in thematic areas such as humanitarian 
assistance.  
MISR is usually contracted  by the OPM to conduct M&E training for members of the National M&E Technical 
Working Group. The training module is an adaptation of the International Program for Development Evaluation
<<<PAGE=15>>>
Page 15 
Training (IPDET) training curriculum. M&E capacity development initiatives that have taken place were not always 
adequately coordinated (BTC Uganda, 2012), and furthermore were concentrated mainly at central line ministry 
level, largely foregoing investment in M&E (and more broadly education management) capacity at district level. In 
fact, district level M&E is often considered to function merely as an outpost for central -level data collection and 
does not necessarily address local level implementation realities. 
 
As part of the initiative to build local M&E capacity USAID pr ovides financial Support to its local implementing 
partners (these include local governments) to attend training courses on M&E. The USAID supported the 
Uganda Monitoring and Evaluation Management Services (MEMS) project, during the management of 
Management Systems International (MSI), offered training in evaluation to USAID partner organizations (Save 
the Children, Africare, TASO, AIC, IRC, CRD, and others). In the second phase of MEMS, under the management 
of the Mitchell Group, USAID requested MEMS to fo cus on training for improved monitoring and reporting of 
results. The Ugandan Evaluation Association (UEA) has existed since 2002, has the potential to provide ongoing 
professional networking and development if provided with support. The association can be  a forum where 
evaluation professionals meet and share information and good practices on evaluation in addition to organizing 
short courses on topics of interest to its members. Once it is nurtured the UEA has the potential to contribute to 
professionalizing home grown evaluation capacity. 
At the regional level, the African Evaluation Association (AfrEA) is an association of M&E associations and 
networks in Africa. Their website1 contains information on both regional and international graduate degrees and 
diplomas, short courses and workshops on evaluation. AfrEA conferences, usually preceded by professional 
development workshops, are another forum for capacity building. The conferences are also an opportunity for 
sharing knowledge and experience on evaluati on, from which good practices and lessons can be identified, just 
as networks can be built. The AfrEA website also contains resource materials on M&E that can be used to 
professionalize evaluation, i.e. guidelines, standards and methodologies on evaluation , web links to evaluation 
journals, and links to other useful websites. 
At the international level, the IPDET evaluation training programme in Canada targets officers occupying senior 
and middle level evaluation, audit and management positions in developed and developing economies and who 
                                                           
1www.afrea.org
<<<PAGE=16>>>
Page 16 
work in government, NGOs and the private sector. As number of Ugandan professionals have already benefited 
from this training program. From 2001 -2007 about 25 CSO representatives from Uganda have taken the IPDET 2 
course.  
Associations such as the International Organization for Cooperation in Evaluation (IOCE), the European 
Evaluation Society (EES), and the International Development Evaluation Association (IDEAS) also provide 
opportunities for the strengthening of institutio ns’ and individuals’ evaluation capacity, through conferences 
where the sharing of knowledge on evaluation theory and practices around the world is facilitated. Their 
websites3 contain extensive electronic resources as well as information on training progr ams that offer 
certificate or graduate qualifications in evaluation. 
3.2.3  Skill levels, Abilities and Specializations 
The general view of persons in GoU, donor representatives as well as people in the consulting environment  is 
that local Ugandan M&E capacity is still generally weak. Regarding capacity for evaluation in the CSOs the study 
observed that generally M&E staffs in CSOs have only taken a 3 -week course in M&E, and local consultants 
offering evaluation services are not necessarily better trained or more experienced. Results from the survey of 
individual evaluators show that 50% of the evaluators have not completed any specific courses in M&E (self -
taught). Survey results showed that individual evaluators had  either completed a short course in eva luation or 
completed an M&E module as part of a degree. Capacity development in most CSOs is ad hoc.  Results from the 
survey of individual evaluators show that majority of the evaluators (43%) have experience in carrying out 
evaluations  of between 1 and 3 years implying that evaluators in Uganda have limited experience. 
 
In the MDAs there are no examples of written and institutionalized guidelines or standards on how to 
commission and manage evaluations. Skills, experience and know -how rest mainly with in dividuals and are not 
yet systematically institutionalized. There are also no systematic approaches to the building of capacity. 
Capacity building is rarely budgeted for and therefore ad hoc, based on individual initiatives. 
Most of the staff in M&E units of MDAs have no certificate or diploma in M&E (Office of the Prime Minister, 2012) 
but have gained significant experience and on -the-job-training in M&E, however, they lack analytical capacities 
                                                           
2International Program for Development Evaluation Training – www.ipdet.org 
3www.internationalevaiuation.com  - www.europeanevaluation.org - www.ideas-int.org
<<<PAGE=17>>>
Page 17 
which hamper the development of the evaluative component. There is lack of evaluative capacity at decentralized 
level because of limited investment in M&E capacity development. In fact, district level M&E is often considered 
to function merely as an outpost for central -level data collection and does not necessarily address local level 
implementation realities. Districts send quarterly work plans (for approval) and reports to the MDAs, yet hardly 
receive any feedback with respect to the data that was locally collected and channeled upwards (interviewees). 
Quality of data is barely controlled and is not analyzed locally for use in local-level decision-making which in itself 
discourages local level data collection. 
 
There are no systematic approaches to ensuring that there is knowledge spec iﬁc to evaluation in the various 
MDAs, including where there is an M&E unit. Findings from survey of individual evaluators showed that 63 % of 
the evaluators specialize in a specific sector or sectors, for instance, agriculture, health, entrepreneurship, 
education, financial literacy among other sectors.  
The Ministry of Public Service, does not have a detailed job description that specify competencies in evaluation, 
i.e. knowledge, skills and abilities (KSA) for the positions of Monitoring and Evaluation O fficer. The job 
description for an Assistant Commissioner for Monitoring and Evaluation in a ministry does not contain 
minimum requirements for level of knowledge, skills and abilities for evaluation. The job description lists key 
areas of responsibility and outputs but makes no distinction between monitoring and evaluation. 
This does not mean that individuals with responsibilities for monitoring and evaluation in MDAs do not have the 
abilities to carry out their work appropriately. On -the-job learning and training, together with access to 
documentation and opportunities for professional development outside of government, constitute ways in 
which evaluation capability is expanded. While institutional capacity for evaluation remains weak however, the 
risk is that evaluation practice, and its quality, can be affected by staff turnover. A 2009 World Bank document 
considers Uganda’s capacity to monitor education indicators such as enrolment rates, number of teachers, 
infrastructure and instructional material to be relatively strong (World Bank, 2009). 
 
Policy and Planning units and M&E units across government are generally understaffed and in some cases go for 
long -periods without staff trained in evaluation or having access to professional development in M&E. Th ere 
are a few cases in which a unit with M&E responsibilities had no staff with skills and competencies to
<<<PAGE=18>>>
Page 18 
commission and manage evaluations. Local supply in many instances is more configured for monitoring of 
policies, and even more of programmes and projects, for example annual progress reports.  
Survey findings showed that most clients had sent, or are planning to send, their staff involved in managing 
evaluations on training courses. The training courses include: certificate /diploma  in Monitoring  and  
Evaluation, project planning and management, basic research skills, research designs frameworks and quality 
control.  
 
3.2.4 The Shape of the Current Pools of Skills  
 
There is limited information on the shape of the current pool of skills in terms of age  , gender, language , 
organization or individuals. Findings from survey of individual evaluators showed that 67 % of the evaluators 
were men, implying that evaluations in Uganda are dominated by men. What is also known is that with a few 
exceptions evaluation teams are led by foreign consultants from the western countries although there is, in 
most cases, participation and support to varying degrees of national evaluators. The majority of the evaluations 
identified by the study from 2005 to 2016 have been i nitiated, commissioned and managed by donors; with 
USAID accounting for about half of these (the study used  web based searches).  
About half of the donor -led evaluations were conducted by international evaluators only (companies or 
individuals), while  th e other half was conducted by mixed teams, i.e. international and national evaluators. 
Findings from survey of individual evaluators showed that most evaluators carry out evaluations for clients, as 
individual suppliers.  Of the evaluations commissioned and /or co -managed by GoU, no sector is overly 
represented and there is a fair balance between teams that are made up either of all international evaluators, 
all national evaluators and mixed teams. The evaluations in CSOs are most often conducted by external local 
consultants, except for larger evaluations, which are typically conducted by international consultants. It is also 
clear that the current supply of good evaluators is constrained, with relatively small group of professional suppliers 
taking up most of the larger evaluation studies. 
3.2.5 Pricing of Skills  
 
The pricing  of skills varies according to experience and education. The pricing also depends on whether an 
evaluator is local or international with the later commanding high pay because of the ext ra costs such as
<<<PAGE=19>>>
Page 19 
accommodation and airfare. Results from the survey of local individual evaluators show that the average 
charge-out rate per day was US$200 and ranged from US$50 -US$300. The low average charge-out rate per day 
may either be a reflection of limited skills or low remuneration for the evaluation service in Uganda. 
3.2.6 Access to Evaluators  
In Uganda, there is a disconnect between evaluation supply and demand as there is little evidence of substantive 
relationships between government and evaluation agents, except in some limited areas. However this challenge is being 
addressed through Uganda Evaluation Association (UEA), which acts as an interface between supply and demand for 
evaluations. As a body of various evaluation practitioners, who are convinced of the importance of evaluations, and who 
have a direct professional interest in evaluations, UEA is able to advocate and draw attention of some organizations on the 
need  to undertake evaluation of their initiatives. In this way, UEA contributes to activating latent demand for evaluations, 
hosting important events with the purpose of nurturing evaluation demand. 
 
3.2.7 Capacity for Evaluation Uganda  
There exists non -state capacity for evaluation in Uganda in civil society organizations, in research an d training 
institutions and with consultants and consulting firms.  Such technically good evaluation actors offer entry points to 
evaluation capacity development efforts. However the development of that capacity is not supported systematically 
by the State or by donors. The various agents who are related to the supply evaluations may be grouped into four (4) 
categories: (1) consultants; (2) universities; (3) research institutions and think tanks; and (4) voluntary organizations of 
professional evaluators (VOPEs). 
Consultants: There is a growing number of individual consultants and consultancy organizations that have been undertaking 
evaluations or related work on projects, programmes and policies. Among the organizations that can be mentioned are the 
Centre for Democratic Development, Innovation for Poverty Actions (IPA), KPMG, Ernst & Young and GIMPA Consultancy 
Unit. The financial incentives associated with evaluation are the major driving force for these consultancy organizations to 
seek opportunities to undertake evaluations or related work. 
 
Universities: Out of over 30 universities in Uganda, Makerere is the only one that made it to the top 50 in Africa 
in 2017 in research ranking.   Gulu University came at 79, MUBS at 122, Mbarara University of Science an d 
Technology came at number 134.  Generally, the public universities are more endowed with research capabilities than 
the private ones.  Meanwhile, Mountains of the moon was ranked 173rd, Uganda Christian University came at
<<<PAGE=20>>>
Page 20 
179th, Kampala International Univ ersity was 250th, Kyambogo ranked 330th, Busitema came 357th, Uganda 
Marty’s 377th and IUIU was in the 378 position.  The Universities are endowed with highly qualified teaching and 
research staff thereby presenting great opportunities to conduct various evaluations. Faculty members undertake research 
work in the various Schools and Faculties.  
 
All the universities in Uganda have a social science capacity (sociologists, economists, political scientists) which could be 
mobilized for evaluation work linked to research. Potential exists to build the capacities of these institutions to 
bridge the evaluation supply -demand gaps. As can be expected an increase in demand for evaluation has the 
potential for driving supply for evaluation. Some development partners ha ve collaborated with evaluation 
agents (research institutions and universities) on in -country evaluation capacity development initiatives. A case 
in point is the partnership between GIZ and UTAMI to start the first ever Master of Arts programme in 
Evaluation Uganda.   
 
Think Tanks: Economic Policy Research Centre(EPRC) is an economic policy research centre that undertakes policy analysis, 
evidence-based advocacy and advice to Uganda government to enable her formulate and implement good policies and 
strengthen public institutions towards accelerated development. The relative strength of EPRC appears to relate to its ability 
to work legitimately within the political economy and therefore provides an entry point for evaluation capacity 
development. EPRC is fully funded by the government and therefore has potential for shifting the latent demand to actual 
demand and developing evaluation capacity. 
 
Organizations and   Networks: T he Uganda Evaluation Association (UEA)was formed in May 2001 and 
registered in 2002 a s a professional association and national chapter of the African Evaluation Association. The 
main objective was to create a national network to facilitate sharing of literature methods, procedure and 
practical evaluation frameworks among evaluators who were operating in isolation, build capacity for evaluation 
and promote professionalism in evaluation practice. The Association started with a membership of over fifty 
individuals drawn from monitoring and evaluation units in Government, Parastatals, local and  international 
NGOs, private organizations and members from the public that are engaged in evaluation practice. The UEA is 
supported by USAID and World Bank.
<<<PAGE=21>>>
Page 21 
The UEA developed and endorsed in 2013 for the first time the Uganda Evaluation Standards that pro vide 
guidance on how evaluation  professionals and users should behave, what concepts and practices 
evaluators should use, the benchmarks their products should meet, and the outcomes they  should achieve. The 
standards are in conformity  with the African Eval uation Association (Afrea) Guidelines and the good practices 
endorsed by the International Organization for Cooperation in Evaluation (IOCE). 
3.2 Uganda Demand Side Profile 
3.3.1 National Government System Requirements for Eligibility  
The Government has devel oped  guidelines for public sector  evaluations and  the UEA has developed and 
approved standard guidelines that stipulate that evaluators and evaluation teams must be independent, 
trustworthy and transparently selected. They must have appropriate knowledg e, skills mix, and proven 
competencies in evaluation methodology and specialist area(s). The evaluators should have professional work 
experience relevant to the evaluation. The evaluation team should be gender sensitive, where applicable. 4 This 
diagnostic study did not find examples of written guidelines on how to conduct, commission or manage 
evaluations; rather ToR are developed on a case-by-case basis, in collaboration with the particular donor. 
 Results from the survey of clients who commission evaluati ons  show that  most of the clients contract 
suppliers using the RFQ approach (57%), while 29 % of the clients use mixed approaches(open tender and RFQ), 
and the rest use open tender approach.  Survey results show that the majority of firms issue RFQ for ev aluation 
separately depending on the timing but also based on donor requests. RFQs are combined for all evaluations at 
specific intervals in a year. This is based on the Annual M&E Calendar. Survey results show that clients receive 
on an average range between 3- 10 per RFQ or tender. Findings from the survey also show that in all cases they 
receive responses from evaluators. Survey results also show that there is difference in responsiveness 
depending on the type of evaluation varying with the technicality of the projects, level of measurements that 
may be required, scope and size of the evaluation. 
3.3.2 Value,  Size and Type of Current  Evaluation Demand  
According the National Policy on Public Sector Monitoring and Evaluation (2013), all projects over 70 billio n 
shillings in value are required to conduct rigorous evaluation, including a baseline study to establish initial 
conditions, a mid-term review and a final evaluation. The value of current country demand by Government as per 
                                                           
4http://ugandaevaluationassociation.org/wp-content/uploads/2016/03/uganda-evaluation-standards.pdf
<<<PAGE=22>>>
Page 22 
the National Policy on Public sector monitoring and evaluation (2013), is approximately 28% of the projects that 
are valued at over 70 billion shillings. To finance evaluation, all projects are supposed to allocate a percentage 
of their budgets to evaluation, as determined by the Develo pment Committee, taking into account the budget 
and scope of the project. This percentage covers the cost of conducting a baseline study during the project 
preparation, a mid-term review at the half-way stage in the project, and a final evaluation. Based on calculations 
of the average costs of conducting a rigorous baseline, mid-term review and final evaluation, within the range of 
1.5 billion to 2.5 billion shillings are required. This is within 4% of projects budgeting over 70 billion shillings. 
Public po licy and major cross -sectoral evaluations are supposed to be budgeted for under the Office of the 
Prime Minister Development budget. This requires a minimum of three billion shillings per annum, based on a 
cost of conducting 3 evaluations per annum at 1 bi llion shillings each.  Survey findings show that the average 
cost of an evaluation depends on the scope, the coverage among other attendant factors. Survey results 
showed that the average cost of an evaluation in most CSOs is Ug Shs 30 million. 
 
According to the available evidence, approximately 12% of total evaluations conducted in Uganda have been 
commissioned and/or co —managed by GoU. The Government Evaluation Facility  has commissioned 23 
evaluations, these include Evaluation of the Government Response to  absenteeism in the public sector, 
Evaluation of the effectiveness and efficiency of the PPDA; Study of the Government Employment Strategy; 
Impact evaluation of the Baraza initiative; Summative evaluation of the Avian and human Influenza 
Preparedness Project; Comparative study of public service delivery models; Impact/Implementation evaluation 
of the land policy on illegal land evictions. Looking at the identified evaluations commissioned and/or co —
managed by GoU, they are generally consistent with accepted  quality standards for evaluation such as OECD 
DAC evaluation quality standards. They all assess efficiency and effectiveness of programmes, performance 
against qualitative and quantitative indicators, etc., and they all use a mixed methods approach, i.e. literature 
review, stakeholder meetings, and field visits. 
CSOs overall have a systematic approach to evaluation, e.g. regular programme performance evaluations, 
mainly based on donor requirements and procedures. As such, the evaluation processes in CSOs a re to a large 
extent driven by donors, with the evaluations being funded by donors and commissioned on the basis of ToR 
developed in collaboration with donors. Compared to government agencies, CSOs are reportedly better at 
following up on findings and reco mmendations from evaluations, due to competition for donor support in the
<<<PAGE=23>>>
Page 23 
CSO environment, meaning that there is pressure to demonstrate that they qualify for support. The CSOs’ 
existence depends on their ability to produce results, demonstrate impact, and  therefore implement the 
changes suggested in evaluations. 
Results from the survey of clients indicated that the average number of  evaluations they had commissioned 
from suppliers in the last three years (including the current financial year) was six evalu ations. Findings from 
survey of 63 % of the clients who were sure about the number of evaluations indicate that they intend to 
commission evaluations from suppliers on average of five evaluations in the next three financial years. 
Results from the survey o f individual evaluators show that the average number of evaluations that they had 
implemented (or in the process of implementing) over the past 3 years was three. Survey findings showed that 
most of the evaluations that had been implemented were impact eva luations (55%), followed by 
implementation evaluation (27%) and economic evaluation (18%). In the survey, no evaluator had carried out 
diagnostic evaluation and design evaluation implying that either such studies are rarely carried out or there are 
no skil ls to undertake such types of evaluations. A breakdown of the past and future evaluations type 
(diagnostic, design, implementation, impact, economic) by clients who commission evaluations from survey 
findings showed that the common types of evaluation unde rtaken are design (baselines), implementation (Mid-
term reviews) and impact evaluations. 
3.3.3 Specific Professional Skills Required from Evaluators  
 
The specific professional skills required from evaluators by both government and CSOs include a minimum of 
academic qualifications of a first degree in Statistics, Economics or SWSA and Postgraduate Diploma in Project 
Planning and Management. The experience required is at least 6 years’ experience with M&E, and impact 
evaluation in working in the specific sector in  developing countries. The core skills required usually include 
computational and analytical skills, business analysis skills, computer skills relevant to data management, 
database design, statistical analysis skills, training skills, good communication an d interpersonal skills, report 
writing and drafting skills. Other qualities and attributes required include: integrity, empathy, confidentiality and 
innovativeness. 
Results from the survey of clients who commission evaluations show that clients require minimum qualifications 
or experience levels from evaluation suppliers. Survey results also show that the minimum qualifications or 
experience vary per evaluation. The minimum qualifications expected from evaluators include among others;
<<<PAGE=24>>>
Page 24 
experts at PhD level b ut also at Masters level with a wealth of experience in research are considered for the 
lead investigator, extent to which the evaluator has been able to publish their work, relationship with academia, 
evidence of similar work done before and list of conta cts, experience in using both qualitative and quantitative 
methodologies, participated as the lead in at least three assessments/evaluations, demonstrate deep 
knowledge of the field to be evaluated, e xperience in designing and managing evaluations, diversi ty  and 
complimentarity of  the team. Survey results also indicated that the majority of evaluators (75%) do not have 
difficulty in meeting the minimum qualifications or experience levels. This may be attributed to most clients 
using RFQ  approach in contracting evaluators. 
3.4 Matching the Supply and Demand of Evaluators 
A study done in Uganda in June 2009 by Ian C. Davies and found out that demand for evaluative information in, and of the 
Government of Uganda, and resulting use, falls broadly into two categories; Monitoring information for budget allocation 
and control purposes, and Evaluation information to meet the accountability requirements. A key finding of the study was 
that there were significant supply and demand gaps, and variances among MDAs, in organizational capacity for evaluation 
of GOU. Survey findings from most clients show that the supply of evaluators is not sufficient to meet the 
demand. Findings from survey of individual evaluators showed that the majority of the evaluators (71 %) ranked 
their estimated capacity at 50% more evaluations to undertake more evaluations annually than they were 
currently undertaking, which may imply that evaluators are not fully occupied with evaluation assignments. 
To better match the supply to demand of evalu ators, there is need for a deeper analysis of the profession of 
evaluation in Uganda more especially organizational capacity gaps at MDAs  level in Uganda. This would give an 
indication of the gaps between government demand and the current supply as govern ments start to regulate 
the markets they generate as they commission evaluation. 
 
 3.5 Efficiency of  Information Systems  
There is no efficient  Government information system on evaluator availability and opportunities. The only source of 
information about available evaluators is the Uganda Evaluation Association (UEA) and the USAID funded Uganda 
Monitoring and Evaluation Management Services (MEMS) project that developed a roster of Ugandan 
institutions, firms and individual consultants for Monitoring and Evaluation Services. However, UEA and MEMS are 
currently inactive because of limited funding. There is also no proper documentation about evaluators who have recently
<<<PAGE=25>>>
Page 25 
conducted evaluations in Uganda. Findings from the survey of clients show that the majori ty of the evaluators 
(71%) have a database of evaluation providers.  
 
Most staff involved in M&E at national and local level highlight that systematic feedback loops of M&E outputs to sector and 
local-level planning and budgeting do not exist. A recent review of  some of the MDAs pointed out that they are under 
the impression that they are obligated to submit copies of studies and reports produced to NIMES, while others 
are not; however, those MDAs that do submit information to NIMES complain that they do n ot receive any 
feedback, i.e. they do not know if or how the information is used, which in turn weakens incentive for making 
sure reports are forwarded to NIMES. There are therefore no efficient  information systems  on evaluation 
findings and evaluator availability and opportunities. 
 
3.6 Opportunities for Transformation and Uganda- specific Empowerment  
 
There are a number of opportunities for local evaluation capacity building in the country  and in other 
communities of practice such as CSOs in the form o f training programs in monitoring and evaluation at the 
universities and training institutes. The Universities are endowed with highly qualified teaching and research staff 
thereby presenting great opportunities to conduct various evaluations. Think tanks and universities may enhance their 
capacities to conduct evaluations within research processes, whereas donors can provide opportunities for 
learning by doing through support within sector -working groups that are country -led. National, regional and 
international Associations also provide opportunities for the strengthening of institutions’ and individuals’ 
evaluation capacity, through conferences where the sharing of knowledge on evaluation theory and practices 
around the world is facilitated. 
 
There exists opportunities to strengthen technical evaluation demand, with Governments playing a more active 
role in demanding and managing evaluations. Besides triggering M&E demand, the Office of the Prime Minister invests 
in the M&E supply side through the set-up of the Evaluation Facility and the proposal to allocate and ring-fence parts of 
sector budgets to monitoring and (particularly) evaluation is another opportunity to transform evaluation in Uganda. 
 
3.7 Obstacles to Evaluation Market Entry
<<<PAGE=26>>>
Page 26 
There is potential rather than actual technical capacity to manage, undertake and demand evaluations. This is a 
major barrier to a more competitive demand and supply in the evaluation market. Local capacities for conducting 
high-quality evaluation are quite limited. The demand and supply evaluators continue to be driven by 
Development Partners with limited country ownership of the processes. High -quality evaluations are more 
often commissioned and managed by Development Partners than Government, which means that they are les s 
likely to be used in policy. 
 
 
Another barrier is that impact evaluation of programmes is not designed from the outset (so a counterfactual is 
a challenge). Consequently, innovative methodologies are needed, the skills for which may be lacking. 
Development of the quality of the supply of evaluations is important, so that decision-makers are assured of the 
quality of the product they are receiving. In this way the Government can become more confident that 
evaluation helps them to understand issues and directs the public service towards results. The broader political 
and economic environment impact on evaluation systems, for example, where donor funding was withdrawn 
from the Office of the Prime Minister due to corruption.  
 
Results from the survey of indivi dual evaluators show that majority of the evaluators(88%) face  evaluation 
market entry obstacles(i.e ability to obtain appointments from clients to carry out evaluations).The obstacles 
include; unfair selection criteria, clients aim at experience which mo st Ugandans do not posses even when they 
have the required qualifications, most clients tend to think that fellow Ugandans can’t  do quality evaluations 
thus give jobs to foreign evaluators, most clients don’t want individual evaluators but prefer consulta ncy firms 
which are evaluated  on the basis of experience  for which foreign firms have an advantage, lack of adequate 
information linking evaluators to clients, lack of Information on existing opportunities for those who are not 
connected with government bureaucrats, among others. 
Findings from survey of individual evaluators showed that most of the evaluators(63%) have experienced 
challenges with the way their clients have managed evaluations that they have undertaken (at any stage of the 
evaluation process including procurement and implementation). The challenges faced by individual evaluators 
include; reluctance to provide financial data and when they provide it is incomplete or delayed, procurement
<<<PAGE=27>>>
Page 27 
delays, poorly designed ToRs that  lead to disagreement s in the scope of the study results and findings, lack of 
adequate supervisory capacity, international evaluator bias, among others. 
Survey findings also show that clients had in most cases problems with the quality of evaluations done by 
suppliers. The problems include: unclear results from the project evaluation, long reports, poor documentation 
of success stories or impact of the project, estimations rather than facts,  lack a proper methodology, poor 
depth of reporting and analysis of data collected, st rong in quantitative and weak in qualitative and vice -versa, 
poor demonstration of impact with limited advanced analysis techniques, turnaround time, limited articulation 
of issues in line with sustainability and related indicators, weak illustrations of lessons and recommendations for 
both the project but also for future reference/projects, wrong data collected, unclear documentation of impact,  
inadequate triangulation of data, absence of standardized evaluation tools and guidelines, n, limited capacity, 
rigid and inflexible donors,much focus on on methodology and limited focus on evaluation purpose among 
others. 
Results from the survey also show that most of the clients experienced problems on particular aspects of 
evaluations. The aspects of evaluations where clients experienced problems include: generation of gender 
disaggregated data, statistical analysis, methodology, qualitative analysis and presentation of the report. 
In most cases, CSOs do not have a budget for M&E, except what is allocated for M&E from programme and 
project budgets. Consequently, the NGOs often have weak M&E systems in place. Some CSOs have no M&E 
function; instead the responsibility to monitor and evaluate activities resides with project officers. Given the 
financial constraints facing CSOs, training of staff in M&E appears to be rare and occurs on an ad hoc basis. 
CSO Field -staff often lack the knowledge and skills to go beyond checking numbers, i.e. to appreciate the 
analytical dimension of evaluations and consequently what inform ation is needed to answer questions about 
outcomes, for instance. When a commissioner of evaluation in a CSO has poor knowledge and training in M&E, 
the person does not know what type of questions to ask, resulting in poor terms of reference. This spills o ver 
into poor management of evaluations, and consequently creates a risk of poor evaluations. 
 
3.8 Citizen-based Monitoring and Empowerment
<<<PAGE=28>>>
Page 28 
The 2030 Agenda for Sustainable Development puts citizens at the center of achieving the new development 
agenda. Aid  Data5, is testing an innovative work in Uganda a crowd -sourcing technology, which allows 
community stakeholders to give feedback to development partners about programmes implemented.  Through 
this programme policy makers and donors get beneficiary feedback and are thus able to monitor results. 
 
Uganda Debt Network is also implementing a Community Participation and Empowerment programme. UDN’s 
work with communities is based on the belief that improvements in economic literacy will empower poor 
people to demand that Government allocate funds to address their needs and concerns. Investment in citizen -
led monitoring will ensure that lessons from local contexts influence the planning, design, and implementation 
of policies. This is because citizens from the loca l community best understand their context and will therefore 
bring their understanding in the planning, designing, implementation and monitoring of policies that affect 
them (Beyond2015 Campaign, 2014).  
 
In Uganda, the Government has used decentralization  as a system of improving service delivery and 
strengthening good governance.  To address the deficiencies in public service delivery at local level, the 
Government has strengthened monitoring of programmes using “monitoring units and inspectorate in centr al 
government ministries, public accounts committee of parliament, constitutionalized statutory accountability 
bodies, local accountability committees, and more recently administrative initiatives such as 
barazas”(Kyohairwe, 2014).  
 
The Government of Ugan da has formalized the use of barazas 6.  The baraza brings together twice a year 
stakeholders from the central government, service providers or bureaucrats and the public/community, as well 
as users of the services, and provides them with an opportunity for  sharing public information with the local 
                                                           
12 AidData is a research and innovation lab that seeks to improve deve lopment outcomes by making development 
finance data more accessible and actionable.  See http://aiddata.org/listen-to-citizen-voices#  5 
6Baraza is a Swahili word that means “ a public meeting which is used as a platform for creating awareness, responding to issues 
affecting a given community, sharing vital information, and providing citizens with the opportunity to identify and propose 
solutions on their concerns” (Kyoheirwe, 2014).
<<<PAGE=29>>>
Page 29 
community. It is focused on effective monitoring of public service provision and given an arena for the 
community to demand accountability from the bureaucrats.   
 
Uganda, community monitoring and evaluation of th e delivery of public health has improved the quality of 
delivery by public hospitals. In some districts, communities and civil society have set up health users’ 
management committees. Where these committees were active, the World Bank reports absenteeism by public 
servants decreased and the quality of service measured by wait time and quality of care, improved (Bjorkman & 
Svensson, 2009). 
 
4. Conclusion and Recommendations 
 
4.1 Conclusion 
 
The study aimed at establishing the capacity and supply of evaluators in Uganda, and what is required to strengthen this 
capacity and supply. On the supply side, the study established that there is a substantial pool of M&E consultants in 
Uganda.However there is no national data base for evaluators. The study established t hat there are   a number 
universities, training institutions and national, regional and international networks for local evaluation capacity 
building in Uganda. It was also established that about half of the donor -led evaluations were conducted by 
international evaluators, while balance was conducted by mixed teams, i.e. international and national 
evaluators. The pricing  of skills varies according to experience, education, or whether one is a local or 
international consultant.Suppliers are not selected on merit, besides they lack information on the evaluation 
subject. In Uganda, there is a disconnect between evaluation supply and demand as there is little evidence of substantive 
relationships between government and evaluation agents, except in some limited areas.  
 
On the demand side, it was established that there are no national government system requirements for eligibility to 
provide evaluation services. However, the UEA has developed and approved standard guidelines that stipulate 
that evaluators and eva luation teams must be independent, trustworthy and transparently selected. The value 
of current country demand by government as per the National Policy on Public sector monitoring and evaluation 
(2013), is approximately 28% of the projects that are valued at over 70 billion shillings. To finance evaluation, all
<<<PAGE=30>>>
Page 30 
projects are supposed to allocate a percentage of not more than 4% of their budgets to evaluation, as 
determined by the Development Committee, taking into account the budget and scope of the project.  The study 
also established the specific professional skills required from evaluators by both government and CSOs including 
the minimum academic qualifications, experience, core skills and other qualities or attributes. The only source of 
information about available evaluators is the Uganda Evaluation Association (UEA) and the USAID funded Uganda 
Monitoring and Evaluation Management Services (MEMS) project that developed a roster of Ugandan 
institutions, firms and individual consultants for Monitoring and Evaluation Services. 
The study also established a number a number of opportunities for local evaluation capacity building in Uganda 
and in other communities of practice such as CSOs in the form of training programs in monitoring and 
evaluation at the univer sities, training institutes and professional networks. It was also established that as 
Uganda government continues to demand more evaluative evidence, the number and quality of human resources 
required to meet this will grow and this will lead to sustainability of the evaluation market. To better match the supply to 
demand of evaluators, it was established that there is need for a deeper analysis of the profession of evaluation in 
Uganda more especially organizational capacity gaps at MDAs level in Uganda. It was also established that local 
capacities for conducting high -quality evaluation are quite limited as well as poor management of evaluation 
programs. The demand and supply evaluators continue to be driven by development partners with limited 
country ow nership of the processes and there is limited demand for evaluation from the cabinet and 
parliament. 
 
4.2 Recommendations 
To strengthen the evaluation capacity demand and supply of evaluators there is need to strengthen the   culture of 
management that und erstands values and uses evaluative information to achieve results and organizational 
performance. Without a culture of management, including policy and accountability, there is little use of, and 
effective demand for, evaluation. This means that managemen t reforms and improvements are a necessary 
counterpart to successful development of meaningful evaluative practices.  
 
The NIMES Secretariat and the National Monitoring and Evaluation Technical Working Group (NMETWG) should 
take the lead in fostering closer collaboration with donors on evaluation and ensuring systematic dissemination 
of evaluation reports and sharing of good practices. Furthermore, sharing evaluation experiences among
<<<PAGE=31>>>
Page 31 
countries can strengthen local and regional evaluation networks, contribu ting to the development of regional 
evaluation capacities and to fostering demand for evaluation, making policy makers aware of the knowledge 
generated by evaluation and the possibilities of using that knowledge to improve policy making. 
 
There is need to create an  efficient  information systems on evaluation findings and  evaluator availability and 
opportunities  by the Office of the Prime Minister(OPM). This will not only match the supply and demand for evaluators 
but can also create incentive for making sure there is submission of evaluation reports to NIMEs and feedback on 
the reports. There is need to maintain a database of evaluations and a knowledge resource center for 
evaluation  in Uganda. There is need to create or update the 2012 MEMS database by profiling and categorizing 
evaluators according to their experience. This data base should be updated regularly. It should be a requirement 
that foreign consultant is understudied by  a local consultant  for capacity building and sustainability.OPM 
should adopt the evaluation standards developed by UEA and make them legal. 
Further local capacity should receive preference in commissioning evaluation, rather than relying upon 
international expertise. In this way, Government can improve the quality of the prov ision of a public good 
(evaluation), through developing and regulating the market. In the longer term this can help to enhance local 
and contextually relevant capacity for both monitoring and evaluation.  
In respect of supply, the diagnostic study found th at evaluation practice and capacity in the GoU need to be 
strengthened and expanded. There is a need to expand the existing pool of evaluators, and enable emerging 
evaluators to enter the market with fewer restrictions. A strategy for developing and streng thening evaluation 
should address capacity issues such as, institutional supports to evaluation such as guidelines, standards and 
competency requirements (knowledge, skills and abilities) for evaluation positions in GoU (job descriptions) as 
well as ongoin g professional development. Development of evaluation norms and standards can help 
government to place demands on the evaluation profession that will raise the overall quality of practice. There 
is also need to develop procedure templates for evaluation in  MDAs. There is also need to support the capacity 
development of Parliament to demand evaluations as part of their mandate. 
 
Persons engaged in designing, conducting and managing evaluation activities should possess core evaluation 
competencies which shoul d be maintained through a regular programme of continuing professional
<<<PAGE=32>>>
Page 32 
development. The professional capacity of evaluators  and commissioners of evaluations should be continuously  
developed through improved knowledge and skills; strengthening evaluation management; stimulating demand 
for evaluations; and supporting an environment  of accountability and learning. 
 
The capacity of UEA should be strengthened. In addition the capacity of commissioners of evaluations should 
be enhanced following a capacity nee ds assessment. Dissemination and enforcement of  evaluation guidelines 
should be  fast tracked. 
  
There is need to develop and maintain ongoing professional development for evaluation in GoU. There should 
be an annual programme of professional development for evaluators in MDAs. For example the programme 
could be made up of annual cross -government core training sessions, for example at introductory 7 and 
intermediate8 levels and on specialised topics, together with attendance at external training sessions ba sed on 
professional development needs 9. Training should provide more than competencies in M&E. Senior officials 
need to understand the strengths and limitations -the relative cost effectiveness of various types of evaluation 
tools and techniques. Introducto ry training can also raise awareness of and demand for M&E information. 
Training should extend to the use of evaluation findings. They must be able to tell when an evaluation is reliable 
or when its methodology or findings are questionable. There is also n eed to provide professional and technical 
support to evaluation in MDAs by organising, brokering, disseminate professional evaluation training and 
conferencing activities.  
 
There is also need to make evaluations participatory. Evaluations should be carrie d by a well -balanced 
combination of internal and experienced external evaluators to take advantage of the strengths and counter the 
limitations of each. Participation of the clients staff in evaluation shall encourage ownership of results and 
capacity building. Experienced evaluators from outside the programme shall provide additional insight, greater 
technical expertise, and be more objective in formulating recommendations. 
                                                           
7For example see: http://gsociology.icaap.org/methods/ 
8For example see: http://www.evaluationcanada.ca/site.cgi 
9These should be ascertained during the annual staff performance appraisal.
<<<PAGE=33>>>
Page 33 
There is also need interface with and support the development of professional evalu ation associations such as 
the Ugandan Evaluation Association. The association has the potential to provide ongoing professional 
networking and development if provided with support. The association can be a forum where evaluation 
professionals meet and sha re information and good practices on evaluation in addition to organizing short 
courses on topics of interest to its members. Once it is nurtured the UEA has the potential to contribute to 
professionalizing home grown evaluation capacity. 
 
There is also ne ed to define very well the key competencies for evaluation that should form part of the 
functional organization of personnel requirements. For example, evaluations should be carried out or managed 
by a unit with required personnel competencies in evaluation. 
To better match the supply to demand of evaluators , there is need for a deeper analysis of the profession of 
evaluation in Uganda more especially organizational capacity gaps at MDAs  level in Uganda. This would give an 
indication of the gaps between go vernment demand and the current supply as governments start to regulate 
the markets they generate as they commission evaluation. 
 
Individual evaluators who were surveyed had views on what should be done to address the challenges with the 
way in which their  clients have managed evaluations that they have undertaken. The views on what should be 
done to address the challenges include; evaluation work should be regulated to ensure client fulfill the 
obligation of providing full information, build capacity for commissioning evaluations to help those who demand 
for evaluation services determine what they want, need for review of procurement processes with a view of 
reducing turnaround time, enhance local evaluation capacity across board, among others. 
Survey findi ngs show that to improve the quality of evaluations done by suppliers, there is need to do the 
following: the suppliers need to read  the ToR/ scope of work and understand them well, commissioners of 
evaluation should provide detailed TOR or RFQ, developme nt and regular update of the supplier data base, 
capacity building for suppliers on statistical,qualitative analysis and report presentation of findings that meet 
the needs of the client or commissioner of the evaluation, improve utilization of evaluation,  deliberate effort to 
ensure all involved in evaluation are all -round grounded in the various skills, improve evaluation design and 
methodology, encouraging suppliers to have competent team composition, establishing communities of
<<<PAGE=34>>>
Page 34 
practices based on the sectors to share and learn, joint partnership with a credible research institutions, among 
others. 
 
Survey findings from most clients also show that the supply of evaluators is not sufficient to meet the demand. 
To address quality or competitiveness problems clients proposed the following solutions: training all private and 
government organisation managers to gain evaluation skills(development of TOR, methodology, etc), promote 
evaluation, strengthen evaluation standards throughout the entire process by weedi ng out incompetent 
evaluators, PPDA needs to reinforce evaluation standards throughout the entire process to remove   unskilled 
evaluators, improve competences in evaluation design, establish a database of evaluators  for each sector,  
continuous professional development, and work through the Uganda Evaluation Association to build capacity of 
evaluators.  
The recommendations that emerged from stakeholders consultative workshop included the need: for 
evaluators to be accredited; to give UEA legal mandate to regulate evaluators; to develop, adopt and 
disseminate standards and regulations for evaluations for all sectors; to support the establishment of an 
evaluation tribunal; to make it a requirement for foreign evaluators to work with local evaluators to encou rage 
transfer of skills; to make it a requirement in any program design to  have an evaluation component,  to  
develop a communication strategy to create awareness about the need for evaluation  and  to publish 
evaluation reports. 
In addition , the stakeho lders in the consultative workshop recommended the need : to ;create a Centre of 
Excellency in evaluation that would establish  an efficient Uganda information systems on evaluation findings and  
evaluator availability and opportunities  , and start e-learning courses; to support implementation of the National 
Monitoring and Evaluation policy; to support  both government and CSOs to establish Monitoring and 
Evaluation  units/departments in their organizational structures; to reduce, harmonize and minimize duplication 
of monitoring functions;  to  identify champions of evaluation to promote evaluations; and to support training 
institutions to  mainstream monitoring and evaluation in the courses being taught.
<<<PAGE=35>>>
Page 35 
References 
Bjorkman, Martina, and Jakob Svensson (2009) Power to the People: Evidence From a Randomized Field 
Experiment on Community-Based Monitoring in Uganda. The Quarterly Journal of Economics 124(2): 735-69. 
BTC Uganda. (2012). Progress Report #2, Education Sector Budget Support (ESBS). Kampala: BTC Uganda.  
 
Beyond 2015 Campaign. (2014). Citizen-led accountability: recommendations and   key research findings for the 
post-2015 Agenda. 
 
Davies, I.(2009).Mapping Evaluation Practice , Demand and Related Capacity Study, Office of the Prime Minister, 
Uganda 
Kyohairwe, S. (2014). Local democracy and public accountability  in Uganda: the  need for organizational 
learning. Commonwealth Journal of local  Governance. June 2014. 
 
National Policy on Public Sector Monitoring and Evaluation(2013), Office of the Prime Minister, Uganda. 
 
S.P, (2013) A Growing Demand for Monitoring and Evaluation in Africa. African Evaluation Journal 2013;1(1), 
 
World Bank (2009). Project Appraisal Document for the proposed credit in the amount of SDR 99 Million (US$ 150 
Million Equivalent) to the Republic of Uganda for a Post Primary Education and Training Adaptable Program 
Lending (apl 1) Project in Support of the First Phase of the Uganda Post Primary Education and Training Program, 
March 5, 2009. Washington, DC.: World Bank.
<<<PAGE=36>>>
Page 36 
Annex I: Action Plan 
ACTION PLAN FOR THE SUPPLY AND DEMAND OF EVALUATORS IN UGANDA 
 
 
TWENDE MBELE 
 
With Approval of : 
Office of the Prime Minister 
 
April, 2018 
TABLE: ACTION PLAN 2018- 2023 
Intervention  Budget(US $) 
Short term(2018-2019)  
1. Support O ffice of the Prime minister to establish a Centre of Excellency in 
evaluation that would establish  an efficient Uganda information systems on 
evaluation findings and  evaluator availability and opportunities   
150,000 
2. Support development of guidelines,  regulations, standards and competency 
requirements (knowledge, skills and abilities) for evaluation positions in GoU (job 
descriptions) as well as ongoing professional development.  
80,000 
3.Support a study on deeper analysis of the profession of evaluation in Uganda  50,000 
4.Support training all private and government organization managers to gain 
evaluation skills  
30,000 
5.Identify champions of evaluation to promote evaluation of government and 
private programs and projects 
5,000 
Medium term and long term  
1.Support the NIMES Secretariat and the National Monitoring and Evaluation 
Technical Working Group (NMETWG) ensure systematic dissemination of 
evaluation reports and sharing of good practices.  
30,000 
2.Support  Ugandan Evaluation Association  to establish communities of practices 20,000
<<<PAGE=37>>>
Page 37 
based on the sectors to share and learn 
3.Support capacity building of evaluators 15,000 
4.Support  formation of an accreditation system for  evaluators 10,000 
5.Support UEA to have a legal mandate to regulate evaluators  10,000 
6. Support the establishment of an evaluation tribunal  15,000 
7.Support a requirement for foreign evaluators to work with local evaluators to 
encourage transfer of skills;  
10,000 
8.Develop a communication strategy to create awarenes s about the need for 
evaluation   
40,000 
9.Support higher training institutions to mainstream monitoring and evaluation in 
the courses being taught. 
30,000 
10.Support  both government and CSOs to establish Monitoring and Evaluation  
units/departments in their organizational structures 
20,000 
11.Support implementation of the National Monitoring and Evaluation policy  30,000 
12.Support publication of evaluation reports. 15,000 
 
 
 
 
 
 
 
 
Annex 2II: Staff Interviewed and Contacts 
1. Violet Alinda     valinda@twaweza.org 
2. Susan Asio   susiep112@yahoo.com 
3. ROSEMARY WAYA  rosemarywaya@yahoo.co.uk 
4. suzan akello   harrietakello80@gmail.com 
5. atukwatse charity  atukwatsecharity1@gmail.com
<<<PAGE=38>>>
Page 38 
6. James Baanabe  baanabej@gmail.com 
7. Edward lwanga  eddielwanga15@gmail.com 
8. Charles Igga   Charles_Igga@wvi.org 
9. Vincent Ssozi   ssoziv@gmail.com 
10. Ceaser Kimbugwe ceaser.kimbugwe@wateraid.org  
11. Eddie Lwanga   eddielwanga15@gmail.com 
12. Kato  Emanuel  ekato@gccu.or.ug 
13. Edmond Owor  emoworao14@gmail.com 
14. Tukesiga Julius  jtukesiga@gmail.com 
15. Namaga Imelda namelda@yahoo.com 
16. Watera Josephine  watera.josephine@yahoo.com 
17. sheilla mbabazi  mbabazi.sheilla1989@gmail.com 
18. Yusuf Kiwala   kyosseuf2@gmail.com 
19. Mutenyo John  jmutenyo@gmail.com 
20. Mukisa Ibrahim imukisa@gmail.com 
21. Wokadara James          jwokadara@gmail.com 
22. Henray Sebukeera  henrichsebs@gmail.com 
23. Gerald Kato   gerald.kato@ftp.org 
24. Nakirya Caroline  nakirya@acfode.org 
25. John Bbaale  jbbaale@gmail.com 
26. Denis Muhangi  denmuhangi@gmail.com 
27. John Bosco Asiimwe     asiimwejb@gmail.com 
28. SABA MOHAMMED  sabahmhayeli@gmail.com 
29. Mbabule Derrick            mbalulederrick91@gmail.com 
30. Patrick Olowo               olowopato@gmail.com 
31. Edward Baale  ebaale@yahoo.com 
32. Nick Kilimani     nick.kilimani1@gmail.com